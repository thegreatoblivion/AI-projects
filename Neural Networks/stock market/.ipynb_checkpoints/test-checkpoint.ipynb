{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60b9057",
   "metadata": {},
   "source": [
    "0 = close/last, 1 = open, 2= high, 3 = low\n",
    "objective: guess the next closing value in the next day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "4aae16aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "a = ['2','3']\n",
    "b = a[0]\n",
    "print(np.linspace(0,9,num=10))\n",
    "def normalize(n):\n",
    "    n = 1/(1+np.exp(-n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6acd1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with open(r'C:\\Users\\user\\Documents\\road to qm\\machine learning\\pytorch\\stock market\\NVDA_10.csv', newline='') as csvfile:\n",
    "    data = list(csv.reader(csvfile))\n",
    "data = np.array(data)\n",
    "num_days = data.shape[0]-1\n",
    "#print(num_days)\n",
    "#print(data)\n",
    "#print(data.shape)days = 200\n",
    "def normalize(n):\n",
    "    n = 1/(1+np.exp(-n))\n",
    "    return n\n",
    "def stock_vector_generator(data, days):\n",
    "    data = np.delete(data,0,axis = 0) #deletes the top row\n",
    "    upper_range = np.random.randint(days,num_days) #gives index between days and num_days as a starting point.\n",
    "    lower_range = upper_range - days\n",
    "    #closing value the next day - this will be our test data\n",
    "    next_day_row = data[lower_range-1]\n",
    "    \n",
    "   \n",
    "    #slicing data into small chunks and removing unneccessary rows and columns\n",
    "    stock_array = data[lower_range:upper_range,:]\n",
    "    #print(stock_array)\n",
    "    #print(stock_array.shape)\n",
    "    #print('Date start: ' + stock_array[-1][0])\n",
    "    #print('Date end: ' + stock_array[0][0])\n",
    "    #print('Days: ' + str(days))\n",
    "    stock_array = np.delete(stock_array, 0, axis = 1)\n",
    "    stock_array = np.delete(stock_array, 1, axis = 1)\n",
    "    \n",
    "    #now to get rid of the dollar sign and change all into floats\n",
    "    for row in range(stock_array.shape[0]):\n",
    "        for element in range(stock_array.shape[1]):\n",
    "            price = stock_array[row][element]\n",
    "            if price[0] == '$':\n",
    "                price = price[1:]\n",
    "            stock_array[row][element] = price\n",
    "    stock_array = np.array([list( map(float,i) ) for i in stock_array])\n",
    "    #stock_array = np.array([list( map(normalize,i) ) for i in stock_array])\n",
    "    #now turn into a single lined vector\n",
    "    stock_vector = stock_array.reshape(days*4,1)\n",
    "    return stock_vector, next_day_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a8c90a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5178],\n",
      "        [0.5168],\n",
      "        [0.5186],\n",
      "        [0.5125],\n",
      "        [0.5162],\n",
      "        [0.5085],\n",
      "        [0.5176],\n",
      "        [0.5032],\n",
      "        [0.5075],\n",
      "        [0.4978],\n",
      "        [0.5088],\n",
      "        [0.4943],\n",
      "        [0.5005],\n",
      "        [0.5025],\n",
      "        [0.5053],\n",
      "        [0.4943],\n",
      "        [0.4990],\n",
      "        [0.4893],\n",
      "        [0.4995],\n",
      "        [0.4878],\n",
      "        [0.4900],\n",
      "        [0.4970],\n",
      "        [0.5000],\n",
      "        [0.4898],\n",
      "        [0.4935],\n",
      "        [0.4863],\n",
      "        [0.4950],\n",
      "        [0.4850],\n",
      "        [0.4914],\n",
      "        [0.4960],\n",
      "        [0.5060],\n",
      "        [0.4881],\n",
      "        [0.4923],\n",
      "        [0.4997],\n",
      "        [0.4999],\n",
      "        [0.4878],\n",
      "        [0.4985],\n",
      "        [0.4983],\n",
      "        [0.5022],\n",
      "        [0.4914]], dtype=torch.float64)\n",
      "tensor([0.5154])\n"
     ]
    }
   ],
   "source": [
    "#initializing\n",
    "stock_vector, next_day_row = stock_vector_generator(data,days)\n",
    "stock_tensor = torch.from_numpy(stock_vector)\n",
    "actual = torch.from_numpy(np.array([float(next_day_row[1][1:])])).float()\n",
    "print(stock_tensor)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "57187e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 0 step, loss is 40.54405212402344\n",
      "At 10 step, loss is 4.736854553222656\n",
      "At 20 step, loss is 17.62688636779785\n",
      "At 30 step, loss is 6.776853561401367\n",
      "At 40 step, loss is 3.7624568939208984\n",
      "At 50 step, loss is 0.7027180790901184\n",
      "At 60 step, loss is 1.1167670488357544\n",
      "At 70 step, loss is 2.292787790298462\n",
      "At 80 step, loss is 16.061826705932617\n",
      "At 90 step, loss is 2.525765895843506\n",
      "At 100 step, loss is 1112.8173828125\n",
      "At 110 step, loss is 737.2779541015625\n",
      "At 120 step, loss is 6.669938011327758e-05\n",
      "At 130 step, loss is 19.68863868713379\n",
      "At 140 step, loss is 164.8701629638672\n",
      "At 150 step, loss is 331.8887939453125\n",
      "At 160 step, loss is 69.87318420410156\n",
      "At 170 step, loss is 187.54690551757812\n",
      "At 180 step, loss is 501.453857421875\n",
      "At 190 step, loss is 504.4461669921875\n",
      "At 200 step, loss is 11.358949661254883\n",
      "At 210 step, loss is 0.041753172874450684\n",
      "At 220 step, loss is 0.12759943306446075\n",
      "At 230 step, loss is 0.08090342581272125\n",
      "At 240 step, loss is 0.04690520837903023\n",
      "At 250 step, loss is 0.0010092755546793342\n",
      "At 260 step, loss is 0.00048571472871117294\n",
      "At 270 step, loss is 47.19916534423828\n",
      "At 280 step, loss is 0.0060806358233094215\n",
      "At 290 step, loss is 2.1477458477020264\n",
      "At 300 step, loss is 0.28173938393592834\n",
      "At 310 step, loss is 0.25047755241394043\n",
      "At 320 step, loss is 0.18036343157291412\n",
      "At 330 step, loss is 0.49644550681114197\n",
      "At 340 step, loss is 0.014497010037302971\n",
      "At 350 step, loss is 1.3978526592254639\n",
      "At 360 step, loss is 0.263595312833786\n",
      "At 370 step, loss is 10.260852813720703\n",
      "At 380 step, loss is 0.13474367558956146\n",
      "At 390 step, loss is 30.877357482910156\n",
      "At 400 step, loss is 24.836017608642578\n",
      "At 410 step, loss is 1.4736194610595703\n",
      "At 420 step, loss is 2.1011362075805664\n",
      "At 430 step, loss is 229.4668426513672\n",
      "At 440 step, loss is 48.52993392944336\n",
      "At 450 step, loss is 2.5950446128845215\n",
      "At 460 step, loss is 0.015706287696957588\n",
      "At 470 step, loss is 2.8890299797058105\n",
      "At 480 step, loss is 1.0415223836898804\n",
      "At 490 step, loss is 0.007859284989535809\n",
      "At 500 step, loss is 6.794889450073242\n",
      "At 510 step, loss is 0.00382174551486969\n",
      "At 520 step, loss is 103.08720397949219\n",
      "At 530 step, loss is 0.9234310984611511\n",
      "At 540 step, loss is 2.588651418685913\n",
      "At 550 step, loss is 1.156471610069275\n",
      "At 560 step, loss is 0.0004397828597575426\n",
      "At 570 step, loss is 0.08643272519111633\n",
      "At 580 step, loss is 8.768466068431735e-05\n",
      "At 590 step, loss is 161.50897216796875\n",
      "At 600 step, loss is 0.05507948622107506\n",
      "At 610 step, loss is 18.92578887939453\n",
      "At 620 step, loss is 0.03186197206377983\n",
      "At 630 step, loss is 39.405426025390625\n",
      "At 640 step, loss is 11.255023956298828\n",
      "At 650 step, loss is 147.11961364746094\n",
      "At 660 step, loss is 1.6951606273651123\n",
      "At 670 step, loss is 12.365533828735352\n",
      "At 680 step, loss is 0.45650631189346313\n",
      "At 690 step, loss is 55.21034622192383\n",
      "At 700 step, loss is 337.8374938964844\n",
      "At 710 step, loss is 0.027478288859128952\n",
      "At 720 step, loss is 11.91084098815918\n",
      "At 730 step, loss is 0.0270851980894804\n",
      "At 740 step, loss is 0.16091576218605042\n",
      "At 750 step, loss is 0.00023928118753246963\n",
      "At 760 step, loss is 0.17363721132278442\n",
      "At 770 step, loss is 3.1672821044921875\n",
      "At 780 step, loss is 0.08732125908136368\n",
      "At 790 step, loss is 0.1476580798625946\n",
      "At 800 step, loss is 0.002520469482988119\n",
      "At 810 step, loss is 2.5272271633148193\n",
      "At 820 step, loss is 0.0020724888890981674\n",
      "At 830 step, loss is 20.093900680541992\n",
      "At 840 step, loss is 0.021536359563469887\n",
      "At 850 step, loss is 2.9235522747039795\n",
      "At 860 step, loss is 0.006150366738438606\n",
      "At 870 step, loss is 0.19129131734371185\n",
      "At 880 step, loss is 0.8483859300613403\n",
      "At 890 step, loss is 0.030159134417772293\n",
      "At 900 step, loss is 0.019964929670095444\n",
      "At 910 step, loss is 61.610260009765625\n",
      "At 920 step, loss is 5.0541090965271\n",
      "At 930 step, loss is 0.12046203762292862\n",
      "At 940 step, loss is 0.14438052475452423\n",
      "At 950 step, loss is 0.04741164296865463\n",
      "At 960 step, loss is 4.026047229766846\n",
      "At 970 step, loss is 0.016075406223535538\n",
      "At 980 step, loss is 0.18665339052677155\n",
      "At 990 step, loss is 0.014208808541297913\n",
      "At 1000 step, loss is 34.90113830566406\n",
      "At 1010 step, loss is 0.4359435439109802\n",
      "At 1020 step, loss is 0.005120137706398964\n",
      "At 1030 step, loss is 0.2932693362236023\n",
      "At 1040 step, loss is 0.0004366061766631901\n",
      "At 1050 step, loss is 0.7118545174598694\n",
      "At 1060 step, loss is 8.219369888305664\n",
      "At 1070 step, loss is 0.18782392144203186\n",
      "At 1080 step, loss is 2.6308863162994385\n",
      "At 1090 step, loss is 0.002014463534578681\n",
      "At 1100 step, loss is 216.60948181152344\n",
      "At 1110 step, loss is 0.004595940932631493\n",
      "At 1120 step, loss is 0.07742507755756378\n",
      "At 1130 step, loss is 3.5690629482269287\n",
      "At 1140 step, loss is 0.009595880284905434\n",
      "At 1150 step, loss is 0.17472538352012634\n",
      "At 1160 step, loss is 0.0309295654296875\n",
      "At 1170 step, loss is 1.2224254608154297\n",
      "At 1180 step, loss is 0.04743150249123573\n",
      "At 1190 step, loss is 0.47346338629722595\n",
      "At 1200 step, loss is 0.005599323194473982\n",
      "At 1210 step, loss is 3.162653684616089\n",
      "At 1220 step, loss is 0.03061271831393242\n",
      "At 1230 step, loss is 0.11585965007543564\n",
      "At 1240 step, loss is 2.9100987911224365\n",
      "At 1250 step, loss is 0.3294238746166229\n",
      "At 1260 step, loss is 1.0967650413513184\n",
      "At 1270 step, loss is 0.004566828720271587\n",
      "At 1280 step, loss is 0.021728981286287308\n",
      "At 1290 step, loss is 0.10199698805809021\n",
      "At 1300 step, loss is 0.016078906133770943\n",
      "At 1310 step, loss is 0.012169805355370045\n",
      "At 1320 step, loss is 0.419604629278183\n",
      "At 1330 step, loss is 5.658877372741699\n",
      "At 1340 step, loss is 28.94260597229004\n",
      "At 1350 step, loss is 1.0258077383041382\n",
      "At 1360 step, loss is 0.04527958855032921\n",
      "At 1370 step, loss is 44.52842330932617\n",
      "At 1380 step, loss is 0.527949869632721\n",
      "At 1390 step, loss is 4.995645523071289\n",
      "At 1400 step, loss is 10.614398956298828\n",
      "At 1410 step, loss is 0.05544199422001839\n",
      "At 1420 step, loss is 29.325498580932617\n",
      "At 1430 step, loss is 11.194731712341309\n",
      "At 1440 step, loss is 0.18996867537498474\n",
      "At 1450 step, loss is 0.4624023735523224\n",
      "At 1460 step, loss is 1.08623468875885\n",
      "At 1470 step, loss is 0.5695191025733948\n",
      "At 1480 step, loss is 0.12148765474557877\n",
      "At 1490 step, loss is 0.2569788694381714\n",
      "At 1500 step, loss is 0.6002792716026306\n",
      "At 1510 step, loss is 0.17964927852153778\n",
      "At 1520 step, loss is 2.2656280994415283\n",
      "At 1530 step, loss is 0.4679153859615326\n",
      "At 1540 step, loss is 0.031817853450775146\n",
      "At 1550 step, loss is 4.070236682891846\n",
      "At 1560 step, loss is 1065.11376953125\n",
      "At 1570 step, loss is 0.008533522486686707\n",
      "At 1580 step, loss is 0.8641065359115601\n",
      "At 1590 step, loss is 0.676781952381134\n",
      "At 1600 step, loss is 2.598151922225952\n",
      "At 1610 step, loss is 0.8545935750007629\n",
      "At 1620 step, loss is 57.1622428894043\n",
      "At 1630 step, loss is 23.238529205322266\n",
      "At 1640 step, loss is 13.17422103881836\n",
      "At 1650 step, loss is 1623.1513671875\n",
      "At 1660 step, loss is 3.004951238632202\n",
      "At 1670 step, loss is 0.8628281950950623\n",
      "At 1680 step, loss is 2.38132643699646\n",
      "At 1690 step, loss is 2.569545269012451\n",
      "At 1700 step, loss is 0.6609801650047302\n",
      "At 1710 step, loss is 474.36767578125\n",
      "At 1720 step, loss is 63.28272247314453\n",
      "At 1730 step, loss is 0.012524778954684734\n",
      "At 1740 step, loss is 0.09787217527627945\n",
      "At 1750 step, loss is 0.1510332077741623\n",
      "At 1760 step, loss is 1.274119257926941\n",
      "At 1770 step, loss is 4.6107940673828125\n",
      "At 1780 step, loss is 2.351835012435913\n",
      "At 1790 step, loss is 7.909949779510498\n",
      "At 1800 step, loss is 2.028576612472534\n",
      "At 1810 step, loss is 0.19239918887615204\n",
      "At 1820 step, loss is 2.478684186935425\n",
      "At 1830 step, loss is 3.605591297149658\n",
      "At 1840 step, loss is 1.772507667541504\n",
      "At 1850 step, loss is 0.13159728050231934\n",
      "At 1860 step, loss is 0.02516321651637554\n",
      "At 1870 step, loss is 1.7222882509231567\n",
      "At 1880 step, loss is 0.11281926929950714\n",
      "At 1890 step, loss is 1.328481912612915\n",
      "At 1900 step, loss is 0.2448125034570694\n",
      "At 1910 step, loss is 0.00028847603243775666\n",
      "At 1920 step, loss is 2.391146659851074\n",
      "At 1930 step, loss is 0.4137454032897949\n",
      "At 1940 step, loss is 45.40494918823242\n",
      "At 1950 step, loss is 0.012388219125568867\n",
      "At 1960 step, loss is 0.07358471304178238\n",
      "At 1970 step, loss is 2.036982297897339\n",
      "At 1980 step, loss is 0.24116382002830505\n",
      "At 1990 step, loss is 12.05359935760498\n",
      "At 2000 step, loss is 0.12123715132474899\n",
      "At 2010 step, loss is 0.08202022314071655\n",
      "At 2020 step, loss is 8.563444137573242\n",
      "At 2030 step, loss is 0.09398695081472397\n",
      "At 2040 step, loss is 0.3886605203151703\n",
      "At 2050 step, loss is 0.05553971603512764\n",
      "At 2060 step, loss is 0.0023647609632462263\n",
      "At 2070 step, loss is 4.085350513458252\n",
      "At 2080 step, loss is 86.30609893798828\n",
      "At 2090 step, loss is 6.9092326164245605\n",
      "At 2100 step, loss is 0.04507230222225189\n",
      "At 2110 step, loss is 0.017807869240641594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 2120 step, loss is 0.03785083070397377\n",
      "At 2130 step, loss is 9.975861549377441\n",
      "At 2140 step, loss is 1.2739039902953664e-06\n",
      "At 2150 step, loss is 0.002740177558735013\n",
      "At 2160 step, loss is 0.08992087841033936\n",
      "At 2170 step, loss is 0.03635121136903763\n",
      "At 2180 step, loss is 0.0018364917486906052\n",
      "At 2190 step, loss is 1.102506399154663\n",
      "At 2200 step, loss is 0.00027904228772968054\n",
      "At 2210 step, loss is 150.62191772460938\n",
      "At 2220 step, loss is 0.19890396296977997\n",
      "At 2230 step, loss is 38.705772399902344\n",
      "At 2240 step, loss is 3.1472389698028564\n",
      "At 2250 step, loss is 0.10828712582588196\n",
      "At 2260 step, loss is 0.40764233469963074\n",
      "At 2270 step, loss is 0.005912730935961008\n",
      "At 2280 step, loss is 0.08629259467124939\n",
      "At 2290 step, loss is 0.1868918240070343\n",
      "At 2300 step, loss is 1.0438288450241089\n",
      "At 2310 step, loss is 0.26868340373039246\n",
      "At 2320 step, loss is 0.0047756037674844265\n",
      "At 2330 step, loss is 4.67479133605957\n",
      "At 2340 step, loss is 0.04753933101892471\n",
      "At 2350 step, loss is 0.10605446994304657\n",
      "At 2360 step, loss is 22.313138961791992\n",
      "At 2370 step, loss is 0.42584389448165894\n",
      "At 2380 step, loss is 27.99831771850586\n",
      "At 2390 step, loss is 9.773775100708008\n",
      "At 2400 step, loss is 0.1491730511188507\n",
      "At 2410 step, loss is 2.217752456665039\n",
      "At 2420 step, loss is 0.12479215115308762\n",
      "At 2430 step, loss is 0.053208086639642715\n",
      "At 2440 step, loss is 48.151878356933594\n",
      "At 2450 step, loss is 3.1291351318359375\n",
      "At 2460 step, loss is 0.0002868544834200293\n",
      "At 2470 step, loss is 2.1577351093292236\n",
      "At 2480 step, loss is 0.044340670108795166\n",
      "At 2490 step, loss is 0.1674647033214569\n",
      "At 2500 step, loss is 31.460594177246094\n",
      "At 2510 step, loss is 0.10622002929449081\n",
      "At 2520 step, loss is 0.00013576615310739726\n",
      "At 2530 step, loss is 25.21225929260254\n",
      "At 2540 step, loss is 0.004208472557365894\n",
      "At 2550 step, loss is 0.32692769169807434\n",
      "At 2560 step, loss is 0.16411691904067993\n",
      "At 2570 step, loss is 0.6876940727233887\n",
      "At 2580 step, loss is 0.0009583200444467366\n",
      "At 2590 step, loss is 0.03399547562003136\n",
      "At 2600 step, loss is 7.478757381439209\n",
      "At 2610 step, loss is 0.47484472393989563\n",
      "At 2620 step, loss is 9.837138175964355\n",
      "At 2630 step, loss is 0.5971564054489136\n",
      "At 2640 step, loss is 0.107840895652771\n",
      "At 2650 step, loss is 4.1598687171936035\n",
      "At 2660 step, loss is 0.028046313673257828\n",
      "At 2670 step, loss is 21.590030670166016\n",
      "At 2680 step, loss is 27.826969146728516\n",
      "At 2690 step, loss is 0.9331610202789307\n",
      "At 2700 step, loss is 13.021998405456543\n",
      "At 2710 step, loss is 104.91404724121094\n",
      "At 2720 step, loss is 0.5492148399353027\n",
      "At 2730 step, loss is 1.327158808708191\n",
      "At 2740 step, loss is 0.06637904793024063\n",
      "At 2750 step, loss is 2.2374062538146973\n",
      "At 2760 step, loss is 0.00618465943261981\n",
      "At 2770 step, loss is 1.3362349271774292\n",
      "At 2780 step, loss is 0.11323235929012299\n",
      "At 2790 step, loss is 0.0022661034017801285\n",
      "At 2800 step, loss is 42.54209899902344\n",
      "At 2810 step, loss is 3.0285484790802\n",
      "At 2820 step, loss is 1.0768414735794067\n",
      "At 2830 step, loss is 5.806904315948486\n",
      "At 2840 step, loss is 44.31315612792969\n",
      "At 2850 step, loss is 0.06565368920564651\n",
      "At 2860 step, loss is 0.0173119455575943\n",
      "At 2870 step, loss is 5.480198860168457\n",
      "At 2880 step, loss is 0.013466697186231613\n",
      "At 2890 step, loss is 0.8313451409339905\n",
      "At 2900 step, loss is 0.028854811564087868\n",
      "At 2910 step, loss is 0.059605080634355545\n",
      "At 2920 step, loss is 0.001360399415716529\n",
      "At 2930 step, loss is 0.2755640149116516\n",
      "At 2940 step, loss is 0.04450207203626633\n",
      "At 2950 step, loss is 0.8062956929206848\n",
      "At 2960 step, loss is 0.13912875950336456\n",
      "At 2970 step, loss is 0.1323012113571167\n",
      "At 2980 step, loss is 2.5306122303009033\n",
      "At 2990 step, loss is 119.60220336914062\n",
      "At 3000 step, loss is 0.0019370833178982139\n",
      "At 3010 step, loss is 0.0914207398891449\n",
      "At 3020 step, loss is 0.9354182481765747\n",
      "At 3030 step, loss is 0.02292473055422306\n",
      "At 3040 step, loss is 11.578824043273926\n",
      "At 3050 step, loss is 0.0717267245054245\n",
      "At 3060 step, loss is 0.030100388452410698\n",
      "At 3070 step, loss is 2.884392499923706\n",
      "At 3080 step, loss is 5.9115681648254395\n",
      "At 3090 step, loss is 1.7486852407455444\n",
      "At 3100 step, loss is 22.14190673828125\n",
      "At 3110 step, loss is 1.8521991968154907\n",
      "At 3120 step, loss is 43.29228210449219\n",
      "At 3130 step, loss is 0.1340196579694748\n",
      "At 3140 step, loss is 0.0486869290471077\n",
      "At 3150 step, loss is 20.090259552001953\n",
      "At 3160 step, loss is 1.6237374544143677\n",
      "At 3170 step, loss is 3.1794486045837402\n",
      "At 3180 step, loss is 0.7156267762184143\n",
      "At 3190 step, loss is 0.3180563449859619\n",
      "At 3200 step, loss is 197.63870239257812\n",
      "At 3210 step, loss is 2.9606025218963623\n",
      "At 3220 step, loss is 0.2478654831647873\n",
      "At 3230 step, loss is 0.20624200999736786\n",
      "At 3240 step, loss is 0.04201343655586243\n",
      "At 3250 step, loss is 0.499673068523407\n",
      "At 3260 step, loss is 1.2041312456130981\n",
      "At 3270 step, loss is 2.007437229156494\n",
      "At 3280 step, loss is 2.8233752250671387\n",
      "At 3290 step, loss is 0.041617944836616516\n",
      "At 3300 step, loss is 0.007177016697824001\n",
      "At 3310 step, loss is 0.058564625680446625\n",
      "At 3320 step, loss is 17.472402572631836\n",
      "At 3330 step, loss is 5.944372653961182\n",
      "At 3340 step, loss is 0.08282215893268585\n",
      "At 3350 step, loss is 0.4193779528141022\n",
      "At 3360 step, loss is 0.3353271484375\n",
      "At 3370 step, loss is 0.45851898193359375\n",
      "At 3380 step, loss is 25.909276962280273\n",
      "At 3390 step, loss is 0.08960758149623871\n",
      "At 3400 step, loss is 0.003044708864763379\n",
      "At 3410 step, loss is 0.007622528821229935\n",
      "At 3420 step, loss is 0.16545620560646057\n",
      "At 3430 step, loss is 0.4161836802959442\n",
      "At 3440 step, loss is 0.0707126259803772\n",
      "At 3450 step, loss is 0.02963661216199398\n",
      "At 3460 step, loss is 0.019921302795410156\n",
      "At 3470 step, loss is 30.616294860839844\n",
      "At 3480 step, loss is 0.05443352088332176\n",
      "At 3490 step, loss is 0.04860345274209976\n",
      "At 3500 step, loss is 0.04470212012529373\n",
      "At 3510 step, loss is 1.6128623485565186\n",
      "At 3520 step, loss is 0.09089920669794083\n",
      "At 3530 step, loss is 0.6064552068710327\n",
      "At 3540 step, loss is 0.08090484887361526\n",
      "At 3550 step, loss is 0.8001925349235535\n",
      "At 3560 step, loss is 0.04394075646996498\n",
      "At 3570 step, loss is 0.18739163875579834\n",
      "At 3580 step, loss is 1.188493251800537\n",
      "At 3590 step, loss is 1.7282733917236328\n",
      "At 3600 step, loss is 45.88488006591797\n",
      "At 3610 step, loss is 0.04338368400931358\n",
      "At 3620 step, loss is 0.6870169043540955\n",
      "At 3630 step, loss is 1.8382703065872192\n",
      "At 3640 step, loss is 3.9410924911499023\n",
      "At 3650 step, loss is 0.8585247993469238\n",
      "At 3660 step, loss is 0.007661796174943447\n",
      "At 3670 step, loss is 4.830163955688477\n",
      "At 3680 step, loss is 2.6859800815582275\n",
      "At 3690 step, loss is 0.01963241957128048\n",
      "At 3700 step, loss is 0.2491195797920227\n",
      "At 3710 step, loss is 0.009182879701256752\n",
      "At 3720 step, loss is 12.985613822937012\n",
      "At 3730 step, loss is 4.2067131996154785\n",
      "At 3740 step, loss is 146.71873474121094\n",
      "At 3750 step, loss is 0.0007026378880254924\n",
      "At 3760 step, loss is 0.02164345234632492\n",
      "At 3770 step, loss is 14.379110336303711\n",
      "At 3780 step, loss is 3.553067684173584\n",
      "At 3790 step, loss is 11.983238220214844\n",
      "At 3800 step, loss is 0.015665937215089798\n",
      "At 3810 step, loss is 0.9650159478187561\n",
      "At 3820 step, loss is 1.7646254301071167\n",
      "At 3830 step, loss is 0.5090346336364746\n",
      "At 3840 step, loss is 0.041985876858234406\n",
      "At 3850 step, loss is 0.016427308320999146\n",
      "At 3860 step, loss is 1.2660069465637207\n",
      "At 3870 step, loss is 2.7157931327819824\n",
      "At 3880 step, loss is 1.0455073118209839\n",
      "At 3890 step, loss is 286.3692321777344\n",
      "At 3900 step, loss is 2.429687261581421\n",
      "At 3910 step, loss is 77.4289779663086\n",
      "At 3920 step, loss is 0.2582969665527344\n",
      "At 3930 step, loss is 8.031521797180176\n",
      "At 3940 step, loss is 0.007693027146160603\n",
      "At 3950 step, loss is 0.10243704169988632\n",
      "At 3960 step, loss is 0.17101208865642548\n",
      "At 3970 step, loss is 0.832451581954956\n",
      "At 3980 step, loss is 0.011296217329800129\n",
      "At 3990 step, loss is 0.15932558476924896\n",
      "At 4000 step, loss is 0.00856814719736576\n",
      "At 4010 step, loss is 0.22235961258411407\n",
      "At 4020 step, loss is 10.829378128051758\n",
      "At 4030 step, loss is 0.8772181868553162\n",
      "At 4040 step, loss is 4.713375568389893\n",
      "At 4050 step, loss is 22.54998016357422\n",
      "At 4060 step, loss is 0.010153667069971561\n",
      "At 4070 step, loss is 0.5154917240142822\n",
      "At 4080 step, loss is 0.2106456458568573\n",
      "At 4090 step, loss is 0.1499093472957611\n",
      "At 4100 step, loss is 1.3012081384658813\n",
      "At 4110 step, loss is 9.018085479736328\n",
      "At 4120 step, loss is 0.49485963582992554\n",
      "At 4130 step, loss is 1.1981488466262817\n",
      "At 4140 step, loss is 6.758128643035889\n",
      "At 4150 step, loss is 40.15713119506836\n",
      "At 4160 step, loss is 0.19069094955921173\n",
      "At 4170 step, loss is 0.2850489914417267\n",
      "At 4180 step, loss is 0.029274798929691315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 4190 step, loss is 0.02814464271068573\n",
      "At 4200 step, loss is 0.0018237322801724076\n",
      "At 4210 step, loss is 0.01735224947333336\n",
      "At 4220 step, loss is 0.008899017237126827\n",
      "At 4230 step, loss is 0.05811050906777382\n",
      "At 4240 step, loss is 0.25737059116363525\n",
      "At 4250 step, loss is 0.2707849442958832\n",
      "At 4260 step, loss is 24.489404678344727\n",
      "At 4270 step, loss is 2.171076536178589\n",
      "At 4280 step, loss is 13.68031120300293\n",
      "At 4290 step, loss is 9.076925277709961\n",
      "At 4300 step, loss is 9.004298210144043\n",
      "At 4310 step, loss is 0.5686593651771545\n",
      "At 4320 step, loss is 1.4852678775787354\n",
      "At 4330 step, loss is 0.048698924481868744\n",
      "At 4340 step, loss is 0.4572308361530304\n",
      "At 4350 step, loss is 0.026987172663211823\n",
      "At 4360 step, loss is 0.022361893206834793\n",
      "At 4370 step, loss is 0.044127658009529114\n",
      "At 4380 step, loss is 0.04584018886089325\n",
      "At 4390 step, loss is 117.04730224609375\n",
      "At 4400 step, loss is 0.09831838309764862\n",
      "At 4410 step, loss is 0.0056617953814566135\n",
      "At 4420 step, loss is 0.0029950106982141733\n",
      "At 4430 step, loss is 0.07979589700698853\n",
      "At 4440 step, loss is 0.0071520330384373665\n",
      "At 4450 step, loss is 0.008768867701292038\n",
      "At 4460 step, loss is 2.796558141708374\n",
      "At 4470 step, loss is 57.048118591308594\n",
      "At 4480 step, loss is 1.4531787633895874\n",
      "At 4490 step, loss is 15.87612533569336\n",
      "At 4500 step, loss is 0.003115555038675666\n",
      "At 4510 step, loss is 26.230154037475586\n",
      "At 4520 step, loss is 17.653499603271484\n",
      "At 4530 step, loss is 0.0213780477643013\n",
      "At 4540 step, loss is 0.02136947400867939\n",
      "At 4550 step, loss is 1.931464672088623\n",
      "At 4560 step, loss is 2.280855417251587\n",
      "At 4570 step, loss is 0.0007412275299429893\n",
      "At 4580 step, loss is 0.01585814170539379\n",
      "At 4590 step, loss is 0.0929703637957573\n",
      "At 4600 step, loss is 3.208611488342285\n",
      "At 4610 step, loss is 0.017059950157999992\n",
      "At 4620 step, loss is 0.3074328303337097\n",
      "At 4630 step, loss is 0.0019169675651937723\n",
      "At 4640 step, loss is 0.011168109253048897\n",
      "At 4650 step, loss is 1.4902325868606567\n",
      "At 4660 step, loss is 0.028814811259508133\n",
      "At 4670 step, loss is 0.3076443672180176\n",
      "At 4680 step, loss is 0.11832951009273529\n",
      "At 4690 step, loss is 0.12322347611188889\n",
      "At 4700 step, loss is 0.39061546325683594\n",
      "At 4710 step, loss is 0.2446092814207077\n",
      "At 4720 step, loss is 0.064519003033638\n",
      "At 4730 step, loss is 0.0398491695523262\n",
      "At 4740 step, loss is 0.16653014719486237\n",
      "At 4750 step, loss is 8.382728576660156\n",
      "At 4760 step, loss is 0.38911309838294983\n",
      "At 4770 step, loss is 0.0065400442108511925\n",
      "At 4780 step, loss is 22.180110931396484\n",
      "At 4790 step, loss is 2.4325809478759766\n",
      "At 4800 step, loss is 0.03864854574203491\n",
      "At 4810 step, loss is 0.00368225434795022\n",
      "At 4820 step, loss is 2.531904935836792\n",
      "At 4830 step, loss is 0.012427523732185364\n",
      "At 4840 step, loss is 0.0928134098649025\n",
      "At 4850 step, loss is 9.167165756225586\n",
      "At 4860 step, loss is 0.0031512384302914143\n",
      "At 4870 step, loss is 0.0520121268928051\n",
      "At 4880 step, loss is 96.78634643554688\n",
      "At 4890 step, loss is 0.012583448551595211\n",
      "At 4900 step, loss is 0.01449626311659813\n",
      "At 4910 step, loss is 0.06551454216241837\n",
      "At 4920 step, loss is 1.384081244468689\n",
      "At 4930 step, loss is 0.7109005451202393\n",
      "At 4940 step, loss is 0.056064169853925705\n",
      "At 4950 step, loss is 0.007190606091171503\n",
      "At 4960 step, loss is 0.016773639246821404\n",
      "At 4970 step, loss is 0.8131179809570312\n",
      "At 4980 step, loss is 2.7534024715423584\n",
      "At 4990 step, loss is 5.6843418860808015e-12\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hyperparameters:\n",
    "loss_array =[]\n",
    "days = 100 #control number of days available to the neural network\n",
    "\n",
    "batch_size = 100 #in huge datasets it is customary to use batch size and updating parameters each batch until it covers the whole \n",
    "#dataset each epoch. It is also customary to use powers of 2 for the batch size.\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_epoch = 5000 #an epoch ends when the whole training set is covered.\n",
    "\n",
    "input_size = days*4 #I hate tensors\n",
    "\n",
    "dimensions = {\"L0\": input_size, \"L1\": 50, \"L2\": 10, \"L3\":10, \"L4\":1} #number of nodes in each layer, L4 is the output. 10 IS FIXED\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, dimensions):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.L1 = nn.Linear(dimensions['L0'], dimensions['L1'])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.L2 = nn.Linear(dimensions['L1'], dimensions['L2'])\n",
    "        self.L3 = nn.Linear(dimensions['L2'], dimensions['L3'])\n",
    "        self.L4 = nn.Linear(dimensions['L3'], dimensions['L4'])\n",
    "    def forward(self, x): #x is the input\n",
    "        #3 layers, each with relu except the last layer.\n",
    "        x = self.L1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.L2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.L3(x)\n",
    "        x= self.relu(x)\n",
    "        y_predict = self.L4(x)\n",
    "        return y_predict\n",
    "model = NeuralNetwork(dimensions)\n",
    "\n",
    "lossf = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "    stock_vector, next_day_row = stock_vector_generator(data,days)\n",
    "    stock_tensor = torch.from_numpy(stock_vector).float().T  #transpose because pytorch is shit\n",
    "    actual = torch.from_numpy(np.array([float(next_day_row[1][1:])])).float()\n",
    "    \n",
    "    y_predict = model(stock_tensor) #forward prop\n",
    "        \n",
    "    loss = lossf(y_predict, actual)\n",
    "    loss_array.append(loss)\n",
    "    loss.backward() #backprop\n",
    "        \n",
    "    optimizer.step() #update parameters\n",
    "    optimizer.zero_grad() #resets the gradients\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'At {epoch} step, loss is {loss}')\n",
    "with torch.no_grad():\n",
    "    \n",
    "    plt.plot(np.linspace(0,n_epoch-1,num=n_epoch), loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "949aa00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted on 07/07/2020 : \n",
      "9.289903\n",
      "Actual was: \n",
      "9.8718\n",
      "5.894541146774109%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    stock_vector, next_day_row = stock_vector_generator(data,days)\n",
    "    stock_tensor = torch.from_numpy(stock_vector).float().T \n",
    "    predicted = model(stock_tensor)\n",
    "    predicted = predicted.numpy()[0][0]\n",
    "    test_actual = float(next_day_row[1][1:])\n",
    "    day = next_day_row[0]\n",
    "    print('Predicted on ' + day + ' : ')\n",
    "    print(predicted)\n",
    "    print('Actual was: ')\n",
    "    print(test_actual)\n",
    "    diff = 100*((test_actual - predicted)**2)**0.5/test_actual \n",
    "    print(str(diff) + '%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
