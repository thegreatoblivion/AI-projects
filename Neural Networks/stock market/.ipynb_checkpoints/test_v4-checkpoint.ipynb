{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60b9057",
   "metadata": {},
   "source": [
    "v2: REMOVED high. low and open. ONLY close/last.\n",
    "v3: using batch and epoch as they are supposed to, cycling through the whole dataset first.\n",
    "0 = close/last\n",
    "objective: guess the next closing value in the next day\n",
    "if days is set to n, this means that n-1 values are inputted into neural network while 1 is used for the test. This is to prevent an error I encountered.\n",
    "v4: output is now 2 numbers, either going up or going down. index 0 is down and index 1 is up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4aae16aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "6\n",
      "9\n",
      "8\n",
      "2\n",
      "4\n",
      "0\n",
      "7\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def random_exhaust(batch_size):\n",
    "    \n",
    "\n",
    "    batch_pile = []\n",
    "    for batch in range(batch_size):\n",
    "        batch_pile.append(batch)\n",
    "    for batch in range(batch_size):\n",
    "        randbatch = batch_pile[random.randrange(0, len(batch_pile))]\n",
    "\n",
    "\n",
    "        batch_pile.remove(randbatch)\n",
    "        print(randbatch)\n",
    "random_exhaust(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "558edc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4924])\n"
     ]
    }
   ],
   "source": [
    "actual = torch.from_numpy(np.array([float(next_day_matrix[2][1][1:])])).float()\n",
    "percentage_change = percentage(prev_day_vector[2], float(next_day_matrix[2][1][1:] ))\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6acd1ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Date' 'Close/Last' 'Volume' 'Open' 'High' 'Low']\n",
      " ['08/16/2024' '$124.58' '302589900' '$121.94' '$125.00' '$121.18']\n",
      " ['08/15/2024' '$122.86' '318086700' '$118.76' '$123.24' '$117.47']\n",
      " ...\n",
      " ['08/21/2014' '$0.4768' '272795280' '$0.48' '$0.4825' '$0.476']\n",
      " ['08/20/2014' '$0.4813' '221469840' '$0.4825' '$0.4848' '$0.4801']\n",
      " ['08/19/2014' '$0.4843' '248259960' '$0.4805' '$0.4868' '$0.4788']]\n",
      "(2517, 6)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "\n",
    "with open(r'C:\\Users\\user\\Documents\\road to qm\\machine learning\\pytorch\\stock market\\NVDA_10.csv', newline='') as csvfile:\n",
    "    data = list(csv.reader(csvfile))\n",
    "data = np.array(data)\n",
    "#lookback_range = 1000\n",
    "#data = data[:lookback_range,:]\n",
    "num_days = data.shape[0]-1\n",
    "\n",
    "#print(num_days)\n",
    "print(data)\n",
    "print(data.shape)\n",
    "def percentage(original, change):\n",
    "    percent = (change - original)/actual\n",
    "    return percent\n",
    "def normalize(n):\n",
    "    n = 1/(1+np.exp(-n))\n",
    "    return n\n",
    "def stock_matrix_generator(data, days, batch_size):  #generates a matrix of stock prices with non-repeating batches\n",
    "    data = np.delete(data,0,axis = 0) #deletes the top row\n",
    "    upper_range = np.random.randint(num_days - days, num_days) #gives index between  num_days and num_Days - days as a starting point.\n",
    "    #this selects the starting point for the batch to cycle over\n",
    "    stock_matrix = []\n",
    "    next_day_matrix = []\n",
    "    prev_day_vector = []\n",
    "    lower_range = upper_range - days\n",
    "    #closing value the next day - this will be our test data\n",
    "    \n",
    "    \n",
    "   \n",
    "    #slicing data into small chunks and removing unneccessary rows and columns\n",
    "    for i in range(batch_size):\n",
    "       \n",
    "        \n",
    "        \n",
    "        next_day_row = data[lower_range-(i*days)-1]\n",
    "        next_day_row = next_day_row[:2]\n",
    "        #print('next day row')\n",
    "        #print(next_day_row)\n",
    "        next_day_matrix.append(next_day_row)\n",
    "\n",
    "        #print('next day matrix')\n",
    "        #print(next_day_matrix)\n",
    "        stock_array = data[lower_range-(i*days):upper_range-(i*days)-1,:]\n",
    "        stock_array = stock_array[:,1] #only need closing price\n",
    "\n",
    "        #now to get rid of the dollar sign and change all into floats\n",
    "        for i in range(len(stock_array)):\n",
    "            price = stock_array[i]\n",
    "            price = price[1:]\n",
    "            stock_array[i] = price\n",
    "        stock_array = np.array([list(map(float,stock_array))])\n",
    "        prev_day_vector.append(stock_array[0][0])\n",
    "        #now turn into a single lined vector\n",
    "        stock_vector = stock_array.reshape(days-1,1)\n",
    "        stock_matrix.append(stock_vector)\n",
    "        \n",
    "        \n",
    "    return stock_matrix, next_day_matrix, prev_day_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8c90a4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "[[[  0.477 ]\n",
      "  [  0.486 ]\n",
      "  [  0.4788]\n",
      "  ...\n",
      "  [  0.4808]\n",
      "  [  0.4864]\n",
      "  [  0.4778]]\n",
      "\n",
      " [[  0.4355]\n",
      "  [  0.4363]\n",
      "  [  0.436 ]\n",
      "  ...\n",
      "  [  0.4628]\n",
      "  [  0.473 ]\n",
      "  [  0.4703]]\n",
      "\n",
      " [[  0.4946]\n",
      "  [  0.4888]\n",
      "  [  0.4911]\n",
      "  ...\n",
      "  [  0.4571]\n",
      "  [  0.4473]\n",
      "  [  0.458 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 85.817 ]\n",
      "  [ 83.041 ]\n",
      "  [ 86.402 ]\n",
      "  ...\n",
      "  [ 87.039 ]\n",
      "  [ 85.354 ]\n",
      "  [ 87.133 ]]\n",
      "\n",
      " [[109.633 ]\n",
      "  [110.5   ]\n",
      "  [114.825 ]\n",
      "  ...\n",
      "  [ 90.412 ]\n",
      "  [ 90.554 ]\n",
      "  [ 92.14  ]]\n",
      "\n",
      " [[124.3   ]\n",
      "  [123.54  ]\n",
      "  [123.99  ]\n",
      "  ...\n",
      "  [120.998 ]\n",
      "  [122.44  ]\n",
      "  [116.437 ]]]\n"
     ]
    }
   ],
   "source": [
    "#initializing\n",
    "days = 20\n",
    "batch_size = math.floor(num_days/days)-1\n",
    "print(batch_size)\n",
    "\n",
    "stock_matrix, next_day_matrix, prev_day_vector = stock_matrix_generator(data,days, batch_size)\n",
    "stock_matrix = np.array(stock_matrix)\n",
    "next_day_matrix = np.array(next_day_matrix)\n",
    "\n",
    "print(stock_matrix)\n",
    "#stock_tensor = torch.from_numpy(stock_vector)\n",
    "#actual = torch.from_numpy(np.array([float(next_day_row[1][1:])])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57187e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "At 0 step, loss is 0.12498526275157928\n",
      "At 10 step, loss is 0.48565366864204407\n",
      "At 20 step, loss is 0.0007044493104331195\n",
      "At 30 step, loss is 0.002978474134579301\n",
      "At 40 step, loss is 0.005498481448739767\n",
      "At 50 step, loss is 0.0039300983771681786\n",
      "At 60 step, loss is 0.0027159033343195915\n",
      "At 70 step, loss is 0.001782830455340445\n",
      "At 80 step, loss is 0.0012589562684297562\n",
      "At 90 step, loss is 0.0004063430242240429\n",
      "At 100 step, loss is 0.004612843040376902\n",
      "At 110 step, loss is 0.00040787970647215843\n",
      "At 120 step, loss is 0.0005684556672349572\n",
      "At 130 step, loss is 0.000408485415391624\n",
      "At 140 step, loss is 0.0002427202125545591\n",
      "At 150 step, loss is 0.004276670515537262\n",
      "At 160 step, loss is 7.770057709421962e-05\n",
      "At 170 step, loss is 0.00013793300604447722\n",
      "At 180 step, loss is 0.0009748710435815156\n",
      "At 190 step, loss is 0.0005642902688123286\n",
      "At 200 step, loss is 8.878293738234788e-05\n",
      "At 210 step, loss is 9.190692799165845e-05\n",
      "At 220 step, loss is 0.0003303028643131256\n",
      "At 230 step, loss is 3.562185156624764e-05\n",
      "At 240 step, loss is 0.0004451911081559956\n",
      "Epoch 1\n",
      "At 0 step, loss is 9.215228601533454e-06\n",
      "At 10 step, loss is 2.436789372950443e-06\n",
      "At 20 step, loss is 0.00010876039596041664\n",
      "At 30 step, loss is 0.0010078155901283026\n",
      "At 40 step, loss is 0.0004369909584056586\n",
      "At 50 step, loss is 0.00021282820671331137\n",
      "At 60 step, loss is 0.0004956537741236389\n",
      "At 70 step, loss is 0.0006020054570399225\n",
      "At 80 step, loss is 0.0001773268450051546\n",
      "At 90 step, loss is 0.00017442196258343756\n",
      "At 100 step, loss is 8.353374141734093e-06\n",
      "At 110 step, loss is 2.9778237149002962e-05\n",
      "At 120 step, loss is 0.00029379272018559277\n",
      "At 130 step, loss is 8.875203639036044e-05\n",
      "At 140 step, loss is 0.000325622531818226\n",
      "At 150 step, loss is 0.0531935840845108\n",
      "At 160 step, loss is 0.0016228703316301107\n",
      "At 170 step, loss is 1.604962017154321e-05\n",
      "At 180 step, loss is 0.000713400891982019\n",
      "At 190 step, loss is 2.04202387976693e-06\n",
      "At 200 step, loss is 0.00010304582247044891\n",
      "At 210 step, loss is 0.0001323572505498305\n",
      "At 220 step, loss is 0.0001206126544275321\n",
      "At 230 step, loss is 2.674192910490092e-05\n",
      "At 240 step, loss is 0.0005194907425902784\n",
      "Epoch 2\n",
      "At 0 step, loss is 0.00027519994182512164\n",
      "At 10 step, loss is 0.001332403626292944\n",
      "At 20 step, loss is 0.00027881423011422157\n",
      "At 30 step, loss is 0.004796127323061228\n",
      "At 40 step, loss is 0.001569772488437593\n",
      "At 50 step, loss is 2.5171032120852033e-06\n",
      "At 60 step, loss is 2.926290108007379e-06\n",
      "At 70 step, loss is 6.783719436498359e-05\n",
      "At 80 step, loss is 7.257862307596952e-05\n",
      "At 90 step, loss is 1.034858632920077e-05\n",
      "At 100 step, loss is 0.00038847510586492717\n",
      "At 110 step, loss is 0.005468585062772036\n",
      "At 120 step, loss is 6.053959805285558e-05\n",
      "At 130 step, loss is 0.0018781888065859675\n",
      "At 140 step, loss is 9.452275116927922e-05\n",
      "At 150 step, loss is 8.866201824275777e-05\n",
      "At 160 step, loss is 1.806758177735901e-06\n",
      "At 170 step, loss is 0.00023557392705697566\n",
      "At 180 step, loss is 0.0010920666391029954\n",
      "At 190 step, loss is 0.0007126207347027957\n",
      "At 200 step, loss is 0.007138828281313181\n",
      "At 210 step, loss is 0.007089539896696806\n",
      "At 220 step, loss is 0.0002750359708443284\n",
      "At 230 step, loss is 6.135682633612305e-05\n",
      "At 240 step, loss is 5.791760850115679e-05\n",
      "Epoch 3\n",
      "At 0 step, loss is 0.0024263407103717327\n",
      "At 10 step, loss is 4.4415155571186915e-06\n",
      "At 20 step, loss is 0.0001659430708969012\n",
      "At 30 step, loss is 7.564930456283037e-06\n",
      "At 40 step, loss is 0.004238440655171871\n",
      "At 50 step, loss is 0.00027243257500231266\n",
      "At 60 step, loss is 5.806664557894692e-05\n",
      "At 70 step, loss is 0.0002503743744455278\n",
      "At 80 step, loss is 0.00883504655212164\n",
      "At 90 step, loss is 0.00514321168884635\n",
      "At 100 step, loss is 0.0022882234770804644\n",
      "At 110 step, loss is 0.0009913903195410967\n",
      "At 120 step, loss is 0.0003413333324715495\n",
      "At 130 step, loss is 0.0030396839138120413\n",
      "At 140 step, loss is 0.0019364802865311503\n",
      "At 150 step, loss is 0.0006864892202429473\n",
      "At 160 step, loss is 6.672969448118238e-07\n",
      "At 170 step, loss is 0.001496013137511909\n",
      "At 180 step, loss is 0.005884786602109671\n",
      "At 190 step, loss is 0.00034943813807331026\n",
      "At 200 step, loss is 0.00013109759311191738\n",
      "At 210 step, loss is 4.1845141822705045e-05\n",
      "At 220 step, loss is 0.003611436113715172\n",
      "At 230 step, loss is 0.0010485873790457845\n",
      "At 240 step, loss is 0.0002403194084763527\n",
      "Epoch 4\n",
      "At 0 step, loss is 0.00022478018945548683\n",
      "At 10 step, loss is 0.0056356326676905155\n",
      "At 20 step, loss is 0.00028816817211918533\n",
      "At 30 step, loss is 1.050188802764751e-05\n",
      "At 40 step, loss is 4.680089659814257e-06\n",
      "At 50 step, loss is 0.0018770727328956127\n",
      "At 60 step, loss is 0.002111797919496894\n",
      "At 70 step, loss is 0.0009395573870278895\n",
      "At 80 step, loss is 0.00026148525648750365\n",
      "At 90 step, loss is 0.00018001269199885428\n",
      "At 100 step, loss is 9.904455033904469e-11\n",
      "At 110 step, loss is 9.110033715842292e-06\n",
      "At 120 step, loss is 0.00014713504060637206\n",
      "At 130 step, loss is 0.006854615174233913\n",
      "At 140 step, loss is 0.006583789829164743\n",
      "At 150 step, loss is 1.6795176634332165e-05\n",
      "At 160 step, loss is 1.886497193481773e-05\n",
      "At 170 step, loss is 0.0005071699270047247\n",
      "At 180 step, loss is 0.0009134981082752347\n",
      "At 190 step, loss is 7.178128726081923e-05\n",
      "At 200 step, loss is 0.008380122482776642\n",
      "At 210 step, loss is 0.00800387654453516\n",
      "At 220 step, loss is 4.546531727100955e-06\n",
      "At 230 step, loss is 4.138938311371021e-05\n",
      "At 240 step, loss is 0.008487703278660774\n",
      "Epoch 5\n",
      "At 0 step, loss is 0.00011273121344856918\n",
      "At 10 step, loss is 6.169297557789832e-05\n",
      "At 20 step, loss is 6.103831401560456e-05\n",
      "At 30 step, loss is 0.0006516066496260464\n",
      "At 40 step, loss is 0.005375501234084368\n",
      "At 50 step, loss is 0.00012149913527537137\n",
      "At 60 step, loss is 0.00018700759392231703\n",
      "At 70 step, loss is 0.0013188897864893079\n",
      "At 80 step, loss is 0.00039850620669312775\n",
      "At 90 step, loss is 0.0011634337715804577\n",
      "At 100 step, loss is 5.686892450285086e-07\n",
      "At 110 step, loss is 0.0002355855394853279\n",
      "At 120 step, loss is 2.6586882086121477e-05\n",
      "At 130 step, loss is 0.00036373434704728425\n",
      "At 140 step, loss is 5.4931497288635e-05\n",
      "At 150 step, loss is 0.0033869370818138123\n",
      "At 160 step, loss is 0.001060547772794962\n",
      "At 170 step, loss is 0.0024672255385667086\n",
      "At 180 step, loss is 0.0004576332285068929\n",
      "At 190 step, loss is 0.00010294259118381888\n",
      "At 200 step, loss is 0.0002469504834152758\n",
      "At 210 step, loss is 7.008557440713048e-05\n",
      "At 220 step, loss is 0.0008578645065426826\n",
      "At 230 step, loss is 0.002513457788154483\n",
      "At 240 step, loss is 4.717520369013073e-06\n",
      "Epoch 6\n",
      "At 0 step, loss is 0.0001435019075870514\n",
      "At 10 step, loss is 7.767445822537411e-06\n",
      "At 20 step, loss is 7.918894698377699e-05\n",
      "At 30 step, loss is 0.00015815981896594167\n",
      "At 40 step, loss is 0.0007201867992989719\n",
      "At 50 step, loss is 5.5255195547943e-06\n",
      "At 60 step, loss is 0.00018112221732735634\n",
      "At 70 step, loss is 4.139325938012917e-06\n",
      "At 80 step, loss is 3.5047072742599994e-05\n",
      "At 90 step, loss is 0.0007804265478625894\n",
      "At 100 step, loss is 0.001284108147956431\n",
      "At 110 step, loss is 0.0009476948762312531\n",
      "At 120 step, loss is 0.000689546694047749\n",
      "At 130 step, loss is 0.0004985721316188574\n",
      "At 140 step, loss is 6.644334644079208e-05\n",
      "At 150 step, loss is 0.0015813716454431415\n",
      "At 160 step, loss is 0.0003474379482213408\n",
      "At 170 step, loss is 0.0021363894920796156\n",
      "At 180 step, loss is 2.9527227525250055e-05\n",
      "At 190 step, loss is 3.062796895392239e-05\n",
      "At 200 step, loss is 0.000302922329865396\n",
      "At 210 step, loss is 0.0005917110829614103\n",
      "At 220 step, loss is 0.0030172981787472963\n",
      "At 230 step, loss is 0.0011334667215123773\n",
      "At 240 step, loss is 0.00012407680333126336\n",
      "Epoch 7\n",
      "At 0 step, loss is 3.4216598578495905e-05\n",
      "At 10 step, loss is 8.62646265886724e-06\n",
      "At 20 step, loss is 0.00018937313870992512\n",
      "At 30 step, loss is 0.008924398571252823\n",
      "At 40 step, loss is 0.0009262448875233531\n",
      "At 50 step, loss is 0.001714693265967071\n",
      "At 60 step, loss is 6.817149369453546e-07\n",
      "At 70 step, loss is 0.0010874334257096052\n",
      "At 80 step, loss is 0.00027758488431572914\n",
      "At 90 step, loss is 1.1200265362276696e-05\n",
      "At 100 step, loss is 4.4827943384007085e-06\n",
      "At 110 step, loss is 0.0002988524502143264\n",
      "At 120 step, loss is 1.484556560171768e-05\n",
      "At 130 step, loss is 0.0020043824333697557\n",
      "At 140 step, loss is 0.0003408483462408185\n",
      "At 150 step, loss is 0.003654514905065298\n",
      "At 160 step, loss is 0.0011337818577885628\n",
      "At 170 step, loss is 3.395258829641534e-07\n",
      "At 180 step, loss is 1.3869477015759912e-06\n",
      "At 190 step, loss is 0.0002469898317940533\n",
      "At 200 step, loss is 0.00011735753651009873\n",
      "At 210 step, loss is 0.0005059749819338322\n",
      "At 220 step, loss is 0.00026518412050791085\n",
      "At 230 step, loss is 0.0010367436334490776\n",
      "At 240 step, loss is 0.00018634840671438724\n",
      "Epoch 8\n",
      "At 0 step, loss is 0.0004537274071481079\n",
      "At 10 step, loss is 0.00023359675833489746\n",
      "At 20 step, loss is 3.2453422136313748e-06\n",
      "At 30 step, loss is 0.0006742093246430159\n",
      "At 40 step, loss is 2.6056855858769268e-05\n",
      "At 50 step, loss is 0.0023915672209113836\n",
      "At 60 step, loss is 0.000314166332827881\n",
      "At 70 step, loss is 0.00036469948827289045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 80 step, loss is 3.118886525044218e-05\n",
      "At 90 step, loss is 3.273883066867711e-06\n",
      "At 100 step, loss is 0.0022971746511757374\n",
      "At 110 step, loss is 0.0012744908453896642\n",
      "At 120 step, loss is 0.00048231781693175435\n",
      "At 130 step, loss is 0.0013810880482196808\n",
      "At 140 step, loss is 3.381540025770846e-08\n",
      "At 150 step, loss is 5.358568344604464e-08\n",
      "At 160 step, loss is 0.00011125335731776431\n",
      "At 170 step, loss is 3.74497676602914e-06\n",
      "At 180 step, loss is 7.76750675868243e-05\n",
      "At 190 step, loss is 0.00038251926889643073\n",
      "At 200 step, loss is 3.907850259565748e-05\n",
      "At 210 step, loss is 0.0006508998922072351\n",
      "At 220 step, loss is 3.352663213718188e-07\n",
      "At 230 step, loss is 0.0034269788302481174\n",
      "At 240 step, loss is 1.3607146684080362e-05\n",
      "Epoch 9\n",
      "At 0 step, loss is 0.0007650413317605853\n",
      "At 10 step, loss is 0.0008864927804097533\n",
      "At 20 step, loss is 0.008229485712945461\n",
      "At 30 step, loss is 0.0006969673559069633\n",
      "At 40 step, loss is 1.0499195923330262e-05\n",
      "At 50 step, loss is 2.8339993150439113e-05\n",
      "At 60 step, loss is 0.0001568200095789507\n",
      "At 70 step, loss is 0.0017269380623474717\n",
      "At 80 step, loss is 0.0004812856495846063\n",
      "At 90 step, loss is 0.00027503917226567864\n",
      "At 100 step, loss is 0.00263387244194746\n",
      "At 110 step, loss is 9.440555004402995e-05\n",
      "At 120 step, loss is 1.1945756341447122e-05\n",
      "At 130 step, loss is 0.0007383681368082762\n",
      "At 140 step, loss is 0.0007995981140993536\n",
      "At 150 step, loss is 0.00027127316570840776\n",
      "At 160 step, loss is 0.00040234444895759225\n",
      "At 170 step, loss is 0.00015437213005498052\n",
      "At 180 step, loss is 0.0034755468368530273\n",
      "At 190 step, loss is 0.0003007222549058497\n",
      "At 200 step, loss is 0.000431052059866488\n",
      "At 210 step, loss is 0.0027944992762058973\n",
      "At 220 step, loss is 1.2846913705288898e-05\n",
      "At 230 step, loss is 0.00047806010115891695\n",
      "At 240 step, loss is 0.005828435532748699\n",
      "Epoch 10\n",
      "At 0 step, loss is 0.0017668016953393817\n",
      "At 10 step, loss is 0.0033361564856022596\n",
      "At 20 step, loss is 1.5802703273948282e-05\n",
      "At 30 step, loss is 4.013846773887053e-05\n",
      "At 40 step, loss is 0.00028251035837456584\n",
      "At 50 step, loss is 4.4934567995369434e-06\n",
      "At 60 step, loss is 0.00011725319927791134\n",
      "At 70 step, loss is 0.0012415662640705705\n",
      "At 80 step, loss is 0.0005716350860893726\n",
      "At 90 step, loss is 4.834628998651169e-05\n",
      "At 100 step, loss is 0.00036522833397611976\n",
      "At 110 step, loss is 0.00010150446178158745\n",
      "At 120 step, loss is 2.568620038800873e-05\n",
      "At 130 step, loss is 1.0567613571765833e-05\n",
      "At 140 step, loss is 0.00011054226342821494\n",
      "At 150 step, loss is 4.0304894355358556e-05\n",
      "At 160 step, loss is 2.9486198400263675e-05\n",
      "At 170 step, loss is 0.00011292991257505491\n",
      "At 180 step, loss is 6.23925297986716e-05\n",
      "At 190 step, loss is 0.002148465719074011\n",
      "At 200 step, loss is 7.469739671250863e-08\n",
      "At 210 step, loss is 0.00016999566287267953\n",
      "At 220 step, loss is 0.0013875104486942291\n",
      "At 230 step, loss is 0.0025497579481452703\n",
      "At 240 step, loss is 8.217457070713863e-05\n",
      "Epoch 11\n",
      "At 0 step, loss is 4.367138899397105e-05\n",
      "At 10 step, loss is 0.00011652798275463283\n",
      "At 20 step, loss is 0.0011513563804328442\n",
      "At 30 step, loss is 7.83194900577655e-06\n",
      "At 40 step, loss is 0.0001416040468029678\n",
      "At 50 step, loss is 0.0002286387752974406\n",
      "At 60 step, loss is 0.002105198334902525\n",
      "At 70 step, loss is 0.00015276842168532312\n",
      "At 80 step, loss is 0.002080285921692848\n",
      "At 90 step, loss is 0.00029848713893443346\n",
      "At 100 step, loss is 2.816720552800689e-05\n",
      "At 110 step, loss is 0.0005812537856400013\n",
      "At 120 step, loss is 0.0010926338145509362\n",
      "At 130 step, loss is 1.956271444214508e-05\n",
      "At 140 step, loss is 8.15486203009641e-07\n",
      "At 150 step, loss is 3.762626147363335e-05\n",
      "At 160 step, loss is 1.5982903278199956e-05\n",
      "At 170 step, loss is 5.890377906325739e-06\n",
      "At 180 step, loss is 8.162819540302735e-06\n",
      "At 190 step, loss is 0.0005652876570820808\n",
      "At 200 step, loss is 0.0005711729754693806\n",
      "At 210 step, loss is 1.927765151776839e-05\n",
      "At 220 step, loss is 2.6526116926106624e-05\n",
      "At 230 step, loss is 3.584403748391196e-05\n",
      "At 240 step, loss is 4.544265539152548e-05\n",
      "Epoch 12\n",
      "At 0 step, loss is 0.0004922904772683978\n",
      "At 10 step, loss is 0.0005307554383762181\n",
      "At 20 step, loss is 0.00011327413812978193\n",
      "At 30 step, loss is 0.00021017312246840447\n",
      "At 40 step, loss is 0.00014434070908464491\n",
      "At 50 step, loss is 7.639021350769326e-05\n",
      "At 60 step, loss is 1.6561252778046764e-05\n",
      "At 70 step, loss is 0.00012236110342200845\n",
      "At 80 step, loss is 0.00043348176404833794\n",
      "At 90 step, loss is 0.00014843080134596676\n",
      "At 100 step, loss is 1.4807813386141788e-05\n",
      "At 110 step, loss is 0.0002834624610841274\n",
      "At 120 step, loss is 0.00023391594004351646\n",
      "At 130 step, loss is 0.0008243768243119121\n",
      "At 140 step, loss is 2.3870152290328406e-05\n",
      "At 150 step, loss is 0.0017665947088971734\n",
      "At 160 step, loss is 1.2268597856746055e-05\n",
      "At 170 step, loss is 7.571110472781584e-05\n",
      "At 180 step, loss is 1.3266416317492258e-05\n",
      "At 190 step, loss is 0.0008504949510097504\n",
      "At 200 step, loss is 5.556986798183061e-05\n",
      "At 210 step, loss is 0.0015283648390322924\n",
      "At 220 step, loss is 0.0001893693406600505\n",
      "At 230 step, loss is 4.174950390734011e-06\n",
      "At 240 step, loss is 0.00024897907860577106\n",
      "Epoch 13\n",
      "At 0 step, loss is 0.002673831768333912\n",
      "At 10 step, loss is 0.0001799713063519448\n",
      "At 20 step, loss is 0.00022202444961294532\n",
      "At 30 step, loss is 1.8820247760231723e-07\n",
      "At 40 step, loss is 0.001164457411505282\n",
      "At 50 step, loss is 0.0024359626695513725\n",
      "At 60 step, loss is 0.0007058001938275993\n",
      "At 70 step, loss is 1.830969085858669e-05\n",
      "At 80 step, loss is 0.0014701465843245387\n",
      "At 90 step, loss is 0.003171402495354414\n",
      "At 100 step, loss is 2.438977844576584e-06\n",
      "At 110 step, loss is 0.00048070098273456097\n",
      "At 120 step, loss is 0.0007093848544172943\n",
      "At 130 step, loss is 4.485652880248381e-06\n",
      "At 140 step, loss is 0.0007756023551337421\n",
      "At 150 step, loss is 0.0007468596450053155\n",
      "At 160 step, loss is 0.00038180005503818393\n",
      "At 170 step, loss is 0.0006325876456685364\n",
      "At 180 step, loss is 0.00042854747152887285\n",
      "At 190 step, loss is 0.0007230351329781115\n",
      "At 200 step, loss is 0.001192751806229353\n",
      "At 210 step, loss is 6.758747076673899e-06\n",
      "At 220 step, loss is 0.000125078993733041\n",
      "At 230 step, loss is 0.0001675353414611891\n",
      "At 240 step, loss is 0.0012125227367505431\n",
      "Epoch 14\n",
      "At 0 step, loss is 0.0024524242617189884\n",
      "At 10 step, loss is 0.004048327449709177\n",
      "At 20 step, loss is 0.0027379218954592943\n",
      "At 30 step, loss is 0.002506623975932598\n",
      "At 40 step, loss is 6.13603424426401e-06\n",
      "At 50 step, loss is 0.00044729429646395147\n",
      "At 60 step, loss is 0.00017189369827974588\n",
      "At 70 step, loss is 9.468049393035471e-05\n",
      "At 80 step, loss is 0.00022292045468930155\n",
      "At 90 step, loss is 8.231291576521471e-05\n",
      "At 100 step, loss is 0.00010201925033470616\n",
      "At 110 step, loss is 1.843370591814164e-05\n",
      "At 120 step, loss is 3.466219641268253e-05\n",
      "At 130 step, loss is 0.00011748645192710683\n",
      "At 140 step, loss is 0.0005335213500075042\n",
      "At 150 step, loss is 0.0021269102580845356\n",
      "At 160 step, loss is 0.0006788974278606474\n",
      "At 170 step, loss is 5.2968265663366765e-05\n",
      "At 180 step, loss is 0.0003282468533143401\n",
      "At 190 step, loss is 7.148274016799405e-05\n",
      "At 200 step, loss is 0.0005485579604282975\n",
      "At 210 step, loss is 0.0009778696112334728\n",
      "At 220 step, loss is 7.878765586610825e-07\n",
      "At 230 step, loss is 0.0002459257375448942\n",
      "At 240 step, loss is 0.0013010217808187008\n",
      "Epoch 15\n",
      "At 0 step, loss is 1.2139626960561145e-05\n",
      "At 10 step, loss is 0.0002423762925900519\n",
      "At 20 step, loss is 0.0021136526484042406\n",
      "At 30 step, loss is 0.0001844781218096614\n",
      "At 40 step, loss is 2.8230202588019893e-05\n",
      "At 50 step, loss is 5.1308948059158865e-06\n",
      "At 60 step, loss is 5.287192834657617e-05\n",
      "At 70 step, loss is 6.046955968486145e-05\n",
      "At 80 step, loss is 4.982519499208138e-07\n",
      "At 90 step, loss is 0.0003907853097189218\n",
      "At 100 step, loss is 1.4813058442086913e-05\n",
      "At 110 step, loss is 0.003140091197565198\n",
      "At 120 step, loss is 0.0017831324366852641\n",
      "At 130 step, loss is 0.0007189479074440897\n",
      "At 140 step, loss is 6.521599061670713e-06\n",
      "At 150 step, loss is 1.5684383470215835e-05\n",
      "At 160 step, loss is 3.4161370422225446e-05\n",
      "At 170 step, loss is 3.548090171534568e-05\n",
      "At 180 step, loss is 7.66142038628459e-05\n",
      "At 190 step, loss is 0.00015310976596083492\n",
      "At 200 step, loss is 0.00011163858289364725\n",
      "At 210 step, loss is 0.00047596843796782196\n",
      "At 220 step, loss is 2.602628228487447e-06\n",
      "At 230 step, loss is 0.0008637563441880047\n",
      "At 240 step, loss is 2.1157923413284152e-07\n",
      "Epoch 16\n",
      "At 0 step, loss is 0.00024186815426219255\n",
      "At 10 step, loss is 0.0009732802282087505\n",
      "At 20 step, loss is 2.346108522033319e-05\n",
      "At 30 step, loss is 0.000992393703199923\n",
      "At 40 step, loss is 1.3527536793844774e-05\n",
      "At 50 step, loss is 0.0003058475849684328\n",
      "At 60 step, loss is 0.0004497067129705101\n",
      "At 70 step, loss is 0.0001154178025899455\n",
      "At 80 step, loss is 3.7089077522978187e-05\n",
      "At 90 step, loss is 9.307727304985747e-05\n",
      "At 100 step, loss is 0.00390999112278223\n",
      "At 110 step, loss is 0.0041302889585494995\n",
      "At 120 step, loss is 8.104142034426332e-05\n",
      "At 130 step, loss is 0.0003561806515790522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 140 step, loss is 0.0003378390392754227\n",
      "At 150 step, loss is 0.00011538316903170198\n",
      "At 160 step, loss is 0.0006269988371059299\n",
      "At 170 step, loss is 0.00028236108482815325\n",
      "At 180 step, loss is 0.00020335393492132425\n",
      "At 190 step, loss is 0.0038921332452446222\n",
      "At 200 step, loss is 0.00035251249209977686\n",
      "At 210 step, loss is 2.5341170939441326e-08\n",
      "At 220 step, loss is 0.0007890817942097783\n",
      "At 230 step, loss is 0.003096868982538581\n",
      "At 240 step, loss is 0.001585940713994205\n",
      "Epoch 17\n",
      "At 0 step, loss is 1.1015345990017522e-05\n",
      "At 10 step, loss is 0.0001461895735701546\n",
      "At 20 step, loss is 6.087979636504315e-05\n",
      "At 30 step, loss is 0.00013374425179790705\n",
      "At 40 step, loss is 0.0007306525367312133\n",
      "At 50 step, loss is 0.0005200307350605726\n",
      "At 60 step, loss is 0.0001772227988112718\n",
      "At 70 step, loss is 1.327023937847116e-06\n",
      "At 80 step, loss is 7.661828567506745e-05\n",
      "At 90 step, loss is 0.00043893366819247603\n",
      "At 100 step, loss is 0.0013050272827968001\n",
      "At 110 step, loss is 0.00028959050541743636\n",
      "At 120 step, loss is 2.1771391402580775e-05\n",
      "At 130 step, loss is 0.0016776537522673607\n",
      "At 140 step, loss is 6.081691390136257e-05\n",
      "At 150 step, loss is 0.0002785524120554328\n",
      "At 160 step, loss is 0.00019948989211115986\n",
      "At 170 step, loss is 0.0001147040820796974\n",
      "At 180 step, loss is 0.0011170660145580769\n",
      "At 190 step, loss is 0.006674900650978088\n",
      "At 200 step, loss is 0.00022451077529694885\n",
      "At 210 step, loss is 3.302109689684585e-05\n",
      "At 220 step, loss is 0.0006373024079948664\n",
      "At 230 step, loss is 0.0004212949424982071\n",
      "At 240 step, loss is 0.0013251974014565349\n",
      "Epoch 18\n",
      "At 0 step, loss is 0.003006311831995845\n",
      "At 10 step, loss is 8.388728929276112e-06\n",
      "At 20 step, loss is 2.1234300220385194e-05\n",
      "At 30 step, loss is 0.0004136079805903137\n",
      "At 40 step, loss is 0.003783261636272073\n",
      "At 50 step, loss is 0.001528145745396614\n",
      "At 60 step, loss is 0.0006909820949658751\n",
      "At 70 step, loss is 7.788788207108155e-05\n",
      "At 80 step, loss is 0.00023551040794700384\n",
      "At 90 step, loss is 0.00250031566247344\n",
      "At 100 step, loss is 3.556602123921948e-08\n",
      "At 110 step, loss is 0.00037596840411424637\n",
      "At 120 step, loss is 0.0003905855992343277\n",
      "At 130 step, loss is 0.00016802013851702213\n",
      "At 140 step, loss is 7.415012959199885e-08\n",
      "At 150 step, loss is 0.0009296389762312174\n",
      "At 160 step, loss is 8.208292274503037e-05\n",
      "At 170 step, loss is 9.199521446134895e-05\n",
      "At 180 step, loss is 0.0002193566906498745\n",
      "At 190 step, loss is 0.00125129334628582\n",
      "At 200 step, loss is 0.0003508262161631137\n",
      "At 210 step, loss is 4.649668335332535e-05\n",
      "At 220 step, loss is 0.00021243102673906833\n",
      "At 230 step, loss is 2.8623358957702294e-05\n",
      "At 240 step, loss is 0.00024140022287610918\n",
      "Epoch 19\n",
      "At 0 step, loss is 0.00013808814401272684\n",
      "At 10 step, loss is 0.0038110024761408567\n",
      "At 20 step, loss is 0.0008936672820709646\n",
      "At 30 step, loss is 0.0002906167064793408\n",
      "At 40 step, loss is 2.744791163422633e-05\n",
      "At 50 step, loss is 0.0010873060673475266\n",
      "At 60 step, loss is 0.00028786773327738047\n",
      "At 70 step, loss is 4.674039246310713e-06\n",
      "At 80 step, loss is 0.00019073771545663476\n",
      "At 90 step, loss is 1.896400863188319e-05\n",
      "At 100 step, loss is 1.1193179716428858e-06\n",
      "At 110 step, loss is 0.0023830290883779526\n",
      "At 120 step, loss is 0.0003750890609808266\n",
      "At 130 step, loss is 0.001744731329381466\n",
      "At 140 step, loss is 0.0011951810447499156\n",
      "At 150 step, loss is 0.00018231103604193777\n",
      "At 160 step, loss is 4.995741164748324e-06\n",
      "At 170 step, loss is 0.00010341458983020857\n",
      "At 180 step, loss is 2.6229789000353776e-05\n",
      "At 190 step, loss is 0.0011740792542696\n",
      "At 200 step, loss is 0.0006506117642857134\n",
      "At 210 step, loss is 0.000919652113225311\n",
      "At 220 step, loss is 0.0004514288157224655\n",
      "At 230 step, loss is 0.00046839588321745396\n",
      "At 240 step, loss is 0.0011258525773882866\n",
      "Epoch 20\n",
      "At 0 step, loss is 1.9427191091381246e-06\n",
      "At 10 step, loss is 7.487225229851902e-05\n",
      "At 20 step, loss is 0.0005845186533406377\n",
      "At 30 step, loss is 0.0003451308875810355\n",
      "At 40 step, loss is 0.0013334862887859344\n",
      "At 50 step, loss is 0.0001514565956313163\n",
      "At 60 step, loss is 0.00013174363994039595\n",
      "At 70 step, loss is 1.2596995020430768e-06\n",
      "At 80 step, loss is 0.00027696703909896314\n",
      "At 90 step, loss is 0.0017805852694436908\n",
      "At 100 step, loss is 0.00490228459239006\n",
      "At 110 step, loss is 0.0012387966271489859\n",
      "At 120 step, loss is 2.6033658286905847e-06\n",
      "At 130 step, loss is 2.2088879632065073e-05\n",
      "At 140 step, loss is 0.001073687570169568\n",
      "At 150 step, loss is 0.00022780119616072625\n",
      "At 160 step, loss is 0.000941457983572036\n",
      "At 170 step, loss is 9.06384302652441e-05\n",
      "At 180 step, loss is 2.9393951990641654e-05\n",
      "At 190 step, loss is 0.0003798370889853686\n",
      "At 200 step, loss is 0.00199901289306581\n",
      "At 210 step, loss is 1.6766122143963003e-06\n",
      "At 220 step, loss is 3.152465069433674e-05\n",
      "At 230 step, loss is 0.0001260196149814874\n",
      "At 240 step, loss is 0.00010731678048614413\n",
      "Epoch 21\n",
      "At 0 step, loss is 0.0010397819569334388\n",
      "At 10 step, loss is 0.0002107329055434093\n",
      "At 20 step, loss is 0.001547419000416994\n",
      "At 30 step, loss is 8.102129868348129e-06\n",
      "At 40 step, loss is 0.0014459943631663918\n",
      "At 50 step, loss is 5.783126653113868e-06\n",
      "At 60 step, loss is 0.0006949171656742692\n",
      "At 70 step, loss is 0.0014887349680066109\n",
      "At 80 step, loss is 0.004159807227551937\n",
      "At 90 step, loss is 0.0021996728610247374\n",
      "At 100 step, loss is 0.0003515499411150813\n",
      "At 110 step, loss is 0.00034719781251624227\n",
      "At 120 step, loss is 3.624204691732302e-05\n",
      "At 130 step, loss is 2.0260926248738542e-05\n",
      "At 140 step, loss is 0.004504401236772537\n",
      "At 150 step, loss is 0.00019299362611491233\n",
      "At 160 step, loss is 0.0010549187427386642\n",
      "At 170 step, loss is 2.128048254235182e-05\n",
      "At 180 step, loss is 0.0004605640715453774\n",
      "At 190 step, loss is 2.794774900394259e-06\n",
      "At 200 step, loss is 0.002783405827358365\n",
      "At 210 step, loss is 0.00018496374832466245\n",
      "At 220 step, loss is 2.7059379135607742e-05\n",
      "At 230 step, loss is 2.1672665752703324e-05\n",
      "At 240 step, loss is 1.5156993868004065e-05\n",
      "Epoch 22\n",
      "At 0 step, loss is 6.440280412789434e-05\n",
      "At 10 step, loss is 0.00039384071715176105\n",
      "At 20 step, loss is 0.0017224446637555957\n",
      "At 30 step, loss is 0.0007589990273118019\n",
      "At 40 step, loss is 0.00011406517296563834\n",
      "At 50 step, loss is 8.358080776815768e-06\n",
      "At 60 step, loss is 0.00098786863964051\n",
      "At 70 step, loss is 0.00015602086205035448\n",
      "At 80 step, loss is 0.00014703140186611563\n",
      "At 90 step, loss is 0.001225451473146677\n",
      "At 100 step, loss is 0.0013613657793030143\n",
      "At 110 step, loss is 0.0006031373632140458\n",
      "At 120 step, loss is 7.16741633368656e-05\n",
      "At 130 step, loss is 0.0017446050187572837\n",
      "At 140 step, loss is 0.0011813055025413632\n",
      "At 150 step, loss is 8.397104102186859e-05\n",
      "At 160 step, loss is 0.00011848140275105834\n",
      "At 170 step, loss is 0.0008603641763329506\n",
      "At 180 step, loss is 0.0028478673193603754\n",
      "At 190 step, loss is 0.0007984345429576933\n",
      "At 200 step, loss is 3.79171542590484e-05\n",
      "At 210 step, loss is 0.0052240630611777306\n",
      "At 220 step, loss is 9.059736476046965e-05\n",
      "At 230 step, loss is 0.0014492934569716454\n",
      "At 240 step, loss is 0.0004497309564612806\n",
      "Epoch 23\n",
      "At 0 step, loss is 0.00028364919126033783\n",
      "At 10 step, loss is 0.00023775598674546927\n",
      "At 20 step, loss is 4.589413947542198e-05\n",
      "At 30 step, loss is 7.489080076084065e-07\n",
      "At 40 step, loss is 0.0016459319740533829\n",
      "At 50 step, loss is 0.000606076733674854\n",
      "At 60 step, loss is 1.8112416000803933e-05\n",
      "At 70 step, loss is 0.0014681173488497734\n",
      "At 80 step, loss is 0.007208558730781078\n",
      "At 90 step, loss is 0.0007477060426026583\n",
      "At 100 step, loss is 0.0008274216088466346\n",
      "At 110 step, loss is 0.0001316626148764044\n",
      "At 120 step, loss is 6.639027560595423e-05\n",
      "At 130 step, loss is 3.95016786569613e-06\n",
      "At 140 step, loss is 0.00016817064897622913\n",
      "At 150 step, loss is 5.284427970764227e-05\n",
      "At 160 step, loss is 0.007753538433462381\n",
      "At 170 step, loss is 0.00047477681073360145\n",
      "At 180 step, loss is 0.0004798052541445941\n",
      "At 190 step, loss is 0.0005962377181276679\n",
      "At 200 step, loss is 0.004081995226442814\n",
      "At 210 step, loss is 9.74975228018593e-06\n",
      "At 220 step, loss is 0.0007928466075100005\n",
      "At 230 step, loss is 0.001277848263271153\n",
      "At 240 step, loss is 0.0003137670864816755\n",
      "Epoch 24\n",
      "At 0 step, loss is 0.0018902445444837213\n",
      "At 10 step, loss is 1.4819011084910017e-05\n",
      "At 20 step, loss is 4.014205114799552e-05\n",
      "At 30 step, loss is 2.8071170163457282e-05\n",
      "At 40 step, loss is 0.0019930824637413025\n",
      "At 50 step, loss is 1.1056355106120463e-05\n",
      "At 60 step, loss is 4.0114133298629895e-05\n",
      "At 70 step, loss is 9.754743359735585e-07\n",
      "At 80 step, loss is 3.6303799788584e-06\n",
      "At 90 step, loss is 2.3838665583753027e-05\n",
      "At 100 step, loss is 0.000689181499183178\n",
      "At 110 step, loss is 0.0018835109658539295\n",
      "At 120 step, loss is 0.00010616142390063033\n",
      "At 130 step, loss is 4.995501399207569e-07\n",
      "At 140 step, loss is 2.5371115043526515e-05\n",
      "At 150 step, loss is 0.00029493661713786423\n",
      "At 160 step, loss is 1.7921745438798098e-06\n",
      "At 170 step, loss is 5.7546174502931535e-05\n",
      "At 180 step, loss is 0.000317592202918604\n",
      "At 190 step, loss is 5.268325367069338e-07\n",
      "At 200 step, loss is 2.6640540454536676e-05\n",
      "At 210 step, loss is 4.173335400992073e-05\n",
      "At 220 step, loss is 0.00012928646174259484\n",
      "At 230 step, loss is 0.0004245636228006333\n",
      "At 240 step, loss is 0.002924011554569006\n",
      "Epoch 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 0 step, loss is 0.004848478827625513\n",
      "At 10 step, loss is 0.0002947095490526408\n",
      "At 20 step, loss is 0.00014468930021394044\n",
      "At 30 step, loss is 2.905200562963728e-05\n",
      "At 40 step, loss is 9.22901090234518e-05\n",
      "At 50 step, loss is 1.3726473298447672e-05\n",
      "At 60 step, loss is 0.0021653862204402685\n",
      "At 70 step, loss is 0.003671057289466262\n",
      "At 80 step, loss is 0.002556213876232505\n",
      "At 90 step, loss is 0.0006345056463032961\n",
      "At 100 step, loss is 7.039131742203608e-05\n",
      "At 110 step, loss is 0.0002574319369159639\n",
      "At 120 step, loss is 0.0006302896072156727\n",
      "At 130 step, loss is 2.3496468202210963e-05\n",
      "At 140 step, loss is 1.285747748624999e-05\n",
      "At 150 step, loss is 7.1204844971362036e-06\n",
      "At 160 step, loss is 0.0018369883764535189\n",
      "At 170 step, loss is 0.0004900138592347503\n",
      "At 180 step, loss is 0.0014088527532294393\n",
      "At 190 step, loss is 0.0012488416396081448\n",
      "At 200 step, loss is 0.0007392536499537528\n",
      "At 210 step, loss is 0.007692854851484299\n",
      "At 220 step, loss is 0.0003310106403660029\n",
      "At 230 step, loss is 0.0004478019254747778\n",
      "At 240 step, loss is 0.00030364791746251285\n",
      "Epoch 26\n",
      "At 0 step, loss is 8.770182466832921e-05\n",
      "At 10 step, loss is 7.438591183017706e-07\n",
      "At 20 step, loss is 0.0004693997325375676\n",
      "At 30 step, loss is 1.5071076120420912e-07\n",
      "At 40 step, loss is 0.0007697551045566797\n",
      "At 50 step, loss is 0.0014206405030563474\n",
      "At 60 step, loss is 0.00011024176637874916\n",
      "At 70 step, loss is 4.45867556209123e-07\n",
      "At 80 step, loss is 0.00039795259363017976\n",
      "At 90 step, loss is 0.00015521852765232325\n",
      "At 100 step, loss is 0.0048744818195700645\n",
      "At 110 step, loss is 0.0006948026129975915\n",
      "At 120 step, loss is 9.699008842289913e-06\n",
      "At 130 step, loss is 0.0042557669803500175\n",
      "At 140 step, loss is 0.005384005606174469\n",
      "At 150 step, loss is 6.758679228369147e-05\n",
      "At 160 step, loss is 0.0005871098255738616\n",
      "At 170 step, loss is 0.00026111729675903916\n",
      "At 180 step, loss is 0.00046019296860322356\n",
      "At 190 step, loss is 1.781332139216829e-05\n",
      "At 200 step, loss is 3.386484968359582e-05\n",
      "At 210 step, loss is 0.00013803929323330522\n",
      "At 220 step, loss is 0.0003563120844773948\n",
      "At 230 step, loss is 8.53431920404546e-05\n",
      "At 240 step, loss is 0.0009233851451426744\n",
      "Epoch 27\n",
      "At 0 step, loss is 8.94355707714567e-07\n",
      "At 10 step, loss is 0.0001131113021983765\n",
      "At 20 step, loss is 2.197746107412968e-05\n",
      "At 30 step, loss is 0.0029536571819335222\n",
      "At 40 step, loss is 0.00021016885875724256\n",
      "At 50 step, loss is 0.00016210593457799405\n",
      "At 60 step, loss is 0.0005219520535320044\n",
      "At 70 step, loss is 0.0008013939368538558\n",
      "At 80 step, loss is 0.00010712265793699771\n",
      "At 90 step, loss is 0.0008094680379144847\n",
      "At 100 step, loss is 2.6026320483651944e-05\n",
      "At 110 step, loss is 8.411501039518043e-05\n",
      "At 120 step, loss is 0.0002147162740584463\n",
      "At 130 step, loss is 0.0006999585893936455\n",
      "At 140 step, loss is 4.010792622466397e-07\n",
      "At 150 step, loss is 2.438228875689674e-05\n",
      "At 160 step, loss is 8.15736930235289e-05\n",
      "At 170 step, loss is 0.0001572519395267591\n",
      "At 180 step, loss is 0.00016919619520194829\n",
      "At 190 step, loss is 0.004332347307354212\n",
      "At 200 step, loss is 0.0006524869240820408\n",
      "At 210 step, loss is 3.080613896599971e-05\n",
      "At 220 step, loss is 8.515534136677161e-05\n",
      "At 230 step, loss is 3.9241645026777405e-06\n",
      "At 240 step, loss is 0.0010450744302943349\n",
      "Epoch 28\n",
      "At 0 step, loss is 0.00038880910142324865\n",
      "At 10 step, loss is 5.160031287232414e-05\n",
      "At 20 step, loss is 0.00034216034691780806\n",
      "At 30 step, loss is 0.00038537339423783123\n",
      "At 40 step, loss is 0.0024223809596151114\n",
      "At 50 step, loss is 0.0006072284304536879\n",
      "At 60 step, loss is 4.516303135915223e-07\n",
      "At 70 step, loss is 0.0003009663778357208\n",
      "At 80 step, loss is 0.0022409893572330475\n",
      "At 90 step, loss is 0.0003268275177106261\n",
      "At 100 step, loss is 0.0003896734269801527\n",
      "At 110 step, loss is 0.0007469052798114717\n",
      "At 120 step, loss is 0.0003330617328174412\n",
      "At 130 step, loss is 0.0004149966698605567\n",
      "At 140 step, loss is 5.5337433877866715e-05\n",
      "At 150 step, loss is 8.180274562619161e-06\n",
      "At 160 step, loss is 0.00020697554282378405\n",
      "At 170 step, loss is 5.019373566028662e-05\n",
      "At 180 step, loss is 5.015568603994325e-05\n",
      "At 190 step, loss is 1.3568274880526587e-05\n",
      "At 200 step, loss is 0.0002831064921338111\n",
      "At 210 step, loss is 0.0030403484124690294\n",
      "At 220 step, loss is 0.0023605534806847572\n",
      "At 230 step, loss is 1.4905799616826698e-06\n",
      "At 240 step, loss is 0.002156435279175639\n",
      "Epoch 29\n",
      "At 0 step, loss is 0.001665709656663239\n",
      "At 10 step, loss is 8.657926809974015e-05\n",
      "At 20 step, loss is 2.7129215595778078e-05\n",
      "At 30 step, loss is 9.144702062258148e-07\n",
      "At 40 step, loss is 0.001197636709548533\n",
      "At 50 step, loss is 0.00017572432989254594\n",
      "At 60 step, loss is 3.083004003201495e-06\n",
      "At 70 step, loss is 0.002873225836083293\n",
      "At 80 step, loss is 7.71440245443955e-05\n",
      "At 90 step, loss is 0.0004155579081270844\n",
      "At 100 step, loss is 0.00027529557701200247\n",
      "At 110 step, loss is 3.86488827643916e-05\n",
      "At 120 step, loss is 1.4524449397868011e-05\n",
      "At 130 step, loss is 0.0006610524724237621\n",
      "At 140 step, loss is 0.00013065467646811157\n",
      "At 150 step, loss is 0.00042724082595668733\n",
      "At 160 step, loss is 0.00236333510838449\n",
      "At 170 step, loss is 0.0014241182943806052\n",
      "At 180 step, loss is 4.121749225305393e-05\n",
      "At 190 step, loss is 7.255102445924422e-06\n",
      "At 200 step, loss is 0.013535240665078163\n",
      "At 210 step, loss is 0.0008788747945800424\n",
      "At 220 step, loss is 0.0015241989167407155\n",
      "At 230 step, loss is 0.0002468941092956811\n",
      "At 240 step, loss is 0.00044054261525161564\n",
      "Epoch 30\n",
      "At 0 step, loss is 0.00022407482902053744\n",
      "At 10 step, loss is 0.00046234848559834063\n",
      "At 20 step, loss is 0.00015424273442476988\n",
      "At 30 step, loss is 0.0004624544526450336\n",
      "At 40 step, loss is 0.00014654196274932474\n",
      "At 50 step, loss is 0.019342169165611267\n",
      "At 60 step, loss is 0.0004689943161793053\n",
      "At 70 step, loss is 0.01071974728256464\n",
      "At 80 step, loss is 0.00020476341887842864\n",
      "At 90 step, loss is 0.0015209497651085258\n",
      "At 100 step, loss is 1.6476651580887847e-05\n",
      "At 110 step, loss is 0.0010162319522351027\n",
      "At 120 step, loss is 0.05825885385274887\n",
      "At 130 step, loss is 0.007227174937725067\n",
      "At 140 step, loss is 0.0014277218142524362\n",
      "At 150 step, loss is 0.00024557363940402865\n",
      "At 160 step, loss is 0.0005987873300909996\n",
      "At 170 step, loss is 7.53908752813004e-05\n",
      "At 180 step, loss is 0.0008968371548689902\n",
      "At 190 step, loss is 0.000813402992207557\n",
      "At 200 step, loss is 0.0010867227101698518\n",
      "At 210 step, loss is 2.476432746334467e-05\n",
      "At 220 step, loss is 0.0029697674326598644\n",
      "At 230 step, loss is 0.0007199529209174216\n",
      "At 240 step, loss is 0.0004323101311456412\n",
      "Epoch 31\n",
      "At 0 step, loss is 5.504190630745143e-05\n",
      "At 10 step, loss is 0.0006072306423448026\n",
      "At 20 step, loss is 0.010992943309247494\n",
      "At 30 step, loss is 0.001284077879972756\n",
      "At 40 step, loss is 0.00017955864313989878\n",
      "At 50 step, loss is 0.00011878373334184289\n",
      "At 60 step, loss is 1.6008090824470855e-05\n",
      "At 70 step, loss is 0.0024198663886636496\n",
      "At 80 step, loss is 0.0001568055449752137\n",
      "At 90 step, loss is 0.0003053651307709515\n",
      "At 100 step, loss is 0.000736393325496465\n",
      "At 110 step, loss is 4.802232069778256e-05\n",
      "At 120 step, loss is 2.253400907648029e-06\n",
      "At 130 step, loss is 6.7253599809191655e-06\n",
      "At 140 step, loss is 0.0005734681035391986\n",
      "At 150 step, loss is 0.0007329665822908282\n",
      "At 160 step, loss is 0.0017418336356058717\n",
      "At 170 step, loss is 0.002478513866662979\n",
      "At 180 step, loss is 0.0001342188916169107\n",
      "At 190 step, loss is 0.00021846518211532384\n",
      "At 200 step, loss is 2.757599531832966e-06\n",
      "At 210 step, loss is 3.079304951825179e-05\n",
      "At 220 step, loss is 0.0003759991959668696\n",
      "At 230 step, loss is 0.0009429570054635406\n",
      "At 240 step, loss is 0.0005764128873124719\n",
      "Epoch 32\n",
      "At 0 step, loss is 0.0023710131645202637\n",
      "At 10 step, loss is 0.00023009325377643108\n",
      "At 20 step, loss is 0.0011757629690691829\n",
      "At 30 step, loss is 0.0010663113789632916\n",
      "At 40 step, loss is 0.0002794979955069721\n",
      "At 50 step, loss is 0.000723639503121376\n",
      "At 60 step, loss is 1.7002604124627396e-07\n",
      "At 70 step, loss is 0.0042480346746742725\n",
      "At 80 step, loss is 0.00043890048982575536\n",
      "At 90 step, loss is 0.0002355767210246995\n",
      "At 100 step, loss is 1.5181485650828108e-05\n",
      "At 110 step, loss is 7.060013012960553e-05\n",
      "At 120 step, loss is 0.0013137537753209472\n",
      "At 130 step, loss is 0.0011493655620142817\n",
      "At 140 step, loss is 0.0001749302464304492\n",
      "At 150 step, loss is 5.6035547459032387e-05\n",
      "At 160 step, loss is 0.0002456876100040972\n",
      "At 170 step, loss is 0.0002655778080224991\n",
      "At 180 step, loss is 0.0038638270925730467\n",
      "At 190 step, loss is 4.0976254240376875e-05\n",
      "At 200 step, loss is 0.0003469629446044564\n",
      "At 210 step, loss is 0.00045226566726341844\n",
      "At 220 step, loss is 0.003474367316812277\n",
      "At 230 step, loss is 0.0019748469348996878\n",
      "At 240 step, loss is 7.795981218805537e-06\n",
      "Epoch 33\n",
      "At 0 step, loss is 0.001041400944814086\n",
      "At 10 step, loss is 1.0140682206838392e-05\n",
      "At 20 step, loss is 0.00010355943231843412\n",
      "At 30 step, loss is 0.00019316859834361821\n",
      "At 40 step, loss is 0.000995338661596179\n",
      "At 50 step, loss is 0.0012020390713587403\n",
      "At 60 step, loss is 0.0005070213228464127\n",
      "At 70 step, loss is 0.0005310503765940666\n",
      "At 80 step, loss is 0.0002833586768247187\n",
      "At 90 step, loss is 4.909466952085495e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 100 step, loss is 8.057988452492282e-06\n",
      "At 110 step, loss is 0.0006490636733360589\n",
      "At 120 step, loss is 0.0023639134597033262\n",
      "At 130 step, loss is 0.002649143571034074\n",
      "At 140 step, loss is 3.158476502562735e-08\n",
      "At 150 step, loss is 9.038364078151062e-05\n",
      "At 160 step, loss is 1.55947454913985e-05\n",
      "At 170 step, loss is 1.4079723769100383e-05\n",
      "At 180 step, loss is 8.797551708994433e-05\n",
      "At 190 step, loss is 0.0004678275145124644\n",
      "At 200 step, loss is 8.366486144950613e-05\n",
      "At 210 step, loss is 0.00010487203689990565\n",
      "At 220 step, loss is 0.00010165660205530003\n",
      "At 230 step, loss is 0.0006601703935302794\n",
      "At 240 step, loss is 0.004721271805465221\n",
      "Epoch 34\n",
      "At 0 step, loss is 0.00018395695951767266\n",
      "At 10 step, loss is 2.0520887119346298e-05\n",
      "At 20 step, loss is 6.027730705682188e-06\n",
      "At 30 step, loss is 0.0002037898957496509\n",
      "At 40 step, loss is 7.279764759005047e-06\n",
      "At 50 step, loss is 0.002383601851761341\n",
      "At 60 step, loss is 0.00015718572831247002\n",
      "At 70 step, loss is 0.0015033510280773044\n",
      "At 80 step, loss is 0.0012721396051347256\n",
      "At 90 step, loss is 0.00016330111247953027\n",
      "At 100 step, loss is 2.7929880161536857e-05\n",
      "At 110 step, loss is 0.00010024411312770098\n",
      "At 120 step, loss is 3.4090659028152004e-05\n",
      "At 130 step, loss is 9.614824136860989e-08\n",
      "At 140 step, loss is 0.00012724626867566258\n",
      "At 150 step, loss is 8.118265395751223e-05\n",
      "At 160 step, loss is 2.9473591212081374e-07\n",
      "At 170 step, loss is 0.00047471633297391236\n",
      "At 180 step, loss is 0.0012436237884685397\n",
      "At 190 step, loss is 0.0008126333123072982\n",
      "At 200 step, loss is 5.952079391136067e-06\n",
      "At 210 step, loss is 0.0033725795801728964\n",
      "At 220 step, loss is 0.00025738446856848896\n",
      "At 230 step, loss is 0.00016179782687686384\n",
      "At 240 step, loss is 0.0001971109741134569\n",
      "Epoch 35\n",
      "At 0 step, loss is 8.099344086076599e-06\n",
      "At 10 step, loss is 4.91742612211965e-05\n",
      "At 20 step, loss is 3.108737701040809e-06\n",
      "At 30 step, loss is 0.0008178917923942208\n",
      "At 40 step, loss is 0.0008527221507392824\n",
      "At 50 step, loss is 0.00015735415217932314\n",
      "At 60 step, loss is 0.00013019965263083577\n",
      "At 70 step, loss is 0.0007139471708796918\n",
      "At 80 step, loss is 0.0025378852151334286\n",
      "At 90 step, loss is 0.002946560736745596\n",
      "At 100 step, loss is 0.009895997121930122\n",
      "At 110 step, loss is 3.2900902624533046e-06\n",
      "At 120 step, loss is 0.008486242964863777\n",
      "At 130 step, loss is 0.000467622623546049\n",
      "At 140 step, loss is 0.0008229814702644944\n",
      "At 150 step, loss is 0.0005548165645450354\n",
      "At 160 step, loss is 0.0019244616851210594\n",
      "At 170 step, loss is 0.00011373156303307042\n",
      "At 180 step, loss is 0.003087816061452031\n",
      "At 190 step, loss is 0.0013883889187127352\n",
      "At 200 step, loss is 0.0013525113463401794\n",
      "At 210 step, loss is 0.0001372595434077084\n",
      "At 220 step, loss is 0.00032696317066438496\n",
      "At 230 step, loss is 0.00026444048853591084\n",
      "At 240 step, loss is 0.001416187034919858\n",
      "Epoch 36\n",
      "At 0 step, loss is 4.54602122772485e-05\n",
      "At 10 step, loss is 8.224558769143187e-06\n",
      "At 20 step, loss is 0.0015954645350575447\n",
      "At 30 step, loss is 2.7248426704318263e-05\n",
      "At 40 step, loss is 1.8442424334352836e-05\n",
      "At 50 step, loss is 3.1044221486808965e-06\n",
      "At 60 step, loss is 0.00020907515136059374\n",
      "At 70 step, loss is 0.00023969604808371514\n",
      "At 80 step, loss is 0.0015756553038954735\n",
      "At 90 step, loss is 0.002216850873082876\n",
      "At 100 step, loss is 0.0024011111818253994\n",
      "At 110 step, loss is 3.847818516078405e-05\n",
      "At 120 step, loss is 0.002239117631688714\n",
      "At 130 step, loss is 6.484607001766562e-05\n",
      "At 140 step, loss is 0.0003494015836622566\n",
      "At 150 step, loss is 0.000668937515001744\n",
      "At 160 step, loss is 0.0023890514858067036\n",
      "At 170 step, loss is 0.0020414087921380997\n",
      "At 180 step, loss is 0.0009263885440304875\n",
      "At 190 step, loss is 1.7302450942224823e-05\n",
      "At 200 step, loss is 0.00022081662609707564\n",
      "At 210 step, loss is 9.452248195884749e-05\n",
      "At 220 step, loss is 9.91082561085932e-06\n",
      "At 230 step, loss is 2.8437118089641444e-05\n",
      "At 240 step, loss is 0.00037348116165958345\n",
      "Epoch 37\n",
      "At 0 step, loss is 0.0006081511965021491\n",
      "At 10 step, loss is 0.009218798018991947\n",
      "At 20 step, loss is 0.004734229762107134\n",
      "At 30 step, loss is 0.001295822556130588\n",
      "At 40 step, loss is 0.0005025543505325913\n",
      "At 50 step, loss is 0.0013894515577703714\n",
      "At 60 step, loss is 0.0033483454026281834\n",
      "At 70 step, loss is 0.0010083861416205764\n",
      "At 80 step, loss is 0.00017236769781447947\n",
      "At 90 step, loss is 0.0001529986911918968\n",
      "At 100 step, loss is 0.0015564181376248598\n",
      "At 110 step, loss is 0.002038479084149003\n",
      "At 120 step, loss is 1.1995865634162328e-07\n",
      "At 130 step, loss is 0.0008705909131094813\n",
      "At 140 step, loss is 0.0010477161267772317\n",
      "At 150 step, loss is 0.00025726843159645796\n",
      "At 160 step, loss is 0.0013192555634304881\n",
      "At 170 step, loss is 0.0035710870288312435\n",
      "At 180 step, loss is 0.00024363971897400916\n",
      "At 190 step, loss is 0.0016494584269821644\n",
      "At 200 step, loss is 0.004071694798767567\n",
      "At 210 step, loss is 2.4023154765018262e-05\n",
      "At 220 step, loss is 0.002362848725169897\n",
      "At 230 step, loss is 2.837875712202731e-08\n",
      "At 240 step, loss is 0.0008737584576010704\n",
      "Epoch 38\n",
      "At 0 step, loss is 0.002047164598479867\n",
      "At 10 step, loss is 5.88517696087365e-06\n",
      "At 20 step, loss is 0.003944334574043751\n",
      "At 30 step, loss is 3.512778494041413e-05\n",
      "At 40 step, loss is 0.0006199554773047566\n",
      "At 50 step, loss is 3.9062913856469095e-05\n",
      "At 60 step, loss is 0.0005241300677880645\n",
      "At 70 step, loss is 2.3223803509608842e-05\n",
      "At 80 step, loss is 3.546683365129866e-05\n",
      "At 90 step, loss is 0.0006124543724581599\n",
      "At 100 step, loss is 0.00033012821222655475\n",
      "At 110 step, loss is 0.001985232811421156\n",
      "At 120 step, loss is 0.0010800535092130303\n",
      "At 130 step, loss is 0.0006537899025715888\n",
      "At 140 step, loss is 3.0216346203815192e-05\n",
      "At 150 step, loss is 1.3680165466212202e-05\n",
      "At 160 step, loss is 7.190429460024461e-05\n",
      "At 170 step, loss is 0.0004474865854717791\n",
      "At 180 step, loss is 0.0003288133011665195\n",
      "At 190 step, loss is 0.0009678585338406265\n",
      "At 200 step, loss is 0.0005679404712282121\n",
      "At 210 step, loss is 0.0027836530935019255\n",
      "At 220 step, loss is 0.024766601622104645\n",
      "At 230 step, loss is 2.3511811377829872e-05\n",
      "At 240 step, loss is 0.00020250513625796884\n",
      "Epoch 39\n",
      "At 0 step, loss is 0.0010139497462660074\n",
      "At 10 step, loss is 0.0004441496275831014\n",
      "At 20 step, loss is 9.527996098768199e-07\n",
      "At 30 step, loss is 0.0005138767301104963\n",
      "At 40 step, loss is 0.0013159277150407434\n",
      "At 50 step, loss is 2.1464529709191993e-05\n",
      "At 60 step, loss is 0.0013403862249106169\n",
      "At 70 step, loss is 4.15725662605837e-06\n",
      "At 80 step, loss is 7.641561387572438e-05\n",
      "At 90 step, loss is 0.001560318167321384\n",
      "At 100 step, loss is 2.96667330985656e-05\n",
      "At 110 step, loss is 0.0011988076148554683\n",
      "At 120 step, loss is 0.0027322699315845966\n",
      "At 130 step, loss is 0.00014327038661576807\n",
      "At 140 step, loss is 0.0009338128147646785\n",
      "At 150 step, loss is 9.945223428076133e-05\n",
      "At 160 step, loss is 0.00020711706019937992\n",
      "At 170 step, loss is 0.0001098749999073334\n",
      "At 180 step, loss is 0.00030492438236251473\n",
      "At 190 step, loss is 0.00569924945011735\n",
      "At 200 step, loss is 0.00015978359442669898\n",
      "At 210 step, loss is 0.00816683005541563\n",
      "At 220 step, loss is 0.0016676548402756453\n",
      "At 230 step, loss is 1.45100802910747e-05\n",
      "At 240 step, loss is 0.00022025643556844443\n",
      "Epoch 40\n",
      "At 0 step, loss is 3.6697208997793496e-05\n",
      "At 10 step, loss is 3.2063609978649765e-05\n",
      "At 20 step, loss is 8.234458800870925e-05\n",
      "At 30 step, loss is 0.010507971048355103\n",
      "At 40 step, loss is 0.00035671814111992717\n",
      "At 50 step, loss is 0.0007691901409998536\n",
      "At 60 step, loss is 0.00023973596398718655\n",
      "At 70 step, loss is 0.0011386862024664879\n",
      "At 80 step, loss is 0.004549117758870125\n",
      "At 90 step, loss is 0.00024628941901028156\n",
      "At 100 step, loss is 0.0003251525922678411\n",
      "At 110 step, loss is 0.0023324231151491404\n",
      "At 120 step, loss is 0.0015363985439762473\n",
      "At 130 step, loss is 0.00015631913265679032\n",
      "At 140 step, loss is 0.000402647303417325\n",
      "At 150 step, loss is 0.0015184397343546152\n",
      "At 160 step, loss is 0.000150140724144876\n",
      "At 170 step, loss is 0.0003693691687658429\n",
      "At 180 step, loss is 8.132354196277447e-06\n",
      "At 190 step, loss is 0.005239952355623245\n",
      "At 200 step, loss is 0.0011122748255729675\n",
      "At 210 step, loss is 0.0008146850159391761\n",
      "At 220 step, loss is 3.92307674701442e-06\n",
      "At 230 step, loss is 0.0016123938839882612\n",
      "At 240 step, loss is 0.0002655475109349936\n",
      "Epoch 41\n",
      "At 0 step, loss is 1.4880305570841301e-05\n",
      "At 10 step, loss is 0.0007442976348102093\n",
      "At 20 step, loss is 0.0015712978783994913\n",
      "At 30 step, loss is 0.0004934463649988174\n",
      "At 40 step, loss is 0.0007037940085865557\n",
      "At 50 step, loss is 0.00014556405949406326\n",
      "At 60 step, loss is 0.00039991148514673114\n",
      "At 70 step, loss is 1.183943277283106e-05\n",
      "At 80 step, loss is 0.0003508120425976813\n",
      "At 90 step, loss is 0.00022525317035615444\n",
      "At 100 step, loss is 3.431454751989804e-05\n",
      "At 110 step, loss is 2.0379524357849732e-05\n",
      "At 120 step, loss is 3.861802542814985e-05\n",
      "At 130 step, loss is 5.878323645447381e-05\n",
      "At 140 step, loss is 5.929182225372642e-05\n",
      "At 150 step, loss is 1.2280377177376067e-06\n",
      "At 160 step, loss is 0.0018350488971918821\n",
      "At 170 step, loss is 5.744680674979463e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 180 step, loss is 0.00019759278802666813\n",
      "At 190 step, loss is 0.0007528953719884157\n",
      "At 200 step, loss is 0.0004190145409666002\n",
      "At 210 step, loss is 0.0022299764677882195\n",
      "At 220 step, loss is 0.0010772402165457606\n",
      "At 230 step, loss is 0.00017238403961528093\n",
      "At 240 step, loss is 0.0018923698225989938\n",
      "Epoch 42\n",
      "At 0 step, loss is 5.411544634625898e-08\n",
      "At 10 step, loss is 0.0006353449425660074\n",
      "At 20 step, loss is 2.1176159862079658e-05\n",
      "At 30 step, loss is 0.0009393377113156021\n",
      "At 40 step, loss is 6.621914508286864e-05\n",
      "At 50 step, loss is 0.0007306002080440521\n",
      "At 60 step, loss is 0.00011663079203572124\n",
      "At 70 step, loss is 0.00017032178584486246\n",
      "At 80 step, loss is 0.002308889990672469\n",
      "At 90 step, loss is 7.721566362306476e-05\n",
      "At 100 step, loss is 2.6956075089401565e-05\n",
      "At 110 step, loss is 1.2977178585060756e-06\n",
      "At 120 step, loss is 0.00011683277989504859\n",
      "At 130 step, loss is 0.0017792977159842849\n",
      "At 140 step, loss is 0.0011818342609331012\n",
      "At 150 step, loss is 1.4602530427509919e-05\n",
      "At 160 step, loss is 0.0006661775405518711\n",
      "At 170 step, loss is 0.0002779456553980708\n",
      "At 180 step, loss is 0.001092769205570221\n",
      "At 190 step, loss is 5.578019772656262e-05\n",
      "At 200 step, loss is 6.939266313565895e-06\n",
      "At 210 step, loss is 0.00010587016004137695\n",
      "At 220 step, loss is 1.8764314518193714e-05\n",
      "At 230 step, loss is 3.7415595670609036e-06\n",
      "At 240 step, loss is 0.00045024967403151095\n",
      "Epoch 43\n",
      "At 0 step, loss is 1.084043105947785e-06\n",
      "At 10 step, loss is 0.00043469356023706496\n",
      "At 20 step, loss is 1.5692392025812296e-06\n",
      "At 30 step, loss is 0.0015411273343488574\n",
      "At 40 step, loss is 2.001391067096847e-06\n",
      "At 50 step, loss is 5.147055344423279e-05\n",
      "At 60 step, loss is 0.00010483319783816114\n",
      "At 70 step, loss is 1.2656326362048276e-05\n",
      "At 80 step, loss is 9.10824237507768e-05\n",
      "At 90 step, loss is 0.0003700879169628024\n",
      "At 100 step, loss is 0.0020343326032161713\n",
      "At 110 step, loss is 8.440866804448888e-05\n",
      "At 120 step, loss is 0.00044023693772032857\n",
      "At 130 step, loss is 1.0831870895344764e-05\n",
      "At 140 step, loss is 0.0002445826248731464\n",
      "At 150 step, loss is 0.0019183900440111756\n",
      "At 160 step, loss is 7.34262794139795e-05\n",
      "At 170 step, loss is 0.00033284089295193553\n",
      "At 180 step, loss is 8.5299281636253e-05\n",
      "At 190 step, loss is 0.00016816648712847382\n",
      "At 200 step, loss is 0.0001692386285867542\n",
      "At 210 step, loss is 0.00045677798334509134\n",
      "At 220 step, loss is 0.0005178620922379196\n",
      "At 230 step, loss is 0.0002927191380877048\n",
      "At 240 step, loss is 0.00015705119585618377\n",
      "Epoch 44\n",
      "At 0 step, loss is 0.0002175411063944921\n",
      "At 10 step, loss is 0.00047788588562980294\n",
      "At 20 step, loss is 0.0005850959569215775\n",
      "At 30 step, loss is 0.002071209717541933\n",
      "At 40 step, loss is 0.000738067552447319\n",
      "At 50 step, loss is 0.0003730395983438939\n",
      "At 60 step, loss is 0.0020207876805216074\n",
      "At 70 step, loss is 0.0007584943668916821\n",
      "At 80 step, loss is 0.0003120996989309788\n",
      "At 90 step, loss is 0.0025705271400511265\n",
      "At 100 step, loss is 0.0012077970895916224\n",
      "At 110 step, loss is 6.516325811389834e-05\n",
      "At 120 step, loss is 5.027646693633869e-05\n",
      "At 130 step, loss is 0.004802936688065529\n",
      "At 140 step, loss is 3.769339309656061e-05\n",
      "At 150 step, loss is 0.003139751497656107\n",
      "At 160 step, loss is 4.992212780052796e-05\n",
      "At 170 step, loss is 4.042035652673803e-05\n",
      "At 180 step, loss is 0.0002239671885035932\n",
      "At 190 step, loss is 0.0009561795159243047\n",
      "At 200 step, loss is 0.00034894770942628384\n",
      "At 210 step, loss is 5.075828539702343e-07\n",
      "At 220 step, loss is 0.0002538458211347461\n",
      "At 230 step, loss is 0.0036126566119492054\n",
      "At 240 step, loss is 0.0012357725063338876\n",
      "Epoch 45\n",
      "At 0 step, loss is 3.0095682745923114e-07\n",
      "At 10 step, loss is 0.00020085986761841923\n",
      "At 20 step, loss is 0.00011246617941651493\n",
      "At 30 step, loss is 0.0009144850191660225\n",
      "At 40 step, loss is 0.00025098194601014256\n",
      "At 50 step, loss is 0.00011420516239013523\n",
      "At 60 step, loss is 0.0006683475221507251\n",
      "At 70 step, loss is 0.00032423375523649156\n",
      "At 80 step, loss is 0.0009957406437024474\n",
      "At 90 step, loss is 2.2008680389262736e-05\n",
      "At 100 step, loss is 0.0031850729137659073\n",
      "At 110 step, loss is 0.00037664498086087406\n",
      "At 120 step, loss is 1.689150303718634e-05\n",
      "At 130 step, loss is 3.3977175917243585e-05\n",
      "At 140 step, loss is 0.00025368595379404724\n",
      "At 150 step, loss is 9.790246986085549e-05\n",
      "At 160 step, loss is 0.0004157815710641444\n",
      "At 170 step, loss is 0.0011644999030977488\n",
      "At 180 step, loss is 0.0013683519791811705\n",
      "At 190 step, loss is 4.345187335275114e-06\n",
      "At 200 step, loss is 3.4439181035850197e-06\n",
      "At 210 step, loss is 0.0016792651731520891\n",
      "At 220 step, loss is 0.0018798342207446694\n",
      "At 230 step, loss is 0.0012297837529331446\n",
      "At 240 step, loss is 0.0008488551829941571\n",
      "Epoch 46\n",
      "At 0 step, loss is 0.00020696216961368918\n",
      "At 10 step, loss is 2.111120011250023e-06\n",
      "At 20 step, loss is 7.78452813392505e-05\n",
      "At 30 step, loss is 0.0004928059061057866\n",
      "At 40 step, loss is 4.0464408812113106e-05\n",
      "At 50 step, loss is 0.0029230338986963034\n",
      "At 60 step, loss is 1.699789208942093e-05\n",
      "At 70 step, loss is 0.00043164953240193427\n",
      "At 80 step, loss is 0.0019660107791423798\n",
      "At 90 step, loss is 0.000396331655792892\n",
      "At 100 step, loss is 0.0014510329347103834\n",
      "At 110 step, loss is 6.219422488129567e-08\n",
      "At 120 step, loss is 0.00038912540185265243\n",
      "At 130 step, loss is 2.6586460080579855e-05\n",
      "At 140 step, loss is 4.8892467020777985e-05\n",
      "At 150 step, loss is 0.0023288477677851915\n",
      "At 160 step, loss is 0.00031258980743587017\n",
      "At 170 step, loss is 0.00022134363825898618\n",
      "At 180 step, loss is 0.0012867557816207409\n",
      "At 190 step, loss is 2.8626289349631406e-05\n",
      "At 200 step, loss is 0.0003302190743852407\n",
      "At 210 step, loss is 0.0054620481096208096\n",
      "At 220 step, loss is 0.0002702981000766158\n",
      "At 230 step, loss is 0.0005724655347876251\n",
      "At 240 step, loss is 0.0021433155052363873\n",
      "Epoch 47\n",
      "At 0 step, loss is 5.671707549481653e-05\n",
      "At 10 step, loss is 0.006045110523700714\n",
      "At 20 step, loss is 0.00047567157889716327\n",
      "At 30 step, loss is 7.834940333850682e-05\n",
      "At 40 step, loss is 0.0017408838029950857\n",
      "At 50 step, loss is 0.00019769549544434994\n",
      "At 60 step, loss is 1.8211356291431002e-05\n",
      "At 70 step, loss is 3.004691507157986e-06\n",
      "At 80 step, loss is 0.00013250834308564663\n",
      "At 90 step, loss is 0.0008300154586322606\n",
      "At 100 step, loss is 0.0012250078143551946\n",
      "At 110 step, loss is 0.0009289035224355757\n",
      "At 120 step, loss is 0.004635805729776621\n",
      "At 130 step, loss is 3.2287812246067915e-07\n",
      "At 140 step, loss is 3.034115536593163e-08\n",
      "At 150 step, loss is 0.00024163987836800516\n",
      "At 160 step, loss is 0.0006572063430212438\n",
      "At 170 step, loss is 0.0027683074586093426\n",
      "At 180 step, loss is 0.00012557546142488718\n",
      "At 190 step, loss is 3.530346293700859e-05\n",
      "At 200 step, loss is 0.0003263860126025975\n",
      "At 210 step, loss is 0.00035538579686544836\n",
      "At 220 step, loss is 0.003186081303283572\n",
      "At 230 step, loss is 0.0008545502787455916\n",
      "At 240 step, loss is 0.0007738452404737473\n",
      "Epoch 48\n",
      "At 0 step, loss is 0.001676295418292284\n",
      "At 10 step, loss is 4.604366040439345e-05\n",
      "At 20 step, loss is 0.0021844154689460993\n",
      "At 30 step, loss is 0.0020132316276431084\n",
      "At 40 step, loss is 0.0006941254250705242\n",
      "At 50 step, loss is 0.0036282679066061974\n",
      "At 60 step, loss is 0.00022141821682453156\n",
      "At 70 step, loss is 0.0015816220548003912\n",
      "At 80 step, loss is 8.977635275186913e-07\n",
      "At 90 step, loss is 0.0019241689005866647\n",
      "At 100 step, loss is 0.0044225542806088924\n",
      "At 110 step, loss is 0.00016030816186685115\n",
      "At 120 step, loss is 0.0023583692964166403\n",
      "At 130 step, loss is 0.00026095984503626823\n",
      "At 140 step, loss is 3.605794699979015e-05\n",
      "At 150 step, loss is 0.0011032416950911283\n",
      "At 160 step, loss is 0.00020877363567706198\n",
      "At 170 step, loss is 0.0012911041267216206\n",
      "At 180 step, loss is 0.0016972009325399995\n",
      "At 190 step, loss is 0.0012057326966896653\n",
      "At 200 step, loss is 4.763250763062388e-05\n",
      "At 210 step, loss is 0.00023380698985420167\n",
      "At 220 step, loss is 6.197665697982302e-06\n",
      "At 230 step, loss is 0.0010647134622558951\n",
      "At 240 step, loss is 0.00012634502490982413\n",
      "Epoch 49\n",
      "At 0 step, loss is 0.00445705046877265\n",
      "At 10 step, loss is 0.0023141654673963785\n",
      "At 20 step, loss is 0.0029827654361724854\n",
      "At 30 step, loss is 0.0005976682878099382\n",
      "At 40 step, loss is 0.0002212592662544921\n",
      "At 50 step, loss is 0.00023235223488882184\n",
      "At 60 step, loss is 0.0003429642238188535\n",
      "At 70 step, loss is 0.0012524384073913097\n",
      "At 80 step, loss is 0.00011752774298656732\n",
      "At 90 step, loss is 0.00012698079808615148\n",
      "At 100 step, loss is 5.5416885516024195e-06\n",
      "At 110 step, loss is 0.000290360621875152\n",
      "At 120 step, loss is 8.765490929363295e-05\n",
      "At 130 step, loss is 0.0036989941727370024\n",
      "At 140 step, loss is 5.802418672828935e-05\n",
      "At 150 step, loss is 0.0016875977162271738\n",
      "At 160 step, loss is 0.0003969506360590458\n",
      "At 170 step, loss is 3.409431883483194e-05\n",
      "At 180 step, loss is 0.00017640333680901676\n",
      "At 190 step, loss is 0.003992429003119469\n",
      "At 200 step, loss is 0.0025262623094022274\n",
      "At 210 step, loss is 0.001742620370350778\n",
      "At 220 step, loss is 0.0003920273156836629\n",
      "At 230 step, loss is 0.00019185349810868502\n",
      "At 240 step, loss is 0.028475718572735786\n",
      "Epoch 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 0 step, loss is 2.0298826711950824e-05\n",
      "At 10 step, loss is 0.0007110314909368753\n",
      "At 20 step, loss is 6.191220563778188e-06\n",
      "At 30 step, loss is 1.0326972187613137e-05\n",
      "At 40 step, loss is 5.9428453823784366e-05\n",
      "At 50 step, loss is 0.0017319460166618228\n",
      "At 60 step, loss is 8.439241355517879e-05\n",
      "At 70 step, loss is 0.0017554021906107664\n",
      "At 80 step, loss is 0.002096040640026331\n",
      "At 90 step, loss is 4.6641194785479456e-05\n",
      "At 100 step, loss is 7.269606072668466e-08\n",
      "At 110 step, loss is 0.0010363080073148012\n",
      "At 120 step, loss is 0.0009649688727222383\n",
      "At 130 step, loss is 3.7701483961427584e-05\n",
      "At 140 step, loss is 0.000885848596226424\n",
      "At 150 step, loss is 6.224013486644253e-05\n",
      "At 160 step, loss is 5.528506153495982e-05\n",
      "At 170 step, loss is 0.0010733285453170538\n",
      "At 180 step, loss is 4.1697467167978175e-07\n",
      "At 190 step, loss is 3.553126589395106e-05\n",
      "At 200 step, loss is 1.1078502211603336e-05\n",
      "At 210 step, loss is 2.266209412482567e-05\n",
      "At 220 step, loss is 9.060112643055618e-05\n",
      "At 230 step, loss is 5.2843306548311375e-06\n",
      "At 240 step, loss is 0.015305577777326107\n",
      "Epoch 51\n",
      "At 0 step, loss is 0.0006095724529586732\n",
      "At 10 step, loss is 0.00014342497161123902\n",
      "At 20 step, loss is 3.5811899579130113e-06\n",
      "At 30 step, loss is 0.0015141032636165619\n",
      "At 40 step, loss is 4.1650932303127775e-07\n",
      "At 50 step, loss is 0.0013450861442834139\n",
      "At 60 step, loss is 2.51684639351879e-07\n",
      "At 70 step, loss is 0.00013178282824810594\n",
      "At 80 step, loss is 0.005560684017837048\n",
      "At 90 step, loss is 0.0002317961334483698\n",
      "At 100 step, loss is 3.9561873563798144e-05\n",
      "At 110 step, loss is 2.2013798570696963e-06\n",
      "At 120 step, loss is 0.002503547351807356\n",
      "At 130 step, loss is 0.007446824572980404\n",
      "At 140 step, loss is 6.923901673872024e-05\n",
      "At 150 step, loss is 0.0007393120904453099\n",
      "At 160 step, loss is 0.00029247207567095757\n",
      "At 170 step, loss is 0.00010200705582974479\n",
      "At 180 step, loss is 5.330023486749269e-05\n",
      "At 190 step, loss is 0.0003516518045216799\n",
      "At 200 step, loss is 0.0014221101300790906\n",
      "At 210 step, loss is 0.00011170341895194724\n",
      "At 220 step, loss is 0.003736695507541299\n",
      "At 230 step, loss is 0.0002935588709078729\n",
      "At 240 step, loss is 0.0013809516094624996\n",
      "Epoch 52\n",
      "At 0 step, loss is 0.00045576461707241833\n",
      "At 10 step, loss is 0.0012398646213114262\n",
      "At 20 step, loss is 0.0006849348428659141\n",
      "At 30 step, loss is 2.804492396535352e-06\n",
      "At 40 step, loss is 5.392781167756766e-05\n",
      "At 50 step, loss is 0.00228657154366374\n",
      "At 60 step, loss is 3.636412202467909e-06\n",
      "At 70 step, loss is 8.034999336814508e-05\n",
      "At 80 step, loss is 0.02169007435441017\n",
      "At 90 step, loss is 0.001272661262191832\n",
      "At 100 step, loss is 1.915514985739719e-05\n",
      "At 110 step, loss is 3.4315914945182158e-06\n",
      "At 120 step, loss is 0.00011121665738755837\n",
      "At 130 step, loss is 0.0010963155655190349\n",
      "At 140 step, loss is 0.00011902002734132111\n",
      "At 150 step, loss is 0.00037776416866108775\n",
      "At 160 step, loss is 1.447132854082156e-05\n",
      "At 170 step, loss is 1.7895647033583373e-05\n",
      "At 180 step, loss is 8.583514863858e-05\n",
      "At 190 step, loss is 0.005623524077236652\n",
      "At 200 step, loss is 0.0001291311637032777\n",
      "At 210 step, loss is 3.137797102681361e-05\n",
      "At 220 step, loss is 1.569178311910946e-05\n",
      "At 230 step, loss is 0.00020583870355039835\n",
      "At 240 step, loss is 2.858090738300234e-06\n",
      "Epoch 53\n",
      "At 0 step, loss is 0.0026222874876111746\n",
      "At 10 step, loss is 7.691329665249214e-05\n",
      "At 20 step, loss is 2.250939132864005e-07\n",
      "At 30 step, loss is 4.770968735101633e-05\n",
      "At 40 step, loss is 1.6391002645832486e-05\n",
      "At 50 step, loss is 0.0011614452814683318\n",
      "At 60 step, loss is 0.0016537695191800594\n",
      "At 70 step, loss is 0.0003586284292396158\n",
      "At 80 step, loss is 0.0008215485722757876\n",
      "At 90 step, loss is 7.269583875313401e-05\n",
      "At 100 step, loss is 0.00021448914776556194\n",
      "At 110 step, loss is 0.002233140869066119\n",
      "At 120 step, loss is 0.00011817946506198496\n",
      "At 130 step, loss is 0.00011985565652139485\n",
      "At 140 step, loss is 4.57405622000806e-05\n",
      "At 150 step, loss is 2.0672596292570233e-05\n",
      "At 160 step, loss is 0.002818786073476076\n",
      "At 170 step, loss is 0.00017041682440321892\n",
      "At 180 step, loss is 0.002092615934088826\n",
      "At 190 step, loss is 0.0007367040379904211\n",
      "At 200 step, loss is 0.00022138240456115454\n",
      "At 210 step, loss is 7.044044468784705e-05\n",
      "At 220 step, loss is 0.0014004702679812908\n",
      "At 230 step, loss is 0.003255126066505909\n",
      "At 240 step, loss is 0.0001738111168378964\n",
      "Epoch 54\n",
      "At 0 step, loss is 5.9401398175396025e-05\n",
      "At 10 step, loss is 0.0002762290823739022\n",
      "At 20 step, loss is 6.05194982199464e-05\n",
      "At 30 step, loss is 6.527236109832302e-05\n",
      "At 40 step, loss is 1.2304317351663485e-06\n",
      "At 50 step, loss is 8.365216490346938e-05\n",
      "At 60 step, loss is 0.00012603765935637057\n",
      "At 70 step, loss is 0.00010586059943307191\n",
      "At 80 step, loss is 0.0004026131355203688\n",
      "At 90 step, loss is 7.45634752092883e-05\n",
      "At 100 step, loss is 2.092034264933318e-05\n",
      "At 110 step, loss is 1.768654378508927e-08\n",
      "At 120 step, loss is 0.0005602213204838336\n",
      "At 130 step, loss is 0.0019061441998928785\n",
      "At 140 step, loss is 1.9018507373402826e-05\n",
      "At 150 step, loss is 0.011847484856843948\n",
      "At 160 step, loss is 0.002026135567575693\n",
      "At 170 step, loss is 0.00039632178959436715\n",
      "At 180 step, loss is 0.0014633939135819674\n",
      "At 190 step, loss is 0.0004959713551215827\n",
      "At 200 step, loss is 6.508478200828449e-10\n",
      "At 210 step, loss is 0.0005000269156880677\n",
      "At 220 step, loss is 0.0018555659335106611\n",
      "At 230 step, loss is 1.587131009728182e-05\n",
      "At 240 step, loss is 3.620966890593991e-05\n",
      "Epoch 55\n",
      "At 0 step, loss is 0.00012262072414159775\n",
      "At 10 step, loss is 0.00035488425055518746\n",
      "At 20 step, loss is 3.106768417637795e-05\n",
      "At 30 step, loss is 3.5322734674991807e-06\n",
      "At 40 step, loss is 0.003991199191659689\n",
      "At 50 step, loss is 0.0016712476499378681\n",
      "At 60 step, loss is 0.00013989335275255144\n",
      "At 70 step, loss is 0.0001144621055573225\n",
      "At 80 step, loss is 2.30734749493422e-05\n",
      "At 90 step, loss is 0.0003238939098082483\n",
      "At 100 step, loss is 0.001119051012210548\n",
      "At 110 step, loss is 0.0003366790770087391\n",
      "At 120 step, loss is 0.0002502611023373902\n",
      "At 130 step, loss is 0.0009376818779855967\n",
      "At 140 step, loss is 0.0028056034352630377\n",
      "At 150 step, loss is 0.0014948972966521978\n",
      "At 160 step, loss is 0.0005697639426216483\n",
      "At 170 step, loss is 0.00021532435494009405\n",
      "At 180 step, loss is 8.202626486308873e-05\n",
      "At 190 step, loss is 0.0007347514037974179\n",
      "At 200 step, loss is 0.00019985323888249695\n",
      "At 210 step, loss is 4.1195009544026107e-05\n",
      "At 220 step, loss is 0.00032321998151019216\n",
      "At 230 step, loss is 0.00010850966646103188\n",
      "At 240 step, loss is 0.00079666095552966\n",
      "Epoch 56\n",
      "At 0 step, loss is 0.00048505779705010355\n",
      "At 10 step, loss is 1.1260185601713601e-05\n",
      "At 20 step, loss is 0.0004213138890918344\n",
      "At 30 step, loss is 1.8968409065678316e-08\n",
      "At 40 step, loss is 4.40000876551494e-05\n",
      "At 50 step, loss is 0.0006208575796335936\n",
      "At 60 step, loss is 0.001041643787175417\n",
      "At 70 step, loss is 0.00033287127735093236\n",
      "At 80 step, loss is 5.119125489727594e-05\n",
      "At 90 step, loss is 0.00017693800327833742\n",
      "At 100 step, loss is 0.00016174120537471026\n",
      "At 110 step, loss is 0.0007916261092759669\n",
      "At 120 step, loss is 0.0007064712699502707\n",
      "At 130 step, loss is 0.0006283832481130958\n",
      "At 140 step, loss is 4.582026303978637e-05\n",
      "At 150 step, loss is 0.00014000154624227434\n",
      "At 160 step, loss is 2.4784299057500903e-06\n",
      "At 170 step, loss is 2.3662832973059267e-05\n",
      "At 180 step, loss is 7.685477612540126e-05\n",
      "At 190 step, loss is 0.0001897460169857368\n",
      "At 200 step, loss is 0.0010618545347824693\n",
      "At 210 step, loss is 0.0005139716085977852\n",
      "At 220 step, loss is 0.0003043089818675071\n",
      "At 230 step, loss is 5.169903670321219e-05\n",
      "At 240 step, loss is 0.0004657345707528293\n",
      "Epoch 57\n",
      "At 0 step, loss is 0.00042649434180930257\n",
      "At 10 step, loss is 0.019704680889844894\n",
      "At 20 step, loss is 0.0005757405888289213\n",
      "At 30 step, loss is 3.3335293210257078e-06\n",
      "At 40 step, loss is 0.000728224462363869\n",
      "At 50 step, loss is 0.001844261889345944\n",
      "At 60 step, loss is 0.000647799635771662\n",
      "At 70 step, loss is 2.8139631467638537e-05\n",
      "At 80 step, loss is 0.0015986573416739702\n",
      "At 90 step, loss is 0.00010313024540664628\n",
      "At 100 step, loss is 3.2543692213948816e-05\n",
      "At 110 step, loss is 0.0001183198910439387\n",
      "At 120 step, loss is 0.0027339935768395662\n",
      "At 130 step, loss is 0.0006465698825195432\n",
      "At 140 step, loss is 9.533933189231902e-05\n",
      "At 150 step, loss is 0.0004689955385401845\n",
      "At 160 step, loss is 0.0015409478219226003\n",
      "At 170 step, loss is 6.515788845717907e-05\n",
      "At 180 step, loss is 2.2775806428398937e-05\n",
      "At 190 step, loss is 9.610555935068987e-06\n",
      "At 200 step, loss is 0.00011516236554598436\n",
      "At 210 step, loss is 0.00023726533981971443\n",
      "At 220 step, loss is 0.0065238820388913155\n",
      "At 230 step, loss is 0.0001293622044613585\n",
      "At 240 step, loss is 0.0008145819301716983\n",
      "Epoch 58\n",
      "At 0 step, loss is 0.00022028584498912096\n",
      "At 10 step, loss is 0.00042267460958100855\n",
      "At 20 step, loss is 0.0020957612432539463\n",
      "At 30 step, loss is 0.0002359510981477797\n",
      "At 40 step, loss is 8.413871910306625e-06\n",
      "At 50 step, loss is 0.0003799360420089215\n",
      "At 60 step, loss is 0.0006889512878842652\n",
      "At 70 step, loss is 6.784521974623203e-05\n",
      "At 80 step, loss is 1.0218141142104287e-06\n",
      "At 90 step, loss is 0.0006851645302958786\n",
      "At 100 step, loss is 1.8234720755572198e-06\n",
      "At 110 step, loss is 2.5818186259130016e-05\n",
      "At 120 step, loss is 6.792623480578186e-06\n",
      "At 130 step, loss is 0.0006876938859932125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 140 step, loss is 0.0002394542098045349\n",
      "At 150 step, loss is 0.005940571427345276\n",
      "At 160 step, loss is 0.00016998915816657245\n",
      "At 170 step, loss is 0.000110887638584245\n",
      "At 180 step, loss is 3.432977609918453e-05\n",
      "At 190 step, loss is 0.0010541032534092665\n",
      "At 200 step, loss is 0.0005019710515625775\n",
      "At 210 step, loss is 9.83779682428576e-05\n",
      "At 220 step, loss is 1.2850496204919182e-05\n",
      "At 230 step, loss is 0.0002227877703262493\n",
      "At 240 step, loss is 0.0004776109999511391\n",
      "Epoch 59\n",
      "At 0 step, loss is 0.00332414242438972\n",
      "At 10 step, loss is 0.0003224168613087386\n",
      "At 20 step, loss is 0.00010002704220823944\n",
      "At 30 step, loss is 0.0002315006568096578\n",
      "At 40 step, loss is 4.4387648813426495e-05\n",
      "At 50 step, loss is 3.1885156204225495e-06\n",
      "At 60 step, loss is 0.0053351866081357\n",
      "At 70 step, loss is 0.0001737561251502484\n",
      "At 80 step, loss is 0.002803794341161847\n",
      "At 90 step, loss is 1.7956847386813024e-06\n",
      "At 100 step, loss is 0.0014282831689342856\n",
      "At 110 step, loss is 0.0015113982371985912\n",
      "At 120 step, loss is 0.004893708974123001\n",
      "At 130 step, loss is 0.00255944044329226\n",
      "At 140 step, loss is 0.00030140639864839613\n",
      "At 150 step, loss is 9.539573511574417e-05\n",
      "At 160 step, loss is 0.0001453919248888269\n",
      "At 170 step, loss is 0.0003060726448893547\n",
      "At 180 step, loss is 4.048722985316999e-06\n",
      "At 190 step, loss is 0.00019613112090155482\n",
      "At 200 step, loss is 2.6763986170408316e-05\n",
      "At 210 step, loss is 0.0017675174167379737\n",
      "At 220 step, loss is 0.0016181519022211432\n",
      "At 230 step, loss is 0.00031641407986171544\n",
      "At 240 step, loss is 0.0002086320164380595\n",
      "Epoch 60\n",
      "At 0 step, loss is 2.6336258088122122e-05\n",
      "At 10 step, loss is 0.0004251808568369597\n",
      "At 20 step, loss is 0.0018272115848958492\n",
      "At 30 step, loss is 0.00026915964554063976\n",
      "At 40 step, loss is 0.00012897273700218648\n",
      "At 50 step, loss is 0.00040413180249743164\n",
      "At 60 step, loss is 0.004072085954248905\n",
      "At 70 step, loss is 0.0006128202076070011\n",
      "At 80 step, loss is 0.0006089424132369459\n",
      "At 90 step, loss is 0.0005251530674286187\n",
      "At 100 step, loss is 7.10963795427233e-05\n",
      "At 110 step, loss is 0.0020497688092291355\n",
      "At 120 step, loss is 0.0002536198007874191\n",
      "At 130 step, loss is 0.0003596029127947986\n",
      "At 140 step, loss is 0.00010743602615548298\n",
      "At 150 step, loss is 0.0014942515408620238\n",
      "At 160 step, loss is 0.0008399280486628413\n",
      "At 170 step, loss is 0.001197701203636825\n",
      "At 180 step, loss is 8.028734737308696e-05\n",
      "At 190 step, loss is 0.0005214781849645078\n",
      "At 200 step, loss is 3.190784354956122e-06\n",
      "At 210 step, loss is 0.00023584339942317456\n",
      "At 220 step, loss is 4.248682307661511e-05\n",
      "At 230 step, loss is 5.97101534367539e-05\n",
      "At 240 step, loss is 9.785580914467573e-05\n",
      "Epoch 61\n",
      "At 0 step, loss is 0.0001679060369497165\n",
      "At 10 step, loss is 0.0003044975455850363\n",
      "At 20 step, loss is 0.0022732869256287813\n",
      "At 30 step, loss is 0.0005400156951509416\n",
      "At 40 step, loss is 0.00016267622413579375\n",
      "At 50 step, loss is 0.00040381800499744713\n",
      "At 60 step, loss is 0.0004196250229142606\n",
      "At 70 step, loss is 1.2153563147876412e-05\n",
      "At 80 step, loss is 3.8244340316850867e-07\n",
      "At 90 step, loss is 2.1866981114726514e-05\n",
      "At 100 step, loss is 7.430126515828306e-06\n",
      "At 110 step, loss is 0.0026602346915751696\n",
      "At 120 step, loss is 0.0021259491331875324\n",
      "At 130 step, loss is 7.064965757308528e-05\n",
      "At 140 step, loss is 0.0008033922640606761\n",
      "At 150 step, loss is 0.00036371772876009345\n",
      "At 160 step, loss is 0.00378041947260499\n",
      "At 170 step, loss is 0.004044473171234131\n",
      "At 180 step, loss is 0.0007114227046258748\n",
      "At 190 step, loss is 6.5919789449253585e-06\n",
      "At 200 step, loss is 0.0004235763044562191\n",
      "At 210 step, loss is 0.0006334285717457533\n",
      "At 220 step, loss is 2.9245384212117642e-05\n",
      "At 230 step, loss is 0.00019451297703199089\n",
      "At 240 step, loss is 1.533834392830613e-06\n",
      "Epoch 62\n",
      "At 0 step, loss is 0.0007519865175709128\n",
      "At 10 step, loss is 0.0015205630334094167\n",
      "At 20 step, loss is 0.004877661820501089\n",
      "At 30 step, loss is 0.0020912985783070326\n",
      "At 40 step, loss is 0.0006471763481386006\n",
      "At 50 step, loss is 1.3092040717310738e-05\n",
      "At 60 step, loss is 1.804002749850042e-05\n",
      "At 70 step, loss is 0.0003358553512953222\n",
      "At 80 step, loss is 2.4859227778506465e-06\n",
      "At 90 step, loss is 0.0013691792264580727\n",
      "At 100 step, loss is 3.6749097489519045e-05\n",
      "At 110 step, loss is 0.0004341370949987322\n",
      "At 120 step, loss is 2.488981408532709e-05\n",
      "At 130 step, loss is 0.001423092558979988\n",
      "At 140 step, loss is 5.047327249485534e-06\n",
      "At 150 step, loss is 0.0006734186317771673\n",
      "At 160 step, loss is 0.0012324103154242039\n",
      "At 170 step, loss is 3.235839187709644e-07\n",
      "At 180 step, loss is 0.0002984760794788599\n",
      "At 190 step, loss is 0.0012639083433896303\n",
      "At 200 step, loss is 0.004993577022105455\n",
      "At 210 step, loss is 0.0020268121734261513\n",
      "At 220 step, loss is 0.0010149675654247403\n",
      "At 230 step, loss is 0.0006035230471752584\n",
      "At 240 step, loss is 0.0001846972736530006\n",
      "Epoch 63\n",
      "At 0 step, loss is 0.0018300666706636548\n",
      "At 10 step, loss is 2.4265727915917523e-06\n",
      "At 20 step, loss is 0.00021206184464972466\n",
      "At 30 step, loss is 0.00015278233331628144\n",
      "At 40 step, loss is 1.2766092368110549e-05\n",
      "At 50 step, loss is 0.0011571102077141404\n",
      "At 60 step, loss is 0.0005440051318146288\n",
      "At 70 step, loss is 0.0006550457910634577\n",
      "At 80 step, loss is 0.00020641757873818278\n",
      "At 90 step, loss is 7.641426236659754e-06\n",
      "At 100 step, loss is 0.00016411014075856656\n",
      "At 110 step, loss is 0.00015363330021500587\n",
      "At 120 step, loss is 0.00012008317571599036\n",
      "At 130 step, loss is 0.004296183120459318\n",
      "At 140 step, loss is 0.0005700522451661527\n",
      "At 150 step, loss is 0.0020298934541642666\n",
      "At 160 step, loss is 0.0025167232379317284\n",
      "At 170 step, loss is 0.00016714986122678965\n",
      "At 180 step, loss is 0.0010846254881471395\n",
      "At 190 step, loss is 0.0002583083405625075\n",
      "At 200 step, loss is 0.0008930289186537266\n",
      "At 210 step, loss is 0.0009227881673723459\n",
      "At 220 step, loss is 4.075580363860354e-05\n",
      "At 230 step, loss is 0.0003416823747102171\n",
      "At 240 step, loss is 0.00012699993385467678\n",
      "Epoch 64\n",
      "At 0 step, loss is 4.678419645642862e-05\n",
      "At 10 step, loss is 3.684535840875469e-05\n",
      "At 20 step, loss is 0.0017235242994502187\n",
      "At 30 step, loss is 1.152210188593017e-05\n",
      "At 40 step, loss is 0.00034352237707935274\n",
      "At 50 step, loss is 0.0009693398606032133\n",
      "At 60 step, loss is 0.0004904117085970938\n",
      "At 70 step, loss is 0.002305784495547414\n",
      "At 80 step, loss is 0.00016876627341844141\n",
      "At 90 step, loss is 0.0009471088415011764\n",
      "At 100 step, loss is 1.344122892987798e-07\n",
      "At 110 step, loss is 0.0012222137302160263\n",
      "At 120 step, loss is 9.000708087114617e-05\n",
      "At 130 step, loss is 0.00020647214842028916\n",
      "At 140 step, loss is 0.000405904371291399\n",
      "At 150 step, loss is 3.370204285602085e-05\n",
      "At 160 step, loss is 0.0010824226774275303\n",
      "At 170 step, loss is 0.0017243599286302924\n",
      "At 180 step, loss is 0.002933515002951026\n",
      "At 190 step, loss is 8.940031693782657e-05\n",
      "At 200 step, loss is 0.00182931253220886\n",
      "At 210 step, loss is 0.00013272384239826351\n",
      "At 220 step, loss is 7.413801358779892e-05\n",
      "At 230 step, loss is 0.00028796514379791915\n",
      "At 240 step, loss is 5.6457964092260227e-05\n",
      "Epoch 65\n",
      "At 0 step, loss is 0.00025969077250920236\n",
      "At 10 step, loss is 1.2655375030590221e-05\n",
      "At 20 step, loss is 0.00118897738866508\n",
      "At 30 step, loss is 0.0001723327295621857\n",
      "At 40 step, loss is 0.00039648081292398274\n",
      "At 50 step, loss is 0.0017006552079692483\n",
      "At 60 step, loss is 0.0016394380945712328\n",
      "At 70 step, loss is 8.036118379095569e-05\n",
      "At 80 step, loss is 0.00019736312970053405\n",
      "At 90 step, loss is 5.760899739470915e-07\n",
      "At 100 step, loss is 7.933961114758858e-07\n",
      "At 110 step, loss is 2.6967580197378993e-05\n",
      "At 120 step, loss is 6.943494099687086e-06\n",
      "At 130 step, loss is 0.0001609827158972621\n",
      "At 140 step, loss is 0.004667365457862616\n",
      "At 150 step, loss is 0.0006360754487104714\n",
      "At 160 step, loss is 0.000834616192150861\n",
      "At 170 step, loss is 0.00024967669742181897\n",
      "At 180 step, loss is 4.281293541907871e-08\n",
      "At 190 step, loss is 0.0002516875392757356\n",
      "At 200 step, loss is 7.493884186260402e-05\n",
      "At 210 step, loss is 1.994387093873229e-05\n",
      "At 220 step, loss is 0.00046761875273659825\n",
      "At 230 step, loss is 0.0005456290673464537\n",
      "At 240 step, loss is 0.0002517167304176837\n",
      "Epoch 66\n",
      "At 0 step, loss is 0.00035229107015766203\n",
      "At 10 step, loss is 0.004912691190838814\n",
      "At 20 step, loss is 0.0020604869350790977\n",
      "At 30 step, loss is 0.0004566606949083507\n",
      "At 40 step, loss is 0.009322868660092354\n",
      "At 50 step, loss is 0.00011045351129723713\n",
      "At 60 step, loss is 0.0007012046407908201\n",
      "At 70 step, loss is 1.6533122106920928e-06\n",
      "At 80 step, loss is 0.0005035895155742764\n",
      "At 90 step, loss is 0.0012574265711009502\n",
      "At 100 step, loss is 0.0052645509131252766\n",
      "At 110 step, loss is 0.002500749658793211\n",
      "At 120 step, loss is 0.0011793209705501795\n",
      "At 130 step, loss is 0.00019956605683546513\n",
      "At 140 step, loss is 0.0007931932341307402\n",
      "At 150 step, loss is 7.470374112017453e-05\n",
      "At 160 step, loss is 0.00020824566308874637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 170 step, loss is 1.242400344381167e-06\n",
      "At 180 step, loss is 0.0013648169115185738\n",
      "At 190 step, loss is 0.0055028535425662994\n",
      "At 200 step, loss is 0.00015762295515742153\n",
      "At 210 step, loss is 0.00042282912181690335\n",
      "At 220 step, loss is 3.226633998565376e-05\n",
      "At 230 step, loss is 0.0015608310932293534\n",
      "At 240 step, loss is 0.00015461987641174346\n",
      "Epoch 67\n",
      "At 0 step, loss is 4.5592627429869026e-05\n",
      "At 10 step, loss is 0.0005080124246887863\n",
      "At 20 step, loss is 0.010246329009532928\n",
      "At 30 step, loss is 0.0003978511376772076\n",
      "At 40 step, loss is 0.0012241709046065807\n",
      "At 50 step, loss is 0.0001354260602965951\n",
      "At 60 step, loss is 0.00021825281146448106\n",
      "At 70 step, loss is 0.005407158751040697\n",
      "At 80 step, loss is 0.0011320614721626043\n",
      "At 90 step, loss is 0.0007976515917107463\n",
      "At 100 step, loss is 8.27247349661775e-05\n",
      "At 110 step, loss is 0.001141975517384708\n",
      "At 120 step, loss is 0.0002407997817499563\n",
      "At 130 step, loss is 0.0003909406950697303\n",
      "At 140 step, loss is 0.0005418561631813645\n",
      "At 150 step, loss is 0.0006287830765359104\n",
      "At 160 step, loss is 0.00031907984521239996\n",
      "At 170 step, loss is 4.9530062824487686e-05\n",
      "At 180 step, loss is 1.4478179764410015e-05\n",
      "At 190 step, loss is 0.0001941829250426963\n",
      "At 200 step, loss is 0.0003920983290299773\n",
      "At 210 step, loss is 0.0012920750305056572\n",
      "At 220 step, loss is 0.00827447697520256\n",
      "At 230 step, loss is 0.00033985154004767537\n",
      "At 240 step, loss is 0.004771899431943893\n",
      "Epoch 68\n",
      "At 0 step, loss is 0.0013535870239138603\n",
      "At 10 step, loss is 0.003134416649118066\n",
      "At 20 step, loss is 0.00019876060832757503\n",
      "At 30 step, loss is 0.0003211555886082351\n",
      "At 40 step, loss is 0.00011338623153278604\n",
      "At 50 step, loss is 3.2356558222090825e-05\n",
      "At 60 step, loss is 0.002532483544200659\n",
      "At 70 step, loss is 8.312777208629996e-05\n",
      "At 80 step, loss is 7.74912114138715e-05\n",
      "At 90 step, loss is 0.002138257957994938\n",
      "At 100 step, loss is 0.0028274604119360447\n",
      "At 110 step, loss is 4.157811781624332e-05\n",
      "At 120 step, loss is 8.026568684726954e-05\n",
      "At 130 step, loss is 0.00062264793086797\n",
      "At 140 step, loss is 1.0896472986132721e-06\n",
      "At 150 step, loss is 0.00010747135820565745\n",
      "At 160 step, loss is 0.00019152714230585843\n",
      "At 170 step, loss is 0.004197065718472004\n",
      "At 180 step, loss is 5.949324622633867e-05\n",
      "At 190 step, loss is 3.8251342630246654e-05\n",
      "At 200 step, loss is 0.0009027095511555672\n",
      "At 210 step, loss is 0.00019637147488538176\n",
      "At 220 step, loss is 0.0005480357795022428\n",
      "At 230 step, loss is 1.669176526775118e-05\n",
      "At 240 step, loss is 0.0024981761816889048\n",
      "Epoch 69\n",
      "At 0 step, loss is 0.00263053085654974\n",
      "At 10 step, loss is 0.0005455933278426528\n",
      "At 20 step, loss is 0.0005093680229038\n",
      "At 30 step, loss is 0.00012091471580788493\n",
      "At 40 step, loss is 0.0025422682520002127\n",
      "At 50 step, loss is 1.4171256225381512e-05\n",
      "At 60 step, loss is 0.0012532399268820882\n",
      "At 70 step, loss is 3.328312914163689e-07\n",
      "At 80 step, loss is 3.9829265006119385e-05\n",
      "At 90 step, loss is 1.964424882316962e-05\n",
      "At 100 step, loss is 6.389947520801798e-05\n",
      "At 110 step, loss is 1.5341482139774598e-05\n",
      "At 120 step, loss is 0.00010879705951083452\n",
      "At 130 step, loss is 0.002576478524133563\n",
      "At 140 step, loss is 0.0009106761426664889\n",
      "At 150 step, loss is 0.012827027589082718\n",
      "At 160 step, loss is 0.002741781994700432\n",
      "At 170 step, loss is 3.2866171295609092e-06\n",
      "At 180 step, loss is 0.0001271741057280451\n",
      "At 190 step, loss is 0.0003379872941877693\n",
      "At 200 step, loss is 6.08181107963901e-06\n",
      "At 210 step, loss is 0.00012530344247352332\n",
      "At 220 step, loss is 0.0012208317639306188\n",
      "At 230 step, loss is 0.00016021417104639113\n",
      "At 240 step, loss is 9.16755961952731e-05\n",
      "Epoch 70\n",
      "At 0 step, loss is 0.0023097721859812737\n",
      "At 10 step, loss is 0.00011618516873568296\n",
      "At 20 step, loss is 0.010653768666088581\n",
      "At 30 step, loss is 0.0009512034012004733\n",
      "At 40 step, loss is 0.00040917147998698056\n",
      "At 50 step, loss is 0.00021342909894883633\n",
      "At 60 step, loss is 0.00014742935309186578\n",
      "At 70 step, loss is 5.667545337928459e-05\n",
      "At 80 step, loss is 0.0016523606609553099\n",
      "At 90 step, loss is 4.9126661906484514e-05\n",
      "At 100 step, loss is 0.0014719568425789475\n",
      "At 110 step, loss is 0.014906158670783043\n",
      "At 120 step, loss is 0.0010602044640108943\n",
      "At 130 step, loss is 0.00040370176429860294\n",
      "At 140 step, loss is 0.00014147668844088912\n",
      "At 150 step, loss is 0.000272148143267259\n",
      "At 160 step, loss is 0.000198503999854438\n",
      "At 170 step, loss is 0.000933803035877645\n",
      "At 180 step, loss is 0.0007835378055460751\n",
      "At 190 step, loss is 0.0036691073328256607\n",
      "At 200 step, loss is 0.0003384944866411388\n",
      "At 210 step, loss is 0.00025936568272300065\n",
      "At 220 step, loss is 0.00032642236328683794\n",
      "At 230 step, loss is 3.404644758120412e-06\n",
      "At 240 step, loss is 0.002792282262817025\n",
      "Epoch 71\n",
      "At 0 step, loss is 9.019928984344006e-05\n",
      "At 10 step, loss is 0.0008070365875028074\n",
      "At 20 step, loss is 1.905957469716668e-05\n",
      "At 30 step, loss is 0.0009473692043684423\n",
      "At 40 step, loss is 0.0002176942361984402\n",
      "At 50 step, loss is 0.0009341430268250406\n",
      "At 60 step, loss is 1.663899456616491e-05\n",
      "At 70 step, loss is 0.0002761690120678395\n",
      "At 80 step, loss is 3.170378113281913e-05\n",
      "At 90 step, loss is 0.001044530188664794\n",
      "At 100 step, loss is 0.00014582979201804847\n",
      "At 110 step, loss is 1.0149026820727158e-05\n",
      "At 120 step, loss is 1.5033815543574747e-05\n",
      "At 130 step, loss is 0.0002818591019604355\n",
      "At 140 step, loss is 0.000528227596078068\n",
      "At 150 step, loss is 3.7341200368246064e-05\n",
      "At 160 step, loss is 0.00031790780485607684\n",
      "At 170 step, loss is 0.00022822555911261588\n",
      "At 180 step, loss is 0.00012588842946570367\n",
      "At 190 step, loss is 6.077032594475895e-06\n",
      "At 200 step, loss is 2.239599052700214e-05\n",
      "At 210 step, loss is 0.0004343865148257464\n",
      "At 220 step, loss is 3.346182347740978e-05\n",
      "At 230 step, loss is 0.0010535761248320341\n",
      "At 240 step, loss is 7.646069207112305e-06\n",
      "Epoch 72\n",
      "At 0 step, loss is 0.00045293939183466136\n",
      "At 10 step, loss is 0.0006498744478449225\n",
      "At 20 step, loss is 0.00012623931979760528\n",
      "At 30 step, loss is 2.250293255201541e-05\n",
      "At 40 step, loss is 0.0004939472419209778\n",
      "At 50 step, loss is 0.0005664013442583382\n",
      "At 60 step, loss is 0.00030419020913541317\n",
      "At 70 step, loss is 0.0003261356323491782\n",
      "At 80 step, loss is 5.149320713826455e-05\n",
      "At 90 step, loss is 0.0041397991590201855\n",
      "At 100 step, loss is 7.218710379675031e-05\n",
      "At 110 step, loss is 0.0015839679399505258\n",
      "At 120 step, loss is 0.0004464678931981325\n",
      "At 130 step, loss is 3.5392044082982466e-05\n",
      "At 140 step, loss is 0.0005841902457177639\n",
      "At 150 step, loss is 6.149428521950995e-09\n",
      "At 160 step, loss is 0.00014273700071498752\n",
      "At 170 step, loss is 0.0008050082833506167\n",
      "At 180 step, loss is 0.0014969628537073731\n",
      "At 190 step, loss is 0.0005618915893137455\n",
      "At 200 step, loss is 0.001220859820023179\n",
      "At 210 step, loss is 5.950508693786105e-06\n",
      "At 220 step, loss is 0.0005103697185404599\n",
      "At 230 step, loss is 0.000566007976885885\n",
      "At 240 step, loss is 0.001663109054788947\n",
      "Epoch 73\n",
      "At 0 step, loss is 2.1522771930904128e-05\n",
      "At 10 step, loss is 2.1642283172695898e-05\n",
      "At 20 step, loss is 4.1423863876843825e-05\n",
      "At 30 step, loss is 2.9173994334996678e-05\n",
      "At 40 step, loss is 0.0008660965249873698\n",
      "At 50 step, loss is 6.39662266621599e-06\n",
      "At 60 step, loss is 0.0016625226708129048\n",
      "At 70 step, loss is 0.0004478015180211514\n",
      "At 80 step, loss is 0.000761626404710114\n",
      "At 90 step, loss is 0.00499134324491024\n",
      "At 100 step, loss is 0.0007754425168968737\n",
      "At 110 step, loss is 0.0024037580005824566\n",
      "At 120 step, loss is 6.349867180688307e-05\n",
      "At 130 step, loss is 0.00020007704733870924\n",
      "At 140 step, loss is 1.3800475244352128e-05\n",
      "At 150 step, loss is 0.008656658232212067\n",
      "At 160 step, loss is 3.515824937494472e-05\n",
      "At 170 step, loss is 0.050413116812705994\n",
      "At 180 step, loss is 0.006021434441208839\n",
      "At 190 step, loss is 7.271701178979129e-05\n",
      "At 200 step, loss is 8.511319720128085e-06\n",
      "At 210 step, loss is 0.0006738725933246315\n",
      "At 220 step, loss is 5.534411684493534e-07\n",
      "At 230 step, loss is 0.00020289754320401698\n",
      "At 240 step, loss is 1.7882155589177273e-05\n",
      "Epoch 74\n",
      "At 0 step, loss is 1.8928878489532508e-05\n",
      "At 10 step, loss is 0.0022574285976588726\n",
      "At 20 step, loss is 4.241480928612873e-05\n",
      "At 30 step, loss is 0.0005321197095327079\n",
      "At 40 step, loss is 0.00034088685060851276\n",
      "At 50 step, loss is 0.00010250777995679528\n",
      "At 60 step, loss is 0.0010672048665583134\n",
      "At 70 step, loss is 1.967201751540415e-05\n",
      "At 80 step, loss is 0.0010265122400596738\n",
      "At 90 step, loss is 4.156760837759066e-07\n",
      "At 100 step, loss is 0.002484762342646718\n",
      "At 110 step, loss is 2.4585449864389375e-05\n",
      "At 120 step, loss is 6.542349001392722e-05\n",
      "At 130 step, loss is 0.0005706559168174863\n",
      "At 140 step, loss is 0.00029639387503266335\n",
      "At 150 step, loss is 0.0021557649597525597\n",
      "At 160 step, loss is 7.046030077617615e-05\n",
      "At 170 step, loss is 2.3237018467625603e-05\n",
      "At 180 step, loss is 0.00036639749305322766\n",
      "At 190 step, loss is 7.981515892652169e-08\n",
      "At 200 step, loss is 1.0654506468199543e-06\n",
      "At 210 step, loss is 1.2770432476827409e-05\n",
      "At 220 step, loss is 0.0001517120690550655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 230 step, loss is 2.6423693270771764e-05\n",
      "At 240 step, loss is 0.0003845128230750561\n",
      "Epoch 75\n",
      "At 0 step, loss is 4.9005127948476e-05\n",
      "At 10 step, loss is 9.313046757597476e-05\n",
      "At 20 step, loss is 1.5116021131689195e-05\n",
      "At 30 step, loss is 0.00040200731018558145\n",
      "At 40 step, loss is 0.00016883610805962235\n",
      "At 50 step, loss is 0.0014260804746299982\n",
      "At 60 step, loss is 0.003914041444659233\n",
      "At 70 step, loss is 0.0034178730566054583\n",
      "At 80 step, loss is 6.08693062531529e-06\n",
      "At 90 step, loss is 2.5601144443498924e-05\n",
      "At 100 step, loss is 0.000678422162309289\n",
      "At 110 step, loss is 0.005747899413108826\n",
      "At 120 step, loss is 2.0068941012141295e-05\n",
      "At 130 step, loss is 5.3850064432481304e-05\n",
      "At 140 step, loss is 0.001320164417847991\n",
      "At 150 step, loss is 1.3394678717304487e-05\n",
      "At 160 step, loss is 0.0007963054231368005\n",
      "At 170 step, loss is 0.0006283566472120583\n",
      "At 180 step, loss is 0.005383101291954517\n",
      "At 190 step, loss is 0.00016500968195032328\n",
      "At 200 step, loss is 0.00010494756133994088\n",
      "At 210 step, loss is 7.410842954413965e-05\n",
      "At 220 step, loss is 0.0009146254160441458\n",
      "At 230 step, loss is 0.005625460762530565\n",
      "At 240 step, loss is 2.815097559505375e-07\n",
      "Epoch 76\n",
      "At 0 step, loss is 0.0007909644627943635\n",
      "At 10 step, loss is 0.0015658335760235786\n",
      "At 20 step, loss is 3.328236562083475e-05\n",
      "At 30 step, loss is 0.0014900622190907598\n",
      "At 40 step, loss is 0.00015667019761167467\n",
      "At 50 step, loss is 6.138872322480893e-06\n",
      "At 60 step, loss is 5.3030846174806356e-05\n",
      "At 70 step, loss is 0.0002794503525365144\n",
      "At 80 step, loss is 0.00016879916074685752\n",
      "At 90 step, loss is 0.0024042539298534393\n",
      "At 100 step, loss is 1.0075845011670026e-06\n",
      "At 110 step, loss is 0.0002981138532049954\n",
      "At 120 step, loss is 1.5076864656293765e-05\n",
      "At 130 step, loss is 0.000811628473456949\n",
      "At 140 step, loss is 1.2623766451724805e-05\n",
      "At 150 step, loss is 6.188162387843477e-06\n",
      "At 160 step, loss is 0.0007643147837370634\n",
      "At 170 step, loss is 0.0007871091947890818\n",
      "At 180 step, loss is 2.5919189283740707e-05\n",
      "At 190 step, loss is 0.0013030926929786801\n",
      "At 200 step, loss is 0.00020018131181132048\n",
      "At 210 step, loss is 2.7310299628879875e-05\n",
      "At 220 step, loss is 2.247909469588194e-05\n",
      "At 230 step, loss is 0.03742219880223274\n",
      "At 240 step, loss is 4.979949881089851e-05\n",
      "Epoch 77\n",
      "At 0 step, loss is 0.0012519248994067311\n",
      "At 10 step, loss is 7.44761155146989e-06\n",
      "At 20 step, loss is 0.0007029878324829042\n",
      "At 30 step, loss is 2.5997644115705043e-05\n",
      "At 40 step, loss is 0.000537670508492738\n",
      "At 50 step, loss is 2.040539402514696e-05\n",
      "At 60 step, loss is 0.0005156200495548546\n",
      "At 70 step, loss is 4.997412543161772e-05\n",
      "At 80 step, loss is 0.007354130502790213\n",
      "At 90 step, loss is 0.000515974301379174\n",
      "At 100 step, loss is 0.001313500222750008\n",
      "At 110 step, loss is 0.0014269783860072494\n",
      "At 120 step, loss is 0.00013426736404653639\n",
      "At 130 step, loss is 0.0007198979146778584\n",
      "At 140 step, loss is 0.00033300952054560184\n",
      "At 150 step, loss is 0.006034094374626875\n",
      "At 160 step, loss is 0.00041254359530285\n",
      "At 170 step, loss is 1.7884511862575891e-06\n",
      "At 180 step, loss is 0.0031322825234383345\n",
      "At 190 step, loss is 0.0013779300497844815\n",
      "At 200 step, loss is 0.00037268814048729837\n",
      "At 210 step, loss is 1.1618469216045924e-05\n",
      "At 220 step, loss is 7.499907951569185e-05\n",
      "At 230 step, loss is 0.0033931713551282883\n",
      "At 240 step, loss is 0.0004784506745636463\n",
      "Epoch 78\n",
      "At 0 step, loss is 0.0001330574887106195\n",
      "At 10 step, loss is 8.033455742406659e-06\n",
      "At 20 step, loss is 9.43226259551011e-05\n",
      "At 30 step, loss is 1.0495723472558893e-05\n",
      "At 40 step, loss is 1.4872662177367602e-06\n",
      "At 50 step, loss is 4.4041277647011157e-07\n",
      "At 60 step, loss is 0.0010228941682726145\n",
      "At 70 step, loss is 0.00018717515922617167\n",
      "At 80 step, loss is 9.20366346690571e-06\n",
      "At 90 step, loss is 0.0008537874673493207\n",
      "At 100 step, loss is 5.957288158242591e-05\n",
      "At 110 step, loss is 0.00010510246647754684\n",
      "At 120 step, loss is 4.290042852517217e-06\n",
      "At 130 step, loss is 0.00024287753331009299\n",
      "At 140 step, loss is 1.487583176640328e-05\n",
      "At 150 step, loss is 0.000226065720198676\n",
      "At 160 step, loss is 3.759720493690111e-05\n",
      "At 170 step, loss is 7.063860539346933e-05\n",
      "At 180 step, loss is 0.0002541961148381233\n",
      "At 190 step, loss is 2.728573053900618e-05\n",
      "At 200 step, loss is 5.9893929574172944e-05\n",
      "At 210 step, loss is 0.0010828175581991673\n",
      "At 220 step, loss is 0.001669753110036254\n",
      "At 230 step, loss is 0.00046398345148190856\n",
      "At 240 step, loss is 0.0005958125693723559\n",
      "Epoch 79\n",
      "At 0 step, loss is 0.0002646049251779914\n",
      "At 10 step, loss is 0.0015284880064427853\n",
      "At 20 step, loss is 0.00018150392861571163\n",
      "At 30 step, loss is 0.0011997718829661608\n",
      "At 40 step, loss is 2.0876183043583296e-05\n",
      "At 50 step, loss is 0.00014163425657898188\n",
      "At 60 step, loss is 0.0003267546708229929\n",
      "At 70 step, loss is 0.000442678778199479\n",
      "At 80 step, loss is 0.000747833342757076\n",
      "At 90 step, loss is 9.425271940699531e-08\n",
      "At 100 step, loss is 0.000389013031963259\n",
      "At 110 step, loss is 0.003720862790942192\n",
      "At 120 step, loss is 0.0007238124962896109\n",
      "At 130 step, loss is 1.0273257430526428e-05\n",
      "At 140 step, loss is 0.00012337665248196572\n",
      "At 150 step, loss is 0.00023342162603512406\n",
      "At 160 step, loss is 0.0001540343655506149\n",
      "At 170 step, loss is 0.0005613677203655243\n",
      "At 180 step, loss is 0.00010479734919499606\n",
      "At 190 step, loss is 0.0002615452976897359\n",
      "At 200 step, loss is 3.758985940294224e-06\n",
      "At 210 step, loss is 2.5934471636901435e-07\n",
      "At 220 step, loss is 0.00014544997247867286\n",
      "At 230 step, loss is 1.3308796042110771e-05\n",
      "At 240 step, loss is 3.5255404782219557e-06\n",
      "Epoch 80\n",
      "At 0 step, loss is 0.00026693474501371384\n",
      "At 10 step, loss is 7.92477949289605e-05\n",
      "At 20 step, loss is 0.0016476790187880397\n",
      "At 30 step, loss is 9.70633263932541e-05\n",
      "At 40 step, loss is 3.175297024426982e-05\n",
      "At 50 step, loss is 0.00039964340976439416\n",
      "At 60 step, loss is 5.9841866459464654e-05\n",
      "At 70 step, loss is 0.0024716162588447332\n",
      "At 80 step, loss is 0.0009023318416438997\n",
      "At 90 step, loss is 1.5566563888569362e-05\n",
      "At 100 step, loss is 0.0025045492220669985\n",
      "At 110 step, loss is 0.0013343406608328223\n",
      "At 120 step, loss is 2.7238927941652946e-05\n",
      "At 130 step, loss is 0.0012297095963731408\n",
      "At 140 step, loss is 0.005971372593194246\n",
      "At 150 step, loss is 0.001568260253407061\n",
      "At 160 step, loss is 0.000558229221496731\n",
      "At 170 step, loss is 0.0011935143265873194\n",
      "At 180 step, loss is 5.0147405090683606e-06\n",
      "At 190 step, loss is 7.630283107573632e-06\n",
      "At 200 step, loss is 0.0007884309161454439\n",
      "At 210 step, loss is 0.000375963281840086\n",
      "At 220 step, loss is 0.0002868036972358823\n",
      "At 230 step, loss is 0.0016182914841920137\n",
      "At 240 step, loss is 2.355244942009449e-05\n",
      "Epoch 81\n",
      "At 0 step, loss is 2.7844831492984667e-05\n",
      "At 10 step, loss is 0.0035119298845529556\n",
      "At 20 step, loss is 0.0003787571331486106\n",
      "At 30 step, loss is 1.5178430658124853e-05\n",
      "At 40 step, loss is 0.0002443658304400742\n",
      "At 50 step, loss is 0.00035420714993961155\n",
      "At 60 step, loss is 0.00014505328726954758\n",
      "At 70 step, loss is 7.36906222300604e-05\n",
      "At 80 step, loss is 0.0006402191356755793\n",
      "At 90 step, loss is 0.00021352182375267148\n",
      "At 100 step, loss is 1.4214680959412362e-05\n",
      "At 110 step, loss is 0.0034760471899062395\n",
      "At 120 step, loss is 0.00021230008860584348\n",
      "At 130 step, loss is 0.0013987900456413627\n",
      "At 140 step, loss is 0.00020741594198625535\n",
      "At 150 step, loss is 0.00030936149414628744\n",
      "At 160 step, loss is 0.000953602371737361\n",
      "At 170 step, loss is 0.0007766442722640932\n",
      "At 180 step, loss is 3.5636181564768776e-05\n",
      "At 190 step, loss is 0.00018657643522601575\n",
      "At 200 step, loss is 0.00022000237368047237\n",
      "At 210 step, loss is 0.004430199973285198\n",
      "At 220 step, loss is 6.946576468180865e-05\n",
      "At 230 step, loss is 6.738878437317908e-05\n",
      "At 240 step, loss is 9.538613085169345e-06\n",
      "Epoch 82\n",
      "At 0 step, loss is 4.578198786475696e-05\n",
      "At 10 step, loss is 0.00371384946629405\n",
      "At 20 step, loss is 0.0006070297094993293\n",
      "At 30 step, loss is 0.0038718702271580696\n",
      "At 40 step, loss is 0.00167645956389606\n",
      "At 50 step, loss is 0.001616974244825542\n",
      "At 60 step, loss is 0.00029946089489385486\n",
      "At 70 step, loss is 9.947751823347062e-05\n",
      "At 80 step, loss is 0.0004222579300403595\n",
      "At 90 step, loss is 4.894376979791559e-05\n",
      "At 100 step, loss is 0.002439305419102311\n",
      "At 110 step, loss is 0.00010300279245711863\n",
      "At 120 step, loss is 0.000728230457752943\n",
      "At 130 step, loss is 5.969733592792181e-06\n",
      "At 140 step, loss is 0.0002231421385658905\n",
      "At 150 step, loss is 0.0008664722554385662\n",
      "At 160 step, loss is 3.713877276823041e-06\n",
      "At 170 step, loss is 4.643547799787484e-05\n",
      "At 180 step, loss is 0.001285173580981791\n",
      "At 190 step, loss is 0.0001497374032624066\n",
      "At 200 step, loss is 1.9023498680326156e-05\n",
      "At 210 step, loss is 0.00030359026277437806\n",
      "At 220 step, loss is 0.0008459623204544187\n",
      "At 230 step, loss is 0.0003514892014209181\n",
      "At 240 step, loss is 0.0015612217830494046\n",
      "Epoch 83\n",
      "At 0 step, loss is 0.00018167759117204696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 10 step, loss is 0.0005076764500699937\n",
      "At 20 step, loss is 0.003732538316398859\n",
      "At 30 step, loss is 0.0005884588463231921\n",
      "At 40 step, loss is 2.1939281396043953e-06\n",
      "At 50 step, loss is 8.030617027543485e-05\n",
      "At 60 step, loss is 0.0007613508496433496\n",
      "At 70 step, loss is 0.0007288241176865995\n",
      "At 80 step, loss is 0.0004997628857381642\n",
      "At 90 step, loss is 2.852345232895459e-06\n",
      "At 100 step, loss is 1.3096629118081182e-05\n",
      "At 110 step, loss is 0.0009625418460927904\n",
      "At 120 step, loss is 0.0028883726336061954\n",
      "At 130 step, loss is 5.597369818133302e-05\n",
      "At 140 step, loss is 0.0005390750011429191\n",
      "At 150 step, loss is 0.002327738329768181\n",
      "At 160 step, loss is 0.0006474753608927131\n",
      "At 170 step, loss is 0.0014243306359276175\n",
      "At 180 step, loss is 0.00022318997071124613\n",
      "At 190 step, loss is 0.0011271291878074408\n",
      "At 200 step, loss is 0.00010785097401821986\n",
      "At 210 step, loss is 5.8638474001782015e-05\n",
      "At 220 step, loss is 0.0008582022273913026\n",
      "At 230 step, loss is 0.0026704256888478994\n",
      "At 240 step, loss is 6.160977500258014e-05\n",
      "Epoch 84\n",
      "At 0 step, loss is 2.082511673506815e-05\n",
      "At 10 step, loss is 0.00010947289410978556\n",
      "At 20 step, loss is 0.0007484483066946268\n",
      "At 30 step, loss is 0.00023108329332899302\n",
      "At 40 step, loss is 1.2993888049095403e-05\n",
      "At 50 step, loss is 0.0002014650235651061\n",
      "At 60 step, loss is 0.00033456404344178736\n",
      "At 70 step, loss is 0.00014764037041459233\n",
      "At 80 step, loss is 7.274402946677583e-08\n",
      "At 90 step, loss is 0.00018419109983369708\n",
      "At 100 step, loss is 0.0008870898745954037\n",
      "At 110 step, loss is 1.0187615771428682e-05\n",
      "At 120 step, loss is 0.0016724041197448969\n",
      "At 130 step, loss is 5.78801655137795e-06\n",
      "At 140 step, loss is 0.0003717030049301684\n",
      "At 150 step, loss is 4.686981992563233e-05\n",
      "At 160 step, loss is 1.6433396012871526e-05\n",
      "At 170 step, loss is 0.00305187632329762\n",
      "At 180 step, loss is 0.012998189777135849\n",
      "At 190 step, loss is 0.006575026549398899\n",
      "At 200 step, loss is 0.0003187875263392925\n",
      "At 210 step, loss is 0.0001465653331251815\n",
      "At 220 step, loss is 0.0002565475006122142\n",
      "At 230 step, loss is 0.0006947778165340424\n",
      "At 240 step, loss is 0.0005124523304402828\n",
      "Epoch 85\n",
      "At 0 step, loss is 7.796596946718637e-06\n",
      "At 10 step, loss is 6.8817080318694934e-06\n",
      "At 20 step, loss is 0.00010725176252890378\n",
      "At 30 step, loss is 0.000549109245184809\n",
      "At 40 step, loss is 0.0006238667410798371\n",
      "At 50 step, loss is 3.701125751831569e-05\n",
      "At 60 step, loss is 0.0008924261783249676\n",
      "At 70 step, loss is 0.00010492686124052852\n",
      "At 80 step, loss is 2.3740719825582346e-06\n",
      "At 90 step, loss is 0.0001683093432802707\n",
      "At 100 step, loss is 0.00028436529100872576\n",
      "At 110 step, loss is 0.0001237827818840742\n",
      "At 120 step, loss is 6.384636071743444e-05\n",
      "At 130 step, loss is 0.004205693956464529\n",
      "At 140 step, loss is 0.00013786468480248004\n",
      "At 150 step, loss is 0.00022628284932579845\n",
      "At 160 step, loss is 0.00016253383364528418\n",
      "At 170 step, loss is 1.788118970580399e-05\n",
      "At 180 step, loss is 0.00024381444382015616\n",
      "At 190 step, loss is 0.00032567602465860546\n",
      "At 200 step, loss is 0.00016709075134713203\n",
      "At 210 step, loss is 0.0013254794757813215\n",
      "At 220 step, loss is 0.00036695331800729036\n",
      "At 230 step, loss is 3.196853504050523e-05\n",
      "At 240 step, loss is 0.0005116878892295063\n",
      "Epoch 86\n",
      "At 0 step, loss is 0.0017465275013819337\n",
      "At 10 step, loss is 0.0008911584154702723\n",
      "At 20 step, loss is 0.0003857199044432491\n",
      "At 30 step, loss is 0.0005375239998102188\n",
      "At 40 step, loss is 2.594683110146434e-06\n",
      "At 50 step, loss is 4.9025828047888353e-05\n",
      "At 60 step, loss is 0.0001054369131452404\n",
      "At 70 step, loss is 0.005534174852073193\n",
      "At 80 step, loss is 8.430780144408345e-05\n",
      "At 90 step, loss is 0.00027897628024220467\n",
      "At 100 step, loss is 6.578551165148383e-06\n",
      "At 110 step, loss is 5.542856342799496e-06\n",
      "At 120 step, loss is 0.000978069263510406\n",
      "At 130 step, loss is 1.3772423699265346e-05\n",
      "At 140 step, loss is 5.667841833201237e-05\n",
      "At 150 step, loss is 0.0009261185768991709\n",
      "At 160 step, loss is 0.0006335873040370643\n",
      "At 170 step, loss is 0.00030806876020506024\n",
      "At 180 step, loss is 0.0001331357198068872\n",
      "At 190 step, loss is 0.014238924719393253\n",
      "At 200 step, loss is 6.636107718804851e-05\n",
      "At 210 step, loss is 3.0415101264225086e-06\n",
      "At 220 step, loss is 0.00016515556490048766\n",
      "At 230 step, loss is 0.00041212287032976747\n",
      "At 240 step, loss is 0.001075097476132214\n",
      "Epoch 87\n",
      "At 0 step, loss is 0.005100702401250601\n",
      "At 10 step, loss is 0.0010459559271112084\n",
      "At 20 step, loss is 0.0002279686159454286\n",
      "At 30 step, loss is 0.004000813700258732\n",
      "At 40 step, loss is 0.0007061668438836932\n",
      "At 50 step, loss is 0.0006686708657070994\n",
      "At 60 step, loss is 0.00017359081539325416\n",
      "At 70 step, loss is 0.00012453232193365693\n",
      "At 80 step, loss is 0.022040894255042076\n",
      "At 90 step, loss is 0.00027634200523607433\n",
      "At 100 step, loss is 0.0014918289380148053\n",
      "At 110 step, loss is 7.442399123647192e-07\n",
      "At 120 step, loss is 0.00011493412603158504\n",
      "At 130 step, loss is 0.00019699752738233656\n",
      "At 140 step, loss is 0.0018130311509594321\n",
      "At 150 step, loss is 0.001370737561956048\n",
      "At 160 step, loss is 1.0336943523370223e-09\n",
      "At 170 step, loss is 0.0011099755065515637\n",
      "At 180 step, loss is 4.641779378289357e-05\n",
      "At 190 step, loss is 0.00010339555592508987\n",
      "At 200 step, loss is 4.3461332097649574e-06\n",
      "At 210 step, loss is 0.00010806113277794793\n",
      "At 220 step, loss is 1.500585653957387e-06\n",
      "At 230 step, loss is 0.0014045521384105086\n",
      "At 240 step, loss is 0.0008959061233326793\n",
      "Epoch 88\n",
      "At 0 step, loss is 0.0002295408776262775\n",
      "At 10 step, loss is 0.0011285090586170554\n",
      "At 20 step, loss is 0.00012030996731482446\n",
      "At 30 step, loss is 2.990849452544353e-06\n",
      "At 40 step, loss is 0.0010071409633383155\n",
      "At 50 step, loss is 0.006467041093856096\n",
      "At 60 step, loss is 0.0005502405692823231\n",
      "At 70 step, loss is 0.0012234533205628395\n",
      "At 80 step, loss is 0.00010527591075515375\n",
      "At 90 step, loss is 4.017771971120965e-06\n",
      "At 100 step, loss is 0.0003738504310604185\n",
      "At 110 step, loss is 1.2394054465403315e-05\n",
      "At 120 step, loss is 1.1783855597968795e-06\n",
      "At 130 step, loss is 8.001259743650735e-07\n",
      "At 140 step, loss is 2.424730382699636e-06\n",
      "At 150 step, loss is 0.0008573291706852615\n",
      "At 160 step, loss is 0.00012740959937218577\n",
      "At 170 step, loss is 0.00014087255112826824\n",
      "At 180 step, loss is 0.00014281131734605879\n",
      "At 190 step, loss is 0.00017708852828945965\n",
      "At 200 step, loss is 0.02657379023730755\n",
      "At 210 step, loss is 0.0012146059889346361\n",
      "At 220 step, loss is 0.0026235422119498253\n",
      "At 230 step, loss is 0.0002976174291688949\n",
      "At 240 step, loss is 0.00016605609562247992\n",
      "Epoch 89\n",
      "At 0 step, loss is 5.556698397413129e-06\n",
      "At 10 step, loss is 0.00432558124884963\n",
      "At 20 step, loss is 8.627279021311551e-05\n",
      "At 30 step, loss is 0.0005620998563244939\n",
      "At 40 step, loss is 0.00021301573724485934\n",
      "At 50 step, loss is 0.018513966351747513\n",
      "At 60 step, loss is 8.725014595256653e-06\n",
      "At 70 step, loss is 1.393260959048348e-06\n",
      "At 80 step, loss is 0.00015457386325579137\n",
      "At 90 step, loss is 0.00024112596292980015\n",
      "At 100 step, loss is 0.0005663218325935304\n",
      "At 110 step, loss is 9.55592440732289e-07\n",
      "At 120 step, loss is 2.201196548412554e-05\n",
      "At 130 step, loss is 0.00011014290794264525\n",
      "At 140 step, loss is 0.0007682206341996789\n",
      "At 150 step, loss is 0.004480874165892601\n",
      "At 160 step, loss is 6.640005449298769e-05\n",
      "At 170 step, loss is 3.0213423087843694e-05\n",
      "At 180 step, loss is 0.00043774559162557125\n",
      "At 190 step, loss is 0.0006076838471926749\n",
      "At 200 step, loss is 0.007301813457161188\n",
      "At 210 step, loss is 0.0006810559425503016\n",
      "At 220 step, loss is 0.0010894212173298001\n",
      "At 230 step, loss is 0.0022433220874518156\n",
      "At 240 step, loss is 1.1015962400051649e-06\n",
      "Epoch 90\n",
      "At 0 step, loss is 0.00010830826067831367\n",
      "At 10 step, loss is 5.6555749324616045e-05\n",
      "At 20 step, loss is 3.189032941008918e-05\n",
      "At 30 step, loss is 8.534439984941855e-06\n",
      "At 40 step, loss is 0.000979132135398686\n",
      "At 50 step, loss is 0.0004889330011792481\n",
      "At 60 step, loss is 7.183233537944034e-05\n",
      "At 70 step, loss is 3.7479333059309283e-07\n",
      "At 80 step, loss is 0.00010160659439861774\n",
      "At 90 step, loss is 0.005293042864650488\n",
      "At 100 step, loss is 3.2982210541376844e-05\n",
      "At 110 step, loss is 0.00034598709316924214\n",
      "At 120 step, loss is 0.00039081196882762015\n",
      "At 130 step, loss is 0.0003725766728166491\n",
      "At 140 step, loss is 0.000266706250840798\n",
      "At 150 step, loss is 0.00011510718468343839\n",
      "At 160 step, loss is 9.187257819576189e-05\n",
      "At 170 step, loss is 0.0011229320662096143\n",
      "At 180 step, loss is 0.00010296428808942437\n",
      "At 190 step, loss is 0.0009599712793715298\n",
      "At 200 step, loss is 3.85143248422537e-05\n",
      "At 210 step, loss is 0.00015012556104920805\n",
      "At 220 step, loss is 0.0005406069685705006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 230 step, loss is 2.025207686529029e-05\n",
      "At 240 step, loss is 9.501937893219292e-05\n",
      "Epoch 91\n",
      "At 0 step, loss is 0.00028631187160499394\n",
      "At 10 step, loss is 0.00038904676330275834\n",
      "At 20 step, loss is 0.00015027413610368967\n",
      "At 30 step, loss is 4.581988566343398e-09\n",
      "At 40 step, loss is 3.154789374093525e-05\n",
      "At 50 step, loss is 0.0008139699348248541\n",
      "At 60 step, loss is 0.00047632763744331896\n",
      "At 70 step, loss is 0.0007912424625828862\n",
      "At 80 step, loss is 0.0020785925444215536\n",
      "At 90 step, loss is 0.0008415695047006011\n",
      "At 100 step, loss is 1.6268444596789777e-05\n",
      "At 110 step, loss is 3.8661488360958174e-05\n",
      "At 120 step, loss is 8.039450040087104e-05\n",
      "At 130 step, loss is 0.001952856546267867\n",
      "At 140 step, loss is 1.918831912917085e-05\n",
      "At 150 step, loss is 9.75665216174093e-07\n",
      "At 160 step, loss is 1.1672841537802014e-05\n",
      "At 170 step, loss is 0.00018161306797992438\n",
      "At 180 step, loss is 0.0001360483147436753\n",
      "At 190 step, loss is 0.0001941535883815959\n",
      "At 200 step, loss is 0.003486867994070053\n",
      "At 210 step, loss is 2.2985885152593255e-05\n",
      "At 220 step, loss is 0.0003025429032277316\n",
      "At 230 step, loss is 7.157921209000051e-05\n",
      "At 240 step, loss is 1.0066916729556397e-05\n",
      "Epoch 92\n",
      "At 0 step, loss is 0.0013140297960489988\n",
      "At 10 step, loss is 0.00012278277426958084\n",
      "At 20 step, loss is 0.002697803545743227\n",
      "At 30 step, loss is 0.002660205587744713\n",
      "At 40 step, loss is 0.0003252220631111413\n",
      "At 50 step, loss is 0.0002362953673582524\n",
      "At 60 step, loss is 0.00676683709025383\n",
      "At 70 step, loss is 8.498338388562843e-08\n",
      "At 80 step, loss is 0.0008963167201727629\n",
      "At 90 step, loss is 8.533071422789362e-07\n",
      "At 100 step, loss is 0.0008228557999245822\n",
      "At 110 step, loss is 0.0006123235216364264\n",
      "At 120 step, loss is 0.00013171890168450773\n",
      "At 130 step, loss is 0.000366680120350793\n",
      "At 140 step, loss is 0.00046381575521081686\n",
      "At 150 step, loss is 2.368511923123151e-05\n",
      "At 160 step, loss is 0.0012707668356597424\n",
      "At 170 step, loss is 0.00038610841147601604\n",
      "At 180 step, loss is 0.000419294141465798\n",
      "At 190 step, loss is 4.429284672369249e-05\n",
      "At 200 step, loss is 0.006532974541187286\n",
      "At 210 step, loss is 0.0002372475282754749\n",
      "At 220 step, loss is 2.4852663045749068e-05\n",
      "At 230 step, loss is 0.0014692314434796572\n",
      "At 240 step, loss is 5.041162694396917e-06\n",
      "Epoch 93\n",
      "At 0 step, loss is 1.972856989596039e-05\n",
      "At 10 step, loss is 6.0008778746123426e-06\n",
      "At 20 step, loss is 0.00015400664415210485\n",
      "At 30 step, loss is 3.333422137075104e-05\n",
      "At 40 step, loss is 0.00015817064559087157\n",
      "At 50 step, loss is 0.0034740387927740812\n",
      "At 60 step, loss is 0.0003078753943555057\n",
      "At 70 step, loss is 0.0005182534223422408\n",
      "At 80 step, loss is 2.623655200295616e-05\n",
      "At 90 step, loss is 0.00047179008834064007\n",
      "At 100 step, loss is 0.000979907694272697\n",
      "At 110 step, loss is 4.1666302422527224e-05\n",
      "At 120 step, loss is 4.472892396734096e-05\n",
      "At 130 step, loss is 0.00015089771477505565\n",
      "At 140 step, loss is 0.00032403491786681116\n",
      "At 150 step, loss is 5.846506610396318e-06\n",
      "At 160 step, loss is 0.000600511091761291\n",
      "At 170 step, loss is 0.0003261447127442807\n",
      "At 180 step, loss is 0.0002834948245435953\n",
      "At 190 step, loss is 0.0003051362291444093\n",
      "At 200 step, loss is 0.00029113207710906863\n",
      "At 210 step, loss is 8.424723637290299e-05\n",
      "At 220 step, loss is 3.462304812273942e-05\n",
      "At 230 step, loss is 0.005015941336750984\n",
      "At 240 step, loss is 2.8477840530172216e-08\n",
      "Epoch 94\n",
      "At 0 step, loss is 0.00029955990612506866\n",
      "At 10 step, loss is 8.637991413706914e-05\n",
      "At 20 step, loss is 0.0005246000364422798\n",
      "At 30 step, loss is 0.0001572715409565717\n",
      "At 40 step, loss is 0.0020219238940626383\n",
      "At 50 step, loss is 0.00013151131861377507\n",
      "At 60 step, loss is 0.003364903386682272\n",
      "At 70 step, loss is 4.394447387312539e-05\n",
      "At 80 step, loss is 0.0011894484050571918\n",
      "At 90 step, loss is 2.8616981580853462e-05\n",
      "At 100 step, loss is 0.00013293710071593523\n",
      "At 110 step, loss is 0.007436386775225401\n",
      "At 120 step, loss is 0.00010695755918277428\n",
      "At 130 step, loss is 0.00010876404849113896\n",
      "At 140 step, loss is 0.0002671641414053738\n",
      "At 150 step, loss is 0.00037089825491420925\n",
      "At 160 step, loss is 0.00014043122064322233\n",
      "At 170 step, loss is 0.0015434827655553818\n",
      "At 180 step, loss is 4.884547524852678e-05\n",
      "At 190 step, loss is 0.0029203263111412525\n",
      "At 200 step, loss is 0.0010878575267270207\n",
      "At 210 step, loss is 0.0005863849073648453\n",
      "At 220 step, loss is 0.00016027347010094672\n",
      "At 230 step, loss is 0.00021277293853927404\n",
      "At 240 step, loss is 7.86974560469389e-05\n",
      "Epoch 95\n",
      "At 0 step, loss is 8.077914390014485e-05\n",
      "At 10 step, loss is 8.173655078280717e-05\n",
      "At 20 step, loss is 0.0001349565281998366\n",
      "At 30 step, loss is 4.380855170893483e-05\n",
      "At 40 step, loss is 2.0935449640546722e-07\n",
      "At 50 step, loss is 0.00018994379206560552\n",
      "At 60 step, loss is 0.0011539349798113108\n",
      "At 70 step, loss is 0.00032043817918747663\n",
      "At 80 step, loss is 0.0012828289764001966\n",
      "At 90 step, loss is 4.5029973989585415e-05\n",
      "At 100 step, loss is 0.00011834019096568227\n",
      "At 110 step, loss is 1.1891330359503627e-05\n",
      "At 120 step, loss is 0.0003478868748061359\n",
      "At 130 step, loss is 0.0014384130481630564\n",
      "At 140 step, loss is 0.0005167961935512722\n",
      "At 150 step, loss is 0.0006312274490483105\n",
      "At 160 step, loss is 0.0005265986546874046\n",
      "At 170 step, loss is 0.0010904164519160986\n",
      "At 180 step, loss is 0.00031844020122662187\n",
      "At 190 step, loss is 0.00013026274973526597\n",
      "At 200 step, loss is 2.1151156033738516e-05\n",
      "At 210 step, loss is 3.8636968383798376e-05\n",
      "At 220 step, loss is 0.0035534072667360306\n",
      "At 230 step, loss is 0.00020201326697133482\n",
      "At 240 step, loss is 0.0009193742298521101\n",
      "Epoch 96\n",
      "At 0 step, loss is 0.0021060791332274675\n",
      "At 10 step, loss is 7.906940481916536e-06\n",
      "At 20 step, loss is 0.000264382193563506\n",
      "At 30 step, loss is 0.004663537722080946\n",
      "At 40 step, loss is 0.00022745637397747487\n",
      "At 50 step, loss is 0.0015678917989134789\n",
      "At 60 step, loss is 4.8106878239195794e-05\n",
      "At 70 step, loss is 0.0003832322545349598\n",
      "At 80 step, loss is 0.0003231878508813679\n",
      "At 90 step, loss is 1.6879655959201045e-05\n",
      "At 100 step, loss is 0.00012875901302322745\n",
      "At 110 step, loss is 0.00040002597961574793\n",
      "At 120 step, loss is 0.0010114592732861638\n",
      "At 130 step, loss is 0.0008466804283671081\n",
      "At 140 step, loss is 0.004626837093383074\n",
      "At 150 step, loss is 0.0003833334776572883\n",
      "At 160 step, loss is 4.837426240555942e-05\n",
      "At 170 step, loss is 6.281817331910133e-05\n",
      "At 180 step, loss is 0.0006550881080329418\n",
      "At 190 step, loss is 0.00039723963709548116\n",
      "At 200 step, loss is 2.532429607526865e-05\n",
      "At 210 step, loss is 0.000809861405286938\n",
      "At 220 step, loss is 5.54310361167154e-07\n",
      "At 230 step, loss is 0.00020450986630748957\n",
      "At 240 step, loss is 2.364756710449001e-06\n",
      "Epoch 97\n",
      "At 0 step, loss is 0.0010291461367160082\n",
      "At 10 step, loss is 0.0005770641146227717\n",
      "At 20 step, loss is 9.097369911614805e-05\n",
      "At 30 step, loss is 0.0006433134549297392\n",
      "At 40 step, loss is 3.1077939638635144e-05\n",
      "At 50 step, loss is 3.242447519369307e-07\n",
      "At 60 step, loss is 0.0008282737107947469\n",
      "At 70 step, loss is 0.0003785731678362936\n",
      "At 80 step, loss is 0.0006622964865528047\n",
      "At 90 step, loss is 0.0012460995931178331\n",
      "At 100 step, loss is 3.951339022023603e-05\n",
      "At 110 step, loss is 0.0015437852125614882\n",
      "At 120 step, loss is 0.0031765270978212357\n",
      "At 130 step, loss is 5.053551649325527e-05\n",
      "At 140 step, loss is 0.0006423892918974161\n",
      "At 150 step, loss is 0.00019916624296456575\n",
      "At 160 step, loss is 1.7299042156082578e-05\n",
      "At 170 step, loss is 3.366794408066198e-05\n",
      "At 180 step, loss is 2.522134491300676e-05\n",
      "At 190 step, loss is 6.251099694054574e-05\n",
      "At 200 step, loss is 2.6654308385332115e-05\n",
      "At 210 step, loss is 3.0774695005675312e-06\n",
      "At 220 step, loss is 0.0007926750113256276\n",
      "At 230 step, loss is 6.262239185161889e-05\n",
      "At 240 step, loss is 0.0005385393160395324\n",
      "Epoch 98\n",
      "At 0 step, loss is 7.644277502549812e-05\n",
      "At 10 step, loss is 0.011685078963637352\n",
      "At 20 step, loss is 0.00012420507846400142\n",
      "At 30 step, loss is 2.4293126443808433e-07\n",
      "At 40 step, loss is 5.477299782796763e-05\n",
      "At 50 step, loss is 0.008128798566758633\n",
      "At 60 step, loss is 2.179512011935003e-05\n",
      "At 70 step, loss is 6.552824197569862e-05\n",
      "At 80 step, loss is 0.0002052595664281398\n",
      "At 90 step, loss is 0.0004028816765639931\n",
      "At 100 step, loss is 0.0005294016445986927\n",
      "At 110 step, loss is 0.00020497397053986788\n",
      "At 120 step, loss is 4.526613793132128e-06\n",
      "At 130 step, loss is 1.3337134078028612e-05\n",
      "At 140 step, loss is 2.8452506739995442e-05\n",
      "At 150 step, loss is 2.5968092813855037e-05\n",
      "At 160 step, loss is 1.8422044377075508e-05\n",
      "At 170 step, loss is 0.0007312829256989062\n",
      "At 180 step, loss is 0.0007736982079222798\n",
      "At 190 step, loss is 0.0013540853979066014\n",
      "At 200 step, loss is 7.846875149652988e-08\n",
      "At 210 step, loss is 0.0010330050718039274\n",
      "At 220 step, loss is 0.00031467084772884846\n",
      "At 230 step, loss is 7.165780698414892e-05\n",
      "At 240 step, loss is 1.7895934433909133e-05\n",
      "Epoch 99\n",
      "At 0 step, loss is 2.446089820296038e-05\n",
      "At 10 step, loss is 3.3537646231707186e-05\n",
      "At 20 step, loss is 0.0005982723087072372\n",
      "At 30 step, loss is 9.487301576882601e-05\n",
      "At 40 step, loss is 2.8811648462578887e-06\n",
      "At 50 step, loss is 0.0007802367326803505\n",
      "At 60 step, loss is 0.001184146269224584\n",
      "At 70 step, loss is 2.986836261698045e-05\n",
      "At 80 step, loss is 0.0010579591616988182\n",
      "At 90 step, loss is 0.00015542362234555185\n",
      "At 100 step, loss is 0.00021291723533067852\n",
      "At 110 step, loss is 8.43498419271782e-05\n",
      "At 120 step, loss is 0.0017676280112937093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 130 step, loss is 8.118359255604446e-05\n",
      "At 140 step, loss is 0.0001906437537400052\n",
      "At 150 step, loss is 5.920482362853363e-05\n",
      "At 160 step, loss is 0.0004017529427073896\n",
      "At 170 step, loss is 1.4925257346476428e-05\n",
      "At 180 step, loss is 5.9935293393209577e-05\n",
      "At 190 step, loss is 0.00046828374615870416\n",
      "At 200 step, loss is 0.004220678936690092\n",
      "At 210 step, loss is 0.0003589869011193514\n",
      "At 220 step, loss is 0.0010692860232666135\n",
      "At 230 step, loss is 0.00014557999384123832\n",
      "At 240 step, loss is 7.55061955715064e-06\n",
      "At the last step, loss is 0.0004929770948365331\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hyperparameters:\n",
    "loss_array =[]\n",
    "days = 10 #control number of days available to the neural network\n",
    "\n",
    "batch_size = math.floor(num_days/days) - 1 #in huge datasets it is customary to use batch size and updating parameters each batch until it covers the whole \n",
    "#dataset each epoch. It is also customary to use powers of 2 for the batch size.\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "n_epoch = 100 #an epoch ends when the whole training set is covered.\n",
    "\n",
    "input_size = days - 1 #I hate tensors\n",
    "\n",
    "batch_pile = []\n",
    "\n",
    "dimensions = {\"L0\": input_size, \"L1\": 8, \"L2\": 4, \"L3\":1} #number of nodes in each layer, L4 is the output. 10 IS FIXED\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, dimensions):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.L1 = nn.Linear(dimensions['L0'], dimensions['L1'])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.L2 = nn.Linear(dimensions['L1'], dimensions['L2'])\n",
    "        self.L3 = nn.Linear(dimensions['L2'], dimensions['L3'])\n",
    "        #self.L4 = nn.Linear(dimensions['L3'], dimensions['L4'])\n",
    "    def forward(self, x): #x is the input\n",
    "        #3 layers, each with relu except the last layer.\n",
    "        x = self.L1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.L2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.L3(x)\n",
    "        y_predict = self.tanh(x)\n",
    "        return y_predict\n",
    "model = NeuralNetwork(dimensions)\n",
    "\n",
    "lossf = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "for epoch in range(n_epoch):\n",
    "    print('Epoch ' + str(epoch))\n",
    "    stock_matrix, next_day_matrix, prev_day_vector = stock_matrix_generator(data, days, batch_size)\n",
    "    \n",
    "    \n",
    "    batch_pile = []\n",
    "    for batch in range(batch_size):\n",
    "        batch_pile.append(batch)\n",
    "        \n",
    "    for batch in range(batch_size):\n",
    "        \n",
    "        randbatch = batch_pile[random.randrange(0, len(batch_pile))]\n",
    "        batch_pile.remove(randbatch)\n",
    "        stock_tensor = torch.from_numpy(stock_matrix[randbatch]).float().T  #transpose because pytorch is shit\n",
    "        actual = torch.from_numpy(np.array([float(next_day_matrix[randbatch][1][1:])])).float()\n",
    "        \n",
    "        percentage_change = percentage(prev_day_vector[randbatch], float(next_day_matrix[randbatch][1][1:] ))\n",
    "        percentage_change = percentage_change.float()\n",
    "        \n",
    "        y_predict = model(stock_tensor) #forward prop\n",
    "\n",
    "        loss = lossf(y_predict, percentage_change)\n",
    "        loss_array.append(loss)\n",
    "        loss.backward() #backprop\n",
    "\n",
    "        optimizer.step() #update parameters\n",
    "        optimizer.zero_grad() #resets the gradients\n",
    "        if batch % 10 == 0:\n",
    "            print(f'At {batch} step, loss is {loss}')\n",
    "print(f'At the last step, loss is {loss}')\n",
    "with torch.no_grad():\n",
    "    \n",
    "    plt.plot(np.linspace(0,batch_size*n_epoch-1,num=batch_size*n_epoch), loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "949aa00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Predicted on 11/19/2014 : \n",
      "-0.013679522\n",
      "Actual was: \n",
      "-0.0075801103\n",
      "-80.4660022141319%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    randnum = np.random.randint(0, batch_size-1)\n",
    "    print(randnum)\n",
    "    stock_matrix, next_day_matrix, prev_day_vector = stock_matrix_generator(data,days, batch_size)\n",
    "    stock_tensor = torch.from_numpy(stock_matrix[randnum]).float().T \n",
    "    predicted = model(stock_tensor)\n",
    "    \n",
    "    actual_percentage_change = percentage(prev_day_vector[randnum], float(next_day_matrix[randnum][1][1:] ))\n",
    "    actual_percentage_change = percentage_change\n",
    "\n",
    "    \n",
    "    predicted = predicted.numpy()[0][0]\n",
    " \n",
    "    day = next_day_matrix[randnum][0]\n",
    "    print('Predicted on ' + day + ' : ')\n",
    "    print(predicted)\n",
    "    print('Actual was: ')\n",
    "    print(actual_percentage_change)\n",
    "    diff = 100*(actual_percentage_change - predicted)/actual_percentage_change \n",
    "    print(str(diff) + '%')\n",
    "    prev_price = stock_matrix[randnum][0][0]\n",
    "\n",
    "    \n",
    "    #plt.plot(np.linspace(0,days-1,num=days-1),  stock_matrix[randnum][::-1])\n",
    "    #plt.plot(days,predicted, 'ro', label = 'predicted')\n",
    "    #plt.plot(days,actual_percentage_change, 'bo')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b4082700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "torch.save(model.state_dict(), r'C:\\Users\\user\\Documents\\road to qm\\machine learning\\pytorch\\stock market\\models\\best of v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f549d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\user\\Documents\\road to qm\\machine learning\\pytorch\\stock market"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
